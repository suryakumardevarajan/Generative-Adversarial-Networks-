{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XUrkeo6b1Eqp",
        "colab_type": "code",
        "outputId": "fcc321f5-5117-421e-c086-6b00ea88cfaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fgWcVHFr1Qa1",
        "colab_type": "code",
        "outputId": "33a34942-91c8-4a76-edb1-ccd797844696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "cd /content/drive/My\\ Drive/McGill/AML/FinalProj/WGAN_RMS"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/McGill/AML/FinalProj/WGAN_RMS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ufUUTnIh1k5T",
        "colab_type": "code",
        "outputId": "f5b2d447-db12-4da9-b941-aacf9d44346f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop\n",
        "import pickle\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G3uwrkia1nnn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "        self.losses = {\"D_real\":[], \"D_fake\":[] ,\"G\":[]}\n",
        "\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.01\n",
        "        optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # The critic takes generated images as input and determines validity\n",
        "        valid = self.critic(img)\n",
        "\n",
        "        # The combined model  (stacked generator and critic)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.ones((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "                \n",
        "                # Sample noise as generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "                # Generate a batch of new images\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the critic\n",
        "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
        "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip critic weights\n",
        "                for l in self.critic.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
        "            self.losses[\"D_real\"].append(d_loss[0])\n",
        "            self.losses[\"D_fake\"].append(d_loss[1])\n",
        "            self.losses[\"G\"].append(g_loss)\n",
        "            with open('WGAN_RMS.pickle', 'wb') as handle:\n",
        "              pickle.dump(self.losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "        self.plot_loss()\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"./images/wgan%d.png\" % epoch)\n",
        "        plt.close()\n",
        "        \n",
        "        \n",
        "    def plot_loss(self):\n",
        "        d_loss = [v for v in self.losses[\"D_real\"]]\n",
        "        g_loss = [v for v in self.losses[\"G\"]]\n",
        "        #d_acc = [v[1] for v in losses[\"D\"]]\n",
        "        #g_acc = [v[1] for v in losses[\"G\"]]\n",
        "\n",
        "        plt.figure(figsize=(10,8))\n",
        "        plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "        plt.plot(g_loss, label=\"Generator loss\")\n",
        "        #plt.plot(d_acc, label=\"Discriminator accuracy\")\n",
        "        #plt.plot(g_acc, label=\"Generator accuracy\")\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('WGAN RMS')\n",
        "        plt.legend()\n",
        "        plt.savefig(\"./WGAN_rms_loss.png\")\n",
        "        plt.show()\n",
        "\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZF4kt6t1vnb",
        "colab_type": "code",
        "outputId": "3b2a1b46-b393-41b7-a2d1-d3b06a28d94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56045
        }
      },
      "cell_type": "code",
      "source": [
        "wgan = WGAN()\n",
        "wgan.train(epochs= 3000, batch_size = 32, sample_interval = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 100,097\n",
            "Trainable params: 99,649\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,028,673\n",
            "Trainable params: 1,028,289\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.999931] [G loss: 1.000157]\n",
            "1 [D loss: 0.999936] [G loss: 1.000151]\n",
            "2 [D loss: 0.999936] [G loss: 1.000148]\n",
            "3 [D loss: 0.999935] [G loss: 1.000152]\n",
            "4 [D loss: 0.999930] [G loss: 1.000144]\n",
            "5 [D loss: 0.999934] [G loss: 1.000149]\n",
            "6 [D loss: 0.999930] [G loss: 1.000159]\n",
            "7 [D loss: 0.999933] [G loss: 1.000151]\n",
            "8 [D loss: 0.999927] [G loss: 1.000144]\n",
            "9 [D loss: 0.999937] [G loss: 1.000143]\n",
            "10 [D loss: 0.999939] [G loss: 1.000136]\n",
            "11 [D loss: 0.999931] [G loss: 1.000153]\n",
            "12 [D loss: 0.999943] [G loss: 1.000137]\n",
            "13 [D loss: 0.999936] [G loss: 1.000146]\n",
            "14 [D loss: 0.999936] [G loss: 1.000144]\n",
            "15 [D loss: 0.999936] [G loss: 1.000133]\n",
            "16 [D loss: 0.999936] [G loss: 1.000132]\n",
            "17 [D loss: 0.999949] [G loss: 1.000116]\n",
            "18 [D loss: 0.999942] [G loss: 1.000114]\n",
            "19 [D loss: 0.999938] [G loss: 1.000114]\n",
            "20 [D loss: 0.999944] [G loss: 1.000119]\n",
            "21 [D loss: 0.999944] [G loss: 1.000103]\n",
            "22 [D loss: 0.999954] [G loss: 1.000110]\n",
            "23 [D loss: 0.999950] [G loss: 1.000101]\n",
            "24 [D loss: 0.999952] [G loss: 1.000102]\n",
            "25 [D loss: 0.999945] [G loss: 1.000095]\n",
            "26 [D loss: 0.999955] [G loss: 1.000093]\n",
            "27 [D loss: 0.999954] [G loss: 1.000091]\n",
            "28 [D loss: 0.999960] [G loss: 1.000082]\n",
            "29 [D loss: 0.999962] [G loss: 1.000088]\n",
            "30 [D loss: 0.999963] [G loss: 1.000084]\n",
            "31 [D loss: 0.999965] [G loss: 1.000090]\n",
            "32 [D loss: 0.999961] [G loss: 1.000080]\n",
            "33 [D loss: 0.999965] [G loss: 1.000073]\n",
            "34 [D loss: 0.999962] [G loss: 1.000084]\n",
            "35 [D loss: 0.999964] [G loss: 1.000075]\n",
            "36 [D loss: 0.999970] [G loss: 1.000074]\n",
            "37 [D loss: 0.999973] [G loss: 1.000085]\n",
            "38 [D loss: 0.999973] [G loss: 1.000076]\n",
            "39 [D loss: 0.999973] [G loss: 1.000084]\n",
            "40 [D loss: 0.999977] [G loss: 1.000068]\n",
            "41 [D loss: 0.999974] [G loss: 1.000062]\n",
            "42 [D loss: 0.999970] [G loss: 1.000063]\n",
            "43 [D loss: 0.999977] [G loss: 1.000078]\n",
            "44 [D loss: 0.999976] [G loss: 1.000078]\n",
            "45 [D loss: 0.999976] [G loss: 1.000082]\n",
            "46 [D loss: 0.999971] [G loss: 1.000072]\n",
            "47 [D loss: 0.999972] [G loss: 1.000085]\n",
            "48 [D loss: 0.999981] [G loss: 1.000074]\n",
            "49 [D loss: 0.999977] [G loss: 1.000069]\n",
            "50 [D loss: 0.999980] [G loss: 1.000076]\n",
            "51 [D loss: 0.999983] [G loss: 1.000082]\n",
            "52 [D loss: 0.999976] [G loss: 1.000067]\n",
            "53 [D loss: 0.999983] [G loss: 1.000076]\n",
            "54 [D loss: 0.999994] [G loss: 1.000079]\n",
            "55 [D loss: 0.999989] [G loss: 1.000065]\n",
            "56 [D loss: 0.999983] [G loss: 1.000072]\n",
            "57 [D loss: 0.999982] [G loss: 1.000088]\n",
            "58 [D loss: 0.999997] [G loss: 1.000081]\n",
            "59 [D loss: 0.999989] [G loss: 1.000092]\n",
            "60 [D loss: 0.999997] [G loss: 1.000098]\n",
            "61 [D loss: 0.999989] [G loss: 1.000109]\n",
            "62 [D loss: 0.999996] [G loss: 1.000095]\n",
            "63 [D loss: 0.999980] [G loss: 1.000108]\n",
            "64 [D loss: 1.000003] [G loss: 1.000104]\n",
            "65 [D loss: 1.000000] [G loss: 1.000095]\n",
            "66 [D loss: 1.000002] [G loss: 1.000107]\n",
            "67 [D loss: 1.000005] [G loss: 1.000158]\n",
            "68 [D loss: 0.999998] [G loss: 1.000169]\n",
            "69 [D loss: 0.999993] [G loss: 1.000145]\n",
            "70 [D loss: 1.000008] [G loss: 1.000130]\n",
            "71 [D loss: 0.999992] [G loss: 1.000160]\n",
            "72 [D loss: 0.999970] [G loss: 1.000170]\n",
            "73 [D loss: 0.999980] [G loss: 1.000157]\n",
            "74 [D loss: 0.999972] [G loss: 1.000163]\n",
            "75 [D loss: 0.999985] [G loss: 1.000165]\n",
            "76 [D loss: 0.999993] [G loss: 1.000172]\n",
            "77 [D loss: 0.999980] [G loss: 1.000182]\n",
            "78 [D loss: 0.999988] [G loss: 1.000183]\n",
            "79 [D loss: 0.999991] [G loss: 1.000172]\n",
            "80 [D loss: 0.999960] [G loss: 1.000203]\n",
            "81 [D loss: 0.999967] [G loss: 1.000193]\n",
            "82 [D loss: 0.999976] [G loss: 1.000197]\n",
            "83 [D loss: 0.999972] [G loss: 1.000213]\n",
            "84 [D loss: 0.999966] [G loss: 1.000197]\n",
            "85 [D loss: 0.999968] [G loss: 1.000198]\n",
            "86 [D loss: 0.999969] [G loss: 1.000190]\n",
            "87 [D loss: 0.999971] [G loss: 1.000201]\n",
            "88 [D loss: 0.999975] [G loss: 1.000186]\n",
            "89 [D loss: 0.999990] [G loss: 1.000192]\n",
            "90 [D loss: 0.999965] [G loss: 1.000182]\n",
            "91 [D loss: 0.999981] [G loss: 1.000181]\n",
            "92 [D loss: 0.999964] [G loss: 1.000170]\n",
            "93 [D loss: 0.999968] [G loss: 1.000178]\n",
            "94 [D loss: 0.999961] [G loss: 1.000171]\n",
            "95 [D loss: 0.999961] [G loss: 1.000171]\n",
            "96 [D loss: 0.999965] [G loss: 1.000152]\n",
            "97 [D loss: 0.999974] [G loss: 1.000146]\n",
            "98 [D loss: 0.999966] [G loss: 1.000162]\n",
            "99 [D loss: 0.999969] [G loss: 1.000162]\n",
            "100 [D loss: 0.999965] [G loss: 1.000171]\n",
            "101 [D loss: 0.999970] [G loss: 1.000153]\n",
            "102 [D loss: 0.999962] [G loss: 1.000152]\n",
            "103 [D loss: 0.999957] [G loss: 1.000145]\n",
            "104 [D loss: 0.999980] [G loss: 1.000140]\n",
            "105 [D loss: 0.999962] [G loss: 1.000146]\n",
            "106 [D loss: 0.999968] [G loss: 1.000155]\n",
            "107 [D loss: 0.999965] [G loss: 1.000131]\n",
            "108 [D loss: 0.999964] [G loss: 1.000123]\n",
            "109 [D loss: 0.999958] [G loss: 1.000127]\n",
            "110 [D loss: 0.999966] [G loss: 1.000131]\n",
            "111 [D loss: 0.999969] [G loss: 1.000116]\n",
            "112 [D loss: 0.999963] [G loss: 1.000127]\n",
            "113 [D loss: 0.999964] [G loss: 1.000115]\n",
            "114 [D loss: 0.999973] [G loss: 1.000107]\n",
            "115 [D loss: 0.999967] [G loss: 1.000126]\n",
            "116 [D loss: 0.999968] [G loss: 1.000113]\n",
            "117 [D loss: 0.999966] [G loss: 1.000119]\n",
            "118 [D loss: 0.999965] [G loss: 1.000102]\n",
            "119 [D loss: 0.999965] [G loss: 1.000087]\n",
            "120 [D loss: 0.999973] [G loss: 1.000097]\n",
            "121 [D loss: 0.999969] [G loss: 1.000106]\n",
            "122 [D loss: 0.999960] [G loss: 1.000108]\n",
            "123 [D loss: 0.999970] [G loss: 1.000092]\n",
            "124 [D loss: 0.999962] [G loss: 1.000093]\n",
            "125 [D loss: 0.999974] [G loss: 1.000107]\n",
            "126 [D loss: 0.999967] [G loss: 1.000086]\n",
            "127 [D loss: 0.999973] [G loss: 1.000085]\n",
            "128 [D loss: 0.999978] [G loss: 1.000098]\n",
            "129 [D loss: 0.999966] [G loss: 1.000088]\n",
            "130 [D loss: 0.999971] [G loss: 1.000086]\n",
            "131 [D loss: 0.999972] [G loss: 1.000083]\n",
            "132 [D loss: 0.999961] [G loss: 1.000088]\n",
            "133 [D loss: 0.999961] [G loss: 1.000074]\n",
            "134 [D loss: 0.999977] [G loss: 1.000084]\n",
            "135 [D loss: 0.999970] [G loss: 1.000089]\n",
            "136 [D loss: 0.999973] [G loss: 1.000084]\n",
            "137 [D loss: 0.999970] [G loss: 1.000091]\n",
            "138 [D loss: 0.999969] [G loss: 1.000084]\n",
            "139 [D loss: 0.999973] [G loss: 1.000080]\n",
            "140 [D loss: 0.999966] [G loss: 1.000075]\n",
            "141 [D loss: 0.999971] [G loss: 1.000073]\n",
            "142 [D loss: 0.999977] [G loss: 1.000074]\n",
            "143 [D loss: 0.999974] [G loss: 1.000077]\n",
            "144 [D loss: 0.999965] [G loss: 1.000080]\n",
            "145 [D loss: 0.999967] [G loss: 1.000068]\n",
            "146 [D loss: 0.999968] [G loss: 1.000079]\n",
            "147 [D loss: 0.999970] [G loss: 1.000085]\n",
            "148 [D loss: 0.999973] [G loss: 1.000077]\n",
            "149 [D loss: 0.999964] [G loss: 1.000087]\n",
            "150 [D loss: 0.999969] [G loss: 1.000086]\n",
            "151 [D loss: 0.999970] [G loss: 1.000075]\n",
            "152 [D loss: 0.999970] [G loss: 1.000076]\n",
            "153 [D loss: 0.999970] [G loss: 1.000071]\n",
            "154 [D loss: 0.999971] [G loss: 1.000077]\n",
            "155 [D loss: 0.999966] [G loss: 1.000083]\n",
            "156 [D loss: 0.999968] [G loss: 1.000086]\n",
            "157 [D loss: 0.999973] [G loss: 1.000074]\n",
            "158 [D loss: 0.999973] [G loss: 1.000074]\n",
            "159 [D loss: 0.999972] [G loss: 1.000075]\n",
            "160 [D loss: 0.999968] [G loss: 1.000070]\n",
            "161 [D loss: 0.999963] [G loss: 1.000072]\n",
            "162 [D loss: 0.999971] [G loss: 1.000069]\n",
            "163 [D loss: 0.999970] [G loss: 1.000069]\n",
            "164 [D loss: 0.999975] [G loss: 1.000076]\n",
            "165 [D loss: 0.999970] [G loss: 1.000070]\n",
            "166 [D loss: 0.999967] [G loss: 1.000073]\n",
            "167 [D loss: 0.999977] [G loss: 1.000067]\n",
            "168 [D loss: 0.999969] [G loss: 1.000072]\n",
            "169 [D loss: 0.999969] [G loss: 1.000068]\n",
            "170 [D loss: 0.999964] [G loss: 1.000072]\n",
            "171 [D loss: 0.999971] [G loss: 1.000074]\n",
            "172 [D loss: 0.999967] [G loss: 1.000073]\n",
            "173 [D loss: 0.999970] [G loss: 1.000072]\n",
            "174 [D loss: 0.999968] [G loss: 1.000075]\n",
            "175 [D loss: 0.999966] [G loss: 1.000076]\n",
            "176 [D loss: 0.999971] [G loss: 1.000080]\n",
            "177 [D loss: 0.999970] [G loss: 1.000068]\n",
            "178 [D loss: 0.999965] [G loss: 1.000065]\n",
            "179 [D loss: 0.999965] [G loss: 1.000074]\n",
            "180 [D loss: 0.999972] [G loss: 1.000074]\n",
            "181 [D loss: 0.999969] [G loss: 1.000081]\n",
            "182 [D loss: 0.999973] [G loss: 1.000070]\n",
            "183 [D loss: 0.999967] [G loss: 1.000057]\n",
            "184 [D loss: 0.999969] [G loss: 1.000066]\n",
            "185 [D loss: 0.999971] [G loss: 1.000075]\n",
            "186 [D loss: 0.999972] [G loss: 1.000079]\n",
            "187 [D loss: 0.999968] [G loss: 1.000064]\n",
            "188 [D loss: 0.999969] [G loss: 1.000065]\n",
            "189 [D loss: 0.999970] [G loss: 1.000067]\n",
            "190 [D loss: 0.999973] [G loss: 1.000065]\n",
            "191 [D loss: 0.999966] [G loss: 1.000072]\n",
            "192 [D loss: 0.999969] [G loss: 1.000066]\n",
            "193 [D loss: 0.999972] [G loss: 1.000067]\n",
            "194 [D loss: 0.999971] [G loss: 1.000073]\n",
            "195 [D loss: 0.999967] [G loss: 1.000065]\n",
            "196 [D loss: 0.999970] [G loss: 1.000070]\n",
            "197 [D loss: 0.999967] [G loss: 1.000074]\n",
            "198 [D loss: 0.999969] [G loss: 1.000075]\n",
            "199 [D loss: 0.999968] [G loss: 1.000068]\n",
            "200 [D loss: 0.999971] [G loss: 1.000069]\n",
            "201 [D loss: 0.999973] [G loss: 1.000069]\n",
            "202 [D loss: 0.999967] [G loss: 1.000064]\n",
            "203 [D loss: 0.999971] [G loss: 1.000071]\n",
            "204 [D loss: 0.999971] [G loss: 1.000058]\n",
            "205 [D loss: 0.999973] [G loss: 1.000060]\n",
            "206 [D loss: 0.999964] [G loss: 1.000069]\n",
            "207 [D loss: 0.999971] [G loss: 1.000068]\n",
            "208 [D loss: 0.999967] [G loss: 1.000076]\n",
            "209 [D loss: 0.999972] [G loss: 1.000073]\n",
            "210 [D loss: 0.999967] [G loss: 1.000056]\n",
            "211 [D loss: 0.999971] [G loss: 1.000057]\n",
            "212 [D loss: 0.999970] [G loss: 1.000068]\n",
            "213 [D loss: 0.999970] [G loss: 1.000075]\n",
            "214 [D loss: 0.999967] [G loss: 1.000070]\n",
            "215 [D loss: 0.999969] [G loss: 1.000064]\n",
            "216 [D loss: 0.999974] [G loss: 1.000080]\n",
            "217 [D loss: 0.999966] [G loss: 1.000073]\n",
            "218 [D loss: 0.999964] [G loss: 1.000071]\n",
            "219 [D loss: 0.999971] [G loss: 1.000067]\n",
            "220 [D loss: 0.999975] [G loss: 1.000059]\n",
            "221 [D loss: 0.999964] [G loss: 1.000065]\n",
            "222 [D loss: 0.999964] [G loss: 1.000068]\n",
            "223 [D loss: 0.999971] [G loss: 1.000070]\n",
            "224 [D loss: 0.999973] [G loss: 1.000059]\n",
            "225 [D loss: 0.999968] [G loss: 1.000063]\n",
            "226 [D loss: 0.999971] [G loss: 1.000061]\n",
            "227 [D loss: 0.999970] [G loss: 1.000071]\n",
            "228 [D loss: 0.999964] [G loss: 1.000076]\n",
            "229 [D loss: 0.999969] [G loss: 1.000070]\n",
            "230 [D loss: 0.999964] [G loss: 1.000064]\n",
            "231 [D loss: 0.999970] [G loss: 1.000073]\n",
            "232 [D loss: 0.999966] [G loss: 1.000059]\n",
            "233 [D loss: 0.999975] [G loss: 1.000058]\n",
            "234 [D loss: 0.999959] [G loss: 1.000065]\n",
            "235 [D loss: 0.999973] [G loss: 1.000047]\n",
            "236 [D loss: 0.999973] [G loss: 1.000080]\n",
            "237 [D loss: 0.999971] [G loss: 1.000073]\n",
            "238 [D loss: 0.999972] [G loss: 1.000058]\n",
            "239 [D loss: 0.999962] [G loss: 1.000069]\n",
            "240 [D loss: 0.999967] [G loss: 1.000059]\n",
            "241 [D loss: 0.999968] [G loss: 1.000048]\n",
            "242 [D loss: 0.999966] [G loss: 1.000076]\n",
            "243 [D loss: 0.999957] [G loss: 1.000064]\n",
            "244 [D loss: 0.999971] [G loss: 1.000074]\n",
            "245 [D loss: 0.999972] [G loss: 1.000059]\n",
            "246 [D loss: 0.999967] [G loss: 1.000067]\n",
            "247 [D loss: 0.999976] [G loss: 1.000068]\n",
            "248 [D loss: 0.999974] [G loss: 1.000072]\n",
            "249 [D loss: 0.999968] [G loss: 1.000071]\n",
            "250 [D loss: 0.999962] [G loss: 1.000076]\n",
            "251 [D loss: 0.999965] [G loss: 1.000061]\n",
            "252 [D loss: 0.999969] [G loss: 1.000074]\n",
            "253 [D loss: 0.999974] [G loss: 1.000073]\n",
            "254 [D loss: 0.999982] [G loss: 1.000054]\n",
            "255 [D loss: 0.999970] [G loss: 1.000080]\n",
            "256 [D loss: 0.999971] [G loss: 1.000077]\n",
            "257 [D loss: 0.999975] [G loss: 1.000070]\n",
            "258 [D loss: 0.999968] [G loss: 1.000055]\n",
            "259 [D loss: 0.999965] [G loss: 1.000067]\n",
            "260 [D loss: 0.999965] [G loss: 1.000068]\n",
            "261 [D loss: 0.999958] [G loss: 1.000068]\n",
            "262 [D loss: 0.999969] [G loss: 1.000064]\n",
            "263 [D loss: 0.999981] [G loss: 1.000066]\n",
            "264 [D loss: 0.999969] [G loss: 1.000061]\n",
            "265 [D loss: 0.999971] [G loss: 1.000061]\n",
            "266 [D loss: 0.999966] [G loss: 1.000071]\n",
            "267 [D loss: 0.999968] [G loss: 1.000077]\n",
            "268 [D loss: 0.999967] [G loss: 1.000056]\n",
            "269 [D loss: 0.999970] [G loss: 1.000055]\n",
            "270 [D loss: 0.999975] [G loss: 1.000070]\n",
            "271 [D loss: 0.999965] [G loss: 1.000070]\n",
            "272 [D loss: 0.999969] [G loss: 1.000069]\n",
            "273 [D loss: 0.999970] [G loss: 1.000065]\n",
            "274 [D loss: 0.999971] [G loss: 1.000078]\n",
            "275 [D loss: 0.999976] [G loss: 1.000071]\n",
            "276 [D loss: 0.999969] [G loss: 1.000053]\n",
            "277 [D loss: 0.999968] [G loss: 1.000062]\n",
            "278 [D loss: 0.999971] [G loss: 1.000058]\n",
            "279 [D loss: 0.999973] [G loss: 1.000054]\n",
            "280 [D loss: 0.999961] [G loss: 1.000072]\n",
            "281 [D loss: 0.999964] [G loss: 1.000054]\n",
            "282 [D loss: 0.999962] [G loss: 1.000059]\n",
            "283 [D loss: 0.999966] [G loss: 1.000053]\n",
            "284 [D loss: 0.999970] [G loss: 1.000063]\n",
            "285 [D loss: 0.999970] [G loss: 1.000076]\n",
            "286 [D loss: 0.999968] [G loss: 1.000054]\n",
            "287 [D loss: 0.999968] [G loss: 1.000060]\n",
            "288 [D loss: 0.999972] [G loss: 1.000067]\n",
            "289 [D loss: 0.999974] [G loss: 1.000070]\n",
            "290 [D loss: 0.999976] [G loss: 1.000065]\n",
            "291 [D loss: 0.999965] [G loss: 1.000070]\n",
            "292 [D loss: 0.999969] [G loss: 1.000069]\n",
            "293 [D loss: 0.999972] [G loss: 1.000063]\n",
            "294 [D loss: 0.999966] [G loss: 1.000077]\n",
            "295 [D loss: 0.999970] [G loss: 1.000057]\n",
            "296 [D loss: 0.999967] [G loss: 1.000074]\n",
            "297 [D loss: 0.999969] [G loss: 1.000058]\n",
            "298 [D loss: 0.999964] [G loss: 1.000062]\n",
            "299 [D loss: 0.999971] [G loss: 1.000068]\n",
            "300 [D loss: 0.999977] [G loss: 1.000071]\n",
            "301 [D loss: 0.999961] [G loss: 1.000068]\n",
            "302 [D loss: 0.999966] [G loss: 1.000073]\n",
            "303 [D loss: 0.999975] [G loss: 1.000069]\n",
            "304 [D loss: 0.999972] [G loss: 1.000061]\n",
            "305 [D loss: 0.999960] [G loss: 1.000064]\n",
            "306 [D loss: 0.999966] [G loss: 1.000063]\n",
            "307 [D loss: 0.999976] [G loss: 1.000075]\n",
            "308 [D loss: 0.999970] [G loss: 1.000061]\n",
            "309 [D loss: 0.999977] [G loss: 1.000056]\n",
            "310 [D loss: 0.999969] [G loss: 1.000063]\n",
            "311 [D loss: 0.999973] [G loss: 1.000065]\n",
            "312 [D loss: 0.999962] [G loss: 1.000059]\n",
            "313 [D loss: 0.999970] [G loss: 1.000055]\n",
            "314 [D loss: 0.999967] [G loss: 1.000067]\n",
            "315 [D loss: 0.999969] [G loss: 1.000062]\n",
            "316 [D loss: 0.999966] [G loss: 1.000069]\n",
            "317 [D loss: 0.999972] [G loss: 1.000066]\n",
            "318 [D loss: 0.999971] [G loss: 1.000083]\n",
            "319 [D loss: 0.999975] [G loss: 1.000070]\n",
            "320 [D loss: 0.999965] [G loss: 1.000072]\n",
            "321 [D loss: 0.999970] [G loss: 1.000062]\n",
            "322 [D loss: 0.999972] [G loss: 1.000069]\n",
            "323 [D loss: 0.999969] [G loss: 1.000071]\n",
            "324 [D loss: 0.999971] [G loss: 1.000072]\n",
            "325 [D loss: 0.999975] [G loss: 1.000063]\n",
            "326 [D loss: 0.999963] [G loss: 1.000066]\n",
            "327 [D loss: 0.999970] [G loss: 1.000061]\n",
            "328 [D loss: 0.999971] [G loss: 1.000063]\n",
            "329 [D loss: 0.999970] [G loss: 1.000076]\n",
            "330 [D loss: 0.999969] [G loss: 1.000068]\n",
            "331 [D loss: 0.999967] [G loss: 1.000065]\n",
            "332 [D loss: 0.999971] [G loss: 1.000054]\n",
            "333 [D loss: 0.999973] [G loss: 1.000059]\n",
            "334 [D loss: 0.999969] [G loss: 1.000073]\n",
            "335 [D loss: 0.999970] [G loss: 1.000053]\n",
            "336 [D loss: 0.999969] [G loss: 1.000059]\n",
            "337 [D loss: 0.999969] [G loss: 1.000061]\n",
            "338 [D loss: 0.999972] [G loss: 1.000059]\n",
            "339 [D loss: 0.999957] [G loss: 1.000063]\n",
            "340 [D loss: 0.999974] [G loss: 1.000059]\n",
            "341 [D loss: 0.999965] [G loss: 1.000073]\n",
            "342 [D loss: 0.999971] [G loss: 1.000062]\n",
            "343 [D loss: 0.999964] [G loss: 1.000066]\n",
            "344 [D loss: 0.999969] [G loss: 1.000071]\n",
            "345 [D loss: 0.999969] [G loss: 1.000060]\n",
            "346 [D loss: 0.999970] [G loss: 1.000066]\n",
            "347 [D loss: 0.999973] [G loss: 1.000064]\n",
            "348 [D loss: 0.999966] [G loss: 1.000072]\n",
            "349 [D loss: 0.999976] [G loss: 1.000065]\n",
            "350 [D loss: 0.999966] [G loss: 1.000059]\n",
            "351 [D loss: 0.999969] [G loss: 1.000058]\n",
            "352 [D loss: 0.999966] [G loss: 1.000066]\n",
            "353 [D loss: 0.999966] [G loss: 1.000061]\n",
            "354 [D loss: 0.999973] [G loss: 1.000063]\n",
            "355 [D loss: 0.999965] [G loss: 1.000072]\n",
            "356 [D loss: 0.999973] [G loss: 1.000061]\n",
            "357 [D loss: 0.999973] [G loss: 1.000057]\n",
            "358 [D loss: 0.999960] [G loss: 1.000062]\n",
            "359 [D loss: 0.999967] [G loss: 1.000057]\n",
            "360 [D loss: 0.999972] [G loss: 1.000063]\n",
            "361 [D loss: 0.999962] [G loss: 1.000060]\n",
            "362 [D loss: 0.999972] [G loss: 1.000057]\n",
            "363 [D loss: 0.999969] [G loss: 1.000070]\n",
            "364 [D loss: 0.999963] [G loss: 1.000068]\n",
            "365 [D loss: 0.999967] [G loss: 1.000065]\n",
            "366 [D loss: 0.999968] [G loss: 1.000064]\n",
            "367 [D loss: 0.999970] [G loss: 1.000057]\n",
            "368 [D loss: 0.999973] [G loss: 1.000050]\n",
            "369 [D loss: 0.999968] [G loss: 1.000065]\n",
            "370 [D loss: 0.999975] [G loss: 1.000048]\n",
            "371 [D loss: 0.999973] [G loss: 1.000063]\n",
            "372 [D loss: 0.999972] [G loss: 1.000077]\n",
            "373 [D loss: 0.999966] [G loss: 1.000056]\n",
            "374 [D loss: 0.999971] [G loss: 1.000057]\n",
            "375 [D loss: 0.999968] [G loss: 1.000061]\n",
            "376 [D loss: 0.999967] [G loss: 1.000061]\n",
            "377 [D loss: 0.999968] [G loss: 1.000059]\n",
            "378 [D loss: 0.999970] [G loss: 1.000068]\n",
            "379 [D loss: 0.999975] [G loss: 1.000060]\n",
            "380 [D loss: 0.999959] [G loss: 1.000076]\n",
            "381 [D loss: 0.999968] [G loss: 1.000058]\n",
            "382 [D loss: 0.999974] [G loss: 1.000050]\n",
            "383 [D loss: 0.999973] [G loss: 1.000064]\n",
            "384 [D loss: 0.999964] [G loss: 1.000061]\n",
            "385 [D loss: 0.999961] [G loss: 1.000065]\n",
            "386 [D loss: 0.999968] [G loss: 1.000067]\n",
            "387 [D loss: 0.999969] [G loss: 1.000069]\n",
            "388 [D loss: 0.999963] [G loss: 1.000072]\n",
            "389 [D loss: 0.999964] [G loss: 1.000054]\n",
            "390 [D loss: 0.999970] [G loss: 1.000075]\n",
            "391 [D loss: 0.999966] [G loss: 1.000064]\n",
            "392 [D loss: 0.999964] [G loss: 1.000066]\n",
            "393 [D loss: 0.999973] [G loss: 1.000067]\n",
            "394 [D loss: 0.999962] [G loss: 1.000065]\n",
            "395 [D loss: 0.999966] [G loss: 1.000059]\n",
            "396 [D loss: 0.999973] [G loss: 1.000062]\n",
            "397 [D loss: 0.999969] [G loss: 1.000069]\n",
            "398 [D loss: 0.999978] [G loss: 1.000046]\n",
            "399 [D loss: 0.999975] [G loss: 1.000060]\n",
            "400 [D loss: 0.999972] [G loss: 1.000050]\n",
            "401 [D loss: 0.999972] [G loss: 1.000057]\n",
            "402 [D loss: 0.999970] [G loss: 1.000072]\n",
            "403 [D loss: 0.999973] [G loss: 1.000076]\n",
            "404 [D loss: 0.999969] [G loss: 1.000065]\n",
            "405 [D loss: 0.999974] [G loss: 1.000060]\n",
            "406 [D loss: 0.999969] [G loss: 1.000070]\n",
            "407 [D loss: 0.999972] [G loss: 1.000055]\n",
            "408 [D loss: 0.999968] [G loss: 1.000064]\n",
            "409 [D loss: 0.999974] [G loss: 1.000050]\n",
            "410 [D loss: 0.999962] [G loss: 1.000062]\n",
            "411 [D loss: 0.999978] [G loss: 1.000055]\n",
            "412 [D loss: 0.999977] [G loss: 1.000067]\n",
            "413 [D loss: 0.999971] [G loss: 1.000054]\n",
            "414 [D loss: 0.999968] [G loss: 1.000065]\n",
            "415 [D loss: 0.999969] [G loss: 1.000070]\n",
            "416 [D loss: 0.999977] [G loss: 1.000058]\n",
            "417 [D loss: 0.999960] [G loss: 1.000063]\n",
            "418 [D loss: 0.999959] [G loss: 1.000052]\n",
            "419 [D loss: 0.999978] [G loss: 1.000059]\n",
            "420 [D loss: 0.999978] [G loss: 1.000070]\n",
            "421 [D loss: 0.999973] [G loss: 1.000068]\n",
            "422 [D loss: 0.999967] [G loss: 1.000069]\n",
            "423 [D loss: 0.999973] [G loss: 1.000065]\n",
            "424 [D loss: 0.999970] [G loss: 1.000067]\n",
            "425 [D loss: 0.999970] [G loss: 1.000067]\n",
            "426 [D loss: 0.999974] [G loss: 1.000057]\n",
            "427 [D loss: 0.999971] [G loss: 1.000063]\n",
            "428 [D loss: 0.999976] [G loss: 1.000055]\n",
            "429 [D loss: 0.999971] [G loss: 1.000064]\n",
            "430 [D loss: 0.999980] [G loss: 1.000070]\n",
            "431 [D loss: 0.999965] [G loss: 1.000055]\n",
            "432 [D loss: 0.999967] [G loss: 1.000055]\n",
            "433 [D loss: 0.999969] [G loss: 1.000069]\n",
            "434 [D loss: 0.999968] [G loss: 1.000075]\n",
            "435 [D loss: 0.999978] [G loss: 1.000059]\n",
            "436 [D loss: 0.999977] [G loss: 1.000057]\n",
            "437 [D loss: 0.999970] [G loss: 1.000073]\n",
            "438 [D loss: 0.999982] [G loss: 1.000074]\n",
            "439 [D loss: 0.999959] [G loss: 1.000069]\n",
            "440 [D loss: 0.999961] [G loss: 1.000070]\n",
            "441 [D loss: 0.999964] [G loss: 1.000055]\n",
            "442 [D loss: 0.999975] [G loss: 1.000065]\n",
            "443 [D loss: 0.999964] [G loss: 1.000069]\n",
            "444 [D loss: 0.999967] [G loss: 1.000073]\n",
            "445 [D loss: 0.999969] [G loss: 1.000045]\n",
            "446 [D loss: 0.999963] [G loss: 1.000068]\n",
            "447 [D loss: 0.999971] [G loss: 1.000063]\n",
            "448 [D loss: 0.999981] [G loss: 1.000076]\n",
            "449 [D loss: 0.999970] [G loss: 1.000066]\n",
            "450 [D loss: 0.999985] [G loss: 1.000059]\n",
            "451 [D loss: 0.999962] [G loss: 1.000069]\n",
            "452 [D loss: 0.999957] [G loss: 1.000062]\n",
            "453 [D loss: 0.999971] [G loss: 1.000060]\n",
            "454 [D loss: 0.999980] [G loss: 1.000065]\n",
            "455 [D loss: 0.999975] [G loss: 1.000058]\n",
            "456 [D loss: 0.999976] [G loss: 1.000050]\n",
            "457 [D loss: 0.999966] [G loss: 1.000054]\n",
            "458 [D loss: 0.999970] [G loss: 1.000046]\n",
            "459 [D loss: 0.999965] [G loss: 1.000055]\n",
            "460 [D loss: 0.999980] [G loss: 1.000067]\n",
            "461 [D loss: 0.999971] [G loss: 1.000068]\n",
            "462 [D loss: 0.999977] [G loss: 1.000061]\n",
            "463 [D loss: 0.999971] [G loss: 1.000056]\n",
            "464 [D loss: 0.999973] [G loss: 1.000061]\n",
            "465 [D loss: 0.999963] [G loss: 1.000068]\n",
            "466 [D loss: 0.999976] [G loss: 1.000052]\n",
            "467 [D loss: 0.999969] [G loss: 1.000061]\n",
            "468 [D loss: 0.999973] [G loss: 1.000077]\n",
            "469 [D loss: 0.999975] [G loss: 1.000071]\n",
            "470 [D loss: 0.999969] [G loss: 1.000058]\n",
            "471 [D loss: 0.999974] [G loss: 1.000076]\n",
            "472 [D loss: 0.999969] [G loss: 1.000055]\n",
            "473 [D loss: 0.999968] [G loss: 1.000055]\n",
            "474 [D loss: 0.999961] [G loss: 1.000061]\n",
            "475 [D loss: 0.999957] [G loss: 1.000059]\n",
            "476 [D loss: 0.999971] [G loss: 1.000065]\n",
            "477 [D loss: 0.999962] [G loss: 1.000060]\n",
            "478 [D loss: 0.999969] [G loss: 1.000058]\n",
            "479 [D loss: 0.999976] [G loss: 1.000066]\n",
            "480 [D loss: 0.999970] [G loss: 1.000056]\n",
            "481 [D loss: 0.999974] [G loss: 1.000065]\n",
            "482 [D loss: 0.999966] [G loss: 1.000077]\n",
            "483 [D loss: 0.999976] [G loss: 1.000064]\n",
            "484 [D loss: 0.999968] [G loss: 1.000060]\n",
            "485 [D loss: 0.999962] [G loss: 1.000073]\n",
            "486 [D loss: 0.999968] [G loss: 1.000070]\n",
            "487 [D loss: 0.999970] [G loss: 1.000061]\n",
            "488 [D loss: 0.999969] [G loss: 1.000071]\n",
            "489 [D loss: 0.999977] [G loss: 1.000075]\n",
            "490 [D loss: 0.999979] [G loss: 1.000058]\n",
            "491 [D loss: 0.999968] [G loss: 1.000046]\n",
            "492 [D loss: 0.999981] [G loss: 1.000058]\n",
            "493 [D loss: 0.999976] [G loss: 1.000064]\n",
            "494 [D loss: 0.999969] [G loss: 1.000067]\n",
            "495 [D loss: 0.999972] [G loss: 1.000049]\n",
            "496 [D loss: 0.999969] [G loss: 1.000053]\n",
            "497 [D loss: 0.999961] [G loss: 1.000068]\n",
            "498 [D loss: 0.999965] [G loss: 1.000062]\n",
            "499 [D loss: 0.999977] [G loss: 1.000042]\n",
            "500 [D loss: 0.999969] [G loss: 1.000061]\n",
            "501 [D loss: 0.999975] [G loss: 1.000067]\n",
            "502 [D loss: 0.999962] [G loss: 1.000072]\n",
            "503 [D loss: 0.999967] [G loss: 1.000065]\n",
            "504 [D loss: 0.999972] [G loss: 1.000058]\n",
            "505 [D loss: 0.999966] [G loss: 1.000053]\n",
            "506 [D loss: 0.999966] [G loss: 1.000061]\n",
            "507 [D loss: 0.999970] [G loss: 1.000073]\n",
            "508 [D loss: 0.999978] [G loss: 1.000075]\n",
            "509 [D loss: 0.999967] [G loss: 1.000065]\n",
            "510 [D loss: 0.999978] [G loss: 1.000069]\n",
            "511 [D loss: 0.999970] [G loss: 1.000065]\n",
            "512 [D loss: 0.999975] [G loss: 1.000068]\n",
            "513 [D loss: 0.999972] [G loss: 1.000057]\n",
            "514 [D loss: 0.999979] [G loss: 1.000069]\n",
            "515 [D loss: 0.999974] [G loss: 1.000063]\n",
            "516 [D loss: 0.999969] [G loss: 1.000066]\n",
            "517 [D loss: 0.999968] [G loss: 1.000062]\n",
            "518 [D loss: 0.999968] [G loss: 1.000063]\n",
            "519 [D loss: 0.999970] [G loss: 1.000062]\n",
            "520 [D loss: 0.999969] [G loss: 1.000059]\n",
            "521 [D loss: 0.999974] [G loss: 1.000067]\n",
            "522 [D loss: 0.999967] [G loss: 1.000062]\n",
            "523 [D loss: 0.999978] [G loss: 1.000066]\n",
            "524 [D loss: 0.999972] [G loss: 1.000072]\n",
            "525 [D loss: 0.999974] [G loss: 1.000062]\n",
            "526 [D loss: 0.999964] [G loss: 1.000058]\n",
            "527 [D loss: 0.999976] [G loss: 1.000069]\n",
            "528 [D loss: 0.999975] [G loss: 1.000058]\n",
            "529 [D loss: 0.999974] [G loss: 1.000060]\n",
            "530 [D loss: 0.999968] [G loss: 1.000070]\n",
            "531 [D loss: 0.999974] [G loss: 1.000059]\n",
            "532 [D loss: 0.999971] [G loss: 1.000060]\n",
            "533 [D loss: 0.999970] [G loss: 1.000068]\n",
            "534 [D loss: 0.999972] [G loss: 1.000072]\n",
            "535 [D loss: 0.999969] [G loss: 1.000067]\n",
            "536 [D loss: 0.999971] [G loss: 1.000055]\n",
            "537 [D loss: 0.999970] [G loss: 1.000073]\n",
            "538 [D loss: 0.999973] [G loss: 1.000058]\n",
            "539 [D loss: 0.999970] [G loss: 1.000066]\n",
            "540 [D loss: 0.999970] [G loss: 1.000058]\n",
            "541 [D loss: 0.999971] [G loss: 1.000054]\n",
            "542 [D loss: 0.999977] [G loss: 1.000063]\n",
            "543 [D loss: 0.999968] [G loss: 1.000064]\n",
            "544 [D loss: 0.999960] [G loss: 1.000068]\n",
            "545 [D loss: 0.999974] [G loss: 1.000058]\n",
            "546 [D loss: 0.999965] [G loss: 1.000058]\n",
            "547 [D loss: 0.999974] [G loss: 1.000047]\n",
            "548 [D loss: 0.999982] [G loss: 1.000072]\n",
            "549 [D loss: 0.999967] [G loss: 1.000074]\n",
            "550 [D loss: 0.999972] [G loss: 1.000067]\n",
            "551 [D loss: 0.999974] [G loss: 1.000064]\n",
            "552 [D loss: 0.999970] [G loss: 1.000073]\n",
            "553 [D loss: 0.999968] [G loss: 1.000073]\n",
            "554 [D loss: 0.999966] [G loss: 1.000067]\n",
            "555 [D loss: 0.999973] [G loss: 1.000065]\n",
            "556 [D loss: 0.999973] [G loss: 1.000050]\n",
            "557 [D loss: 0.999976] [G loss: 1.000064]\n",
            "558 [D loss: 0.999972] [G loss: 1.000070]\n",
            "559 [D loss: 0.999969] [G loss: 1.000061]\n",
            "560 [D loss: 0.999967] [G loss: 1.000071]\n",
            "561 [D loss: 0.999973] [G loss: 1.000068]\n",
            "562 [D loss: 0.999968] [G loss: 1.000062]\n",
            "563 [D loss: 0.999970] [G loss: 1.000061]\n",
            "564 [D loss: 0.999968] [G loss: 1.000062]\n",
            "565 [D loss: 0.999967] [G loss: 1.000066]\n",
            "566 [D loss: 0.999968] [G loss: 1.000075]\n",
            "567 [D loss: 0.999971] [G loss: 1.000068]\n",
            "568 [D loss: 0.999973] [G loss: 1.000073]\n",
            "569 [D loss: 0.999974] [G loss: 1.000056]\n",
            "570 [D loss: 0.999973] [G loss: 1.000067]\n",
            "571 [D loss: 0.999964] [G loss: 1.000066]\n",
            "572 [D loss: 0.999970] [G loss: 1.000074]\n",
            "573 [D loss: 0.999972] [G loss: 1.000064]\n",
            "574 [D loss: 0.999977] [G loss: 1.000058]\n",
            "575 [D loss: 0.999974] [G loss: 1.000062]\n",
            "576 [D loss: 0.999962] [G loss: 1.000062]\n",
            "577 [D loss: 0.999979] [G loss: 1.000059]\n",
            "578 [D loss: 0.999964] [G loss: 1.000066]\n",
            "579 [D loss: 0.999974] [G loss: 1.000052]\n",
            "580 [D loss: 0.999972] [G loss: 1.000066]\n",
            "581 [D loss: 0.999971] [G loss: 1.000061]\n",
            "582 [D loss: 0.999964] [G loss: 1.000070]\n",
            "583 [D loss: 0.999972] [G loss: 1.000060]\n",
            "584 [D loss: 0.999976] [G loss: 1.000056]\n",
            "585 [D loss: 0.999975] [G loss: 1.000072]\n",
            "586 [D loss: 0.999972] [G loss: 1.000057]\n",
            "587 [D loss: 0.999967] [G loss: 1.000068]\n",
            "588 [D loss: 0.999962] [G loss: 1.000057]\n",
            "589 [D loss: 0.999964] [G loss: 1.000062]\n",
            "590 [D loss: 0.999971] [G loss: 1.000075]\n",
            "591 [D loss: 0.999973] [G loss: 1.000069]\n",
            "592 [D loss: 0.999974] [G loss: 1.000063]\n",
            "593 [D loss: 0.999974] [G loss: 1.000058]\n",
            "594 [D loss: 0.999969] [G loss: 1.000066]\n",
            "595 [D loss: 0.999969] [G loss: 1.000063]\n",
            "596 [D loss: 0.999975] [G loss: 1.000064]\n",
            "597 [D loss: 0.999967] [G loss: 1.000061]\n",
            "598 [D loss: 0.999970] [G loss: 1.000067]\n",
            "599 [D loss: 0.999969] [G loss: 1.000062]\n",
            "600 [D loss: 0.999972] [G loss: 1.000060]\n",
            "601 [D loss: 0.999973] [G loss: 1.000059]\n",
            "602 [D loss: 0.999972] [G loss: 1.000059]\n",
            "603 [D loss: 0.999974] [G loss: 1.000064]\n",
            "604 [D loss: 0.999972] [G loss: 1.000058]\n",
            "605 [D loss: 0.999969] [G loss: 1.000063]\n",
            "606 [D loss: 0.999978] [G loss: 1.000060]\n",
            "607 [D loss: 0.999973] [G loss: 1.000062]\n",
            "608 [D loss: 0.999970] [G loss: 1.000060]\n",
            "609 [D loss: 0.999968] [G loss: 1.000058]\n",
            "610 [D loss: 0.999972] [G loss: 1.000070]\n",
            "611 [D loss: 0.999956] [G loss: 1.000062]\n",
            "612 [D loss: 0.999969] [G loss: 1.000069]\n",
            "613 [D loss: 0.999979] [G loss: 1.000057]\n",
            "614 [D loss: 0.999953] [G loss: 1.000061]\n",
            "615 [D loss: 0.999977] [G loss: 1.000068]\n",
            "616 [D loss: 0.999971] [G loss: 1.000058]\n",
            "617 [D loss: 0.999973] [G loss: 1.000062]\n",
            "618 [D loss: 0.999971] [G loss: 1.000059]\n",
            "619 [D loss: 0.999982] [G loss: 1.000065]\n",
            "620 [D loss: 0.999972] [G loss: 1.000077]\n",
            "621 [D loss: 0.999974] [G loss: 1.000064]\n",
            "622 [D loss: 0.999961] [G loss: 1.000058]\n",
            "623 [D loss: 0.999973] [G loss: 1.000062]\n",
            "624 [D loss: 0.999980] [G loss: 1.000061]\n",
            "625 [D loss: 0.999962] [G loss: 1.000074]\n",
            "626 [D loss: 0.999967] [G loss: 1.000071]\n",
            "627 [D loss: 0.999976] [G loss: 1.000072]\n",
            "628 [D loss: 0.999963] [G loss: 1.000065]\n",
            "629 [D loss: 0.999974] [G loss: 1.000061]\n",
            "630 [D loss: 0.999961] [G loss: 1.000068]\n",
            "631 [D loss: 0.999972] [G loss: 1.000062]\n",
            "632 [D loss: 0.999963] [G loss: 1.000057]\n",
            "633 [D loss: 0.999973] [G loss: 1.000067]\n",
            "634 [D loss: 0.999972] [G loss: 1.000057]\n",
            "635 [D loss: 0.999968] [G loss: 1.000046]\n",
            "636 [D loss: 0.999974] [G loss: 1.000077]\n",
            "637 [D loss: 0.999971] [G loss: 1.000062]\n",
            "638 [D loss: 0.999966] [G loss: 1.000063]\n",
            "639 [D loss: 0.999977] [G loss: 1.000066]\n",
            "640 [D loss: 0.999966] [G loss: 1.000068]\n",
            "641 [D loss: 0.999968] [G loss: 1.000054]\n",
            "642 [D loss: 0.999969] [G loss: 1.000050]\n",
            "643 [D loss: 0.999973] [G loss: 1.000079]\n",
            "644 [D loss: 0.999964] [G loss: 1.000056]\n",
            "645 [D loss: 0.999966] [G loss: 1.000065]\n",
            "646 [D loss: 0.999964] [G loss: 1.000057]\n",
            "647 [D loss: 0.999972] [G loss: 1.000064]\n",
            "648 [D loss: 0.999962] [G loss: 1.000067]\n",
            "649 [D loss: 0.999961] [G loss: 1.000064]\n",
            "650 [D loss: 0.999969] [G loss: 1.000057]\n",
            "651 [D loss: 0.999969] [G loss: 1.000062]\n",
            "652 [D loss: 0.999968] [G loss: 1.000063]\n",
            "653 [D loss: 0.999964] [G loss: 1.000066]\n",
            "654 [D loss: 0.999970] [G loss: 1.000065]\n",
            "655 [D loss: 0.999974] [G loss: 1.000061]\n",
            "656 [D loss: 0.999969] [G loss: 1.000074]\n",
            "657 [D loss: 0.999966] [G loss: 1.000069]\n",
            "658 [D loss: 0.999968] [G loss: 1.000053]\n",
            "659 [D loss: 0.999971] [G loss: 1.000060]\n",
            "660 [D loss: 0.999967] [G loss: 1.000061]\n",
            "661 [D loss: 0.999972] [G loss: 1.000066]\n",
            "662 [D loss: 0.999972] [G loss: 1.000064]\n",
            "663 [D loss: 0.999970] [G loss: 1.000065]\n",
            "664 [D loss: 0.999970] [G loss: 1.000069]\n",
            "665 [D loss: 0.999970] [G loss: 1.000058]\n",
            "666 [D loss: 0.999970] [G loss: 1.000067]\n",
            "667 [D loss: 0.999975] [G loss: 1.000067]\n",
            "668 [D loss: 0.999970] [G loss: 1.000061]\n",
            "669 [D loss: 0.999973] [G loss: 1.000066]\n",
            "670 [D loss: 0.999963] [G loss: 1.000065]\n",
            "671 [D loss: 0.999976] [G loss: 1.000068]\n",
            "672 [D loss: 0.999975] [G loss: 1.000058]\n",
            "673 [D loss: 0.999965] [G loss: 1.000061]\n",
            "674 [D loss: 0.999971] [G loss: 1.000063]\n",
            "675 [D loss: 0.999968] [G loss: 1.000062]\n",
            "676 [D loss: 0.999970] [G loss: 1.000059]\n",
            "677 [D loss: 0.999969] [G loss: 1.000061]\n",
            "678 [D loss: 0.999974] [G loss: 1.000067]\n",
            "679 [D loss: 0.999966] [G loss: 1.000060]\n",
            "680 [D loss: 0.999973] [G loss: 1.000063]\n",
            "681 [D loss: 0.999969] [G loss: 1.000075]\n",
            "682 [D loss: 0.999973] [G loss: 1.000057]\n",
            "683 [D loss: 0.999963] [G loss: 1.000058]\n",
            "684 [D loss: 0.999970] [G loss: 1.000061]\n",
            "685 [D loss: 0.999971] [G loss: 1.000049]\n",
            "686 [D loss: 0.999970] [G loss: 1.000053]\n",
            "687 [D loss: 0.999969] [G loss: 1.000061]\n",
            "688 [D loss: 0.999968] [G loss: 1.000067]\n",
            "689 [D loss: 0.999970] [G loss: 1.000069]\n",
            "690 [D loss: 0.999974] [G loss: 1.000070]\n",
            "691 [D loss: 0.999969] [G loss: 1.000068]\n",
            "692 [D loss: 0.999967] [G loss: 1.000069]\n",
            "693 [D loss: 0.999971] [G loss: 1.000068]\n",
            "694 [D loss: 0.999975] [G loss: 1.000056]\n",
            "695 [D loss: 0.999971] [G loss: 1.000070]\n",
            "696 [D loss: 0.999968] [G loss: 1.000070]\n",
            "697 [D loss: 0.999976] [G loss: 1.000067]\n",
            "698 [D loss: 0.999971] [G loss: 1.000073]\n",
            "699 [D loss: 0.999971] [G loss: 1.000067]\n",
            "700 [D loss: 0.999974] [G loss: 1.000061]\n",
            "701 [D loss: 0.999969] [G loss: 1.000068]\n",
            "702 [D loss: 0.999970] [G loss: 1.000068]\n",
            "703 [D loss: 0.999967] [G loss: 1.000068]\n",
            "704 [D loss: 0.999966] [G loss: 1.000071]\n",
            "705 [D loss: 0.999970] [G loss: 1.000068]\n",
            "706 [D loss: 0.999970] [G loss: 1.000057]\n",
            "707 [D loss: 0.999964] [G loss: 1.000064]\n",
            "708 [D loss: 0.999978] [G loss: 1.000066]\n",
            "709 [D loss: 0.999964] [G loss: 1.000065]\n",
            "710 [D loss: 0.999982] [G loss: 1.000068]\n",
            "711 [D loss: 0.999980] [G loss: 1.000062]\n",
            "712 [D loss: 0.999975] [G loss: 1.000071]\n",
            "713 [D loss: 0.999965] [G loss: 1.000064]\n",
            "714 [D loss: 0.999969] [G loss: 1.000065]\n",
            "715 [D loss: 0.999962] [G loss: 1.000064]\n",
            "716 [D loss: 0.999980] [G loss: 1.000074]\n",
            "717 [D loss: 0.999969] [G loss: 1.000068]\n",
            "718 [D loss: 0.999963] [G loss: 1.000068]\n",
            "719 [D loss: 0.999973] [G loss: 1.000062]\n",
            "720 [D loss: 0.999967] [G loss: 1.000072]\n",
            "721 [D loss: 0.999966] [G loss: 1.000073]\n",
            "722 [D loss: 0.999970] [G loss: 1.000070]\n",
            "723 [D loss: 0.999974] [G loss: 1.000067]\n",
            "724 [D loss: 0.999967] [G loss: 1.000077]\n",
            "725 [D loss: 0.999969] [G loss: 1.000071]\n",
            "726 [D loss: 0.999961] [G loss: 1.000061]\n",
            "727 [D loss: 0.999976] [G loss: 1.000050]\n",
            "728 [D loss: 0.999974] [G loss: 1.000059]\n",
            "729 [D loss: 0.999974] [G loss: 1.000067]\n",
            "730 [D loss: 0.999973] [G loss: 1.000067]\n",
            "731 [D loss: 0.999966] [G loss: 1.000075]\n",
            "732 [D loss: 0.999973] [G loss: 1.000064]\n",
            "733 [D loss: 0.999977] [G loss: 1.000069]\n",
            "734 [D loss: 0.999966] [G loss: 1.000068]\n",
            "735 [D loss: 0.999967] [G loss: 1.000057]\n",
            "736 [D loss: 0.999967] [G loss: 1.000067]\n",
            "737 [D loss: 0.999975] [G loss: 1.000058]\n",
            "738 [D loss: 0.999973] [G loss: 1.000064]\n",
            "739 [D loss: 0.999971] [G loss: 1.000061]\n",
            "740 [D loss: 0.999972] [G loss: 1.000060]\n",
            "741 [D loss: 0.999966] [G loss: 1.000056]\n",
            "742 [D loss: 0.999968] [G loss: 1.000066]\n",
            "743 [D loss: 0.999978] [G loss: 1.000077]\n",
            "744 [D loss: 0.999974] [G loss: 1.000074]\n",
            "745 [D loss: 0.999968] [G loss: 1.000055]\n",
            "746 [D loss: 0.999962] [G loss: 1.000076]\n",
            "747 [D loss: 0.999977] [G loss: 1.000071]\n",
            "748 [D loss: 0.999974] [G loss: 1.000063]\n",
            "749 [D loss: 0.999970] [G loss: 1.000062]\n",
            "750 [D loss: 0.999972] [G loss: 1.000072]\n",
            "751 [D loss: 0.999970] [G loss: 1.000070]\n",
            "752 [D loss: 0.999970] [G loss: 1.000056]\n",
            "753 [D loss: 0.999970] [G loss: 1.000067]\n",
            "754 [D loss: 0.999964] [G loss: 1.000070]\n",
            "755 [D loss: 0.999966] [G loss: 1.000065]\n",
            "756 [D loss: 0.999964] [G loss: 1.000051]\n",
            "757 [D loss: 0.999961] [G loss: 1.000064]\n",
            "758 [D loss: 0.999958] [G loss: 1.000057]\n",
            "759 [D loss: 0.999966] [G loss: 1.000050]\n",
            "760 [D loss: 0.999969] [G loss: 1.000047]\n",
            "761 [D loss: 0.999975] [G loss: 1.000065]\n",
            "762 [D loss: 0.999971] [G loss: 1.000065]\n",
            "763 [D loss: 0.999972] [G loss: 1.000067]\n",
            "764 [D loss: 0.999969] [G loss: 1.000059]\n",
            "765 [D loss: 0.999968] [G loss: 1.000077]\n",
            "766 [D loss: 0.999974] [G loss: 1.000072]\n",
            "767 [D loss: 0.999975] [G loss: 1.000058]\n",
            "768 [D loss: 0.999973] [G loss: 1.000066]\n",
            "769 [D loss: 0.999975] [G loss: 1.000070]\n",
            "770 [D loss: 0.999974] [G loss: 1.000061]\n",
            "771 [D loss: 0.999963] [G loss: 1.000070]\n",
            "772 [D loss: 0.999966] [G loss: 1.000056]\n",
            "773 [D loss: 0.999973] [G loss: 1.000071]\n",
            "774 [D loss: 0.999970] [G loss: 1.000065]\n",
            "775 [D loss: 0.999977] [G loss: 1.000073]\n",
            "776 [D loss: 0.999969] [G loss: 1.000070]\n",
            "777 [D loss: 0.999962] [G loss: 1.000058]\n",
            "778 [D loss: 0.999970] [G loss: 1.000066]\n",
            "779 [D loss: 0.999969] [G loss: 1.000065]\n",
            "780 [D loss: 0.999974] [G loss: 1.000066]\n",
            "781 [D loss: 0.999974] [G loss: 1.000055]\n",
            "782 [D loss: 0.999971] [G loss: 1.000071]\n",
            "783 [D loss: 0.999975] [G loss: 1.000051]\n",
            "784 [D loss: 0.999969] [G loss: 1.000068]\n",
            "785 [D loss: 0.999965] [G loss: 1.000061]\n",
            "786 [D loss: 0.999972] [G loss: 1.000063]\n",
            "787 [D loss: 0.999972] [G loss: 1.000069]\n",
            "788 [D loss: 0.999976] [G loss: 1.000056]\n",
            "789 [D loss: 0.999975] [G loss: 1.000060]\n",
            "790 [D loss: 0.999976] [G loss: 1.000062]\n",
            "791 [D loss: 0.999965] [G loss: 1.000074]\n",
            "792 [D loss: 0.999974] [G loss: 1.000068]\n",
            "793 [D loss: 0.999965] [G loss: 1.000060]\n",
            "794 [D loss: 0.999969] [G loss: 1.000073]\n",
            "795 [D loss: 0.999968] [G loss: 1.000062]\n",
            "796 [D loss: 0.999973] [G loss: 1.000061]\n",
            "797 [D loss: 0.999971] [G loss: 1.000061]\n",
            "798 [D loss: 0.999969] [G loss: 1.000054]\n",
            "799 [D loss: 0.999973] [G loss: 1.000074]\n",
            "800 [D loss: 0.999978] [G loss: 1.000059]\n",
            "801 [D loss: 0.999966] [G loss: 1.000051]\n",
            "802 [D loss: 0.999966] [G loss: 1.000060]\n",
            "803 [D loss: 0.999966] [G loss: 1.000064]\n",
            "804 [D loss: 0.999974] [G loss: 1.000062]\n",
            "805 [D loss: 0.999969] [G loss: 1.000063]\n",
            "806 [D loss: 0.999968] [G loss: 1.000060]\n",
            "807 [D loss: 0.999972] [G loss: 1.000058]\n",
            "808 [D loss: 0.999970] [G loss: 1.000059]\n",
            "809 [D loss: 0.999978] [G loss: 1.000065]\n",
            "810 [D loss: 0.999971] [G loss: 1.000064]\n",
            "811 [D loss: 0.999975] [G loss: 1.000056]\n",
            "812 [D loss: 0.999964] [G loss: 1.000065]\n",
            "813 [D loss: 0.999966] [G loss: 1.000067]\n",
            "814 [D loss: 0.999968] [G loss: 1.000066]\n",
            "815 [D loss: 0.999972] [G loss: 1.000068]\n",
            "816 [D loss: 0.999976] [G loss: 1.000058]\n",
            "817 [D loss: 0.999965] [G loss: 1.000061]\n",
            "818 [D loss: 0.999971] [G loss: 1.000063]\n",
            "819 [D loss: 0.999973] [G loss: 1.000061]\n",
            "820 [D loss: 0.999971] [G loss: 1.000067]\n",
            "821 [D loss: 0.999977] [G loss: 1.000063]\n",
            "822 [D loss: 0.999963] [G loss: 1.000067]\n",
            "823 [D loss: 0.999967] [G loss: 1.000068]\n",
            "824 [D loss: 0.999965] [G loss: 1.000067]\n",
            "825 [D loss: 0.999967] [G loss: 1.000065]\n",
            "826 [D loss: 0.999975] [G loss: 1.000058]\n",
            "827 [D loss: 0.999978] [G loss: 1.000060]\n",
            "828 [D loss: 0.999967] [G loss: 1.000069]\n",
            "829 [D loss: 0.999967] [G loss: 1.000060]\n",
            "830 [D loss: 0.999973] [G loss: 1.000081]\n",
            "831 [D loss: 0.999970] [G loss: 1.000062]\n",
            "832 [D loss: 0.999971] [G loss: 1.000067]\n",
            "833 [D loss: 0.999972] [G loss: 1.000064]\n",
            "834 [D loss: 0.999963] [G loss: 1.000061]\n",
            "835 [D loss: 0.999964] [G loss: 1.000060]\n",
            "836 [D loss: 0.999971] [G loss: 1.000063]\n",
            "837 [D loss: 0.999966] [G loss: 1.000061]\n",
            "838 [D loss: 0.999971] [G loss: 1.000062]\n",
            "839 [D loss: 0.999974] [G loss: 1.000071]\n",
            "840 [D loss: 0.999975] [G loss: 1.000056]\n",
            "841 [D loss: 0.999966] [G loss: 1.000063]\n",
            "842 [D loss: 0.999965] [G loss: 1.000066]\n",
            "843 [D loss: 0.999974] [G loss: 1.000067]\n",
            "844 [D loss: 0.999963] [G loss: 1.000065]\n",
            "845 [D loss: 0.999967] [G loss: 1.000068]\n",
            "846 [D loss: 0.999976] [G loss: 1.000063]\n",
            "847 [D loss: 0.999960] [G loss: 1.000069]\n",
            "848 [D loss: 0.999967] [G loss: 1.000063]\n",
            "849 [D loss: 0.999967] [G loss: 1.000067]\n",
            "850 [D loss: 0.999965] [G loss: 1.000066]\n",
            "851 [D loss: 0.999970] [G loss: 1.000069]\n",
            "852 [D loss: 0.999968] [G loss: 1.000073]\n",
            "853 [D loss: 0.999972] [G loss: 1.000061]\n",
            "854 [D loss: 0.999975] [G loss: 1.000074]\n",
            "855 [D loss: 0.999967] [G loss: 1.000057]\n",
            "856 [D loss: 0.999970] [G loss: 1.000070]\n",
            "857 [D loss: 0.999963] [G loss: 1.000075]\n",
            "858 [D loss: 0.999968] [G loss: 1.000059]\n",
            "859 [D loss: 0.999978] [G loss: 1.000065]\n",
            "860 [D loss: 0.999975] [G loss: 1.000058]\n",
            "861 [D loss: 0.999966] [G loss: 1.000056]\n",
            "862 [D loss: 0.999971] [G loss: 1.000068]\n",
            "863 [D loss: 0.999971] [G loss: 1.000056]\n",
            "864 [D loss: 0.999974] [G loss: 1.000045]\n",
            "865 [D loss: 0.999964] [G loss: 1.000063]\n",
            "866 [D loss: 0.999968] [G loss: 1.000071]\n",
            "867 [D loss: 0.999975] [G loss: 1.000066]\n",
            "868 [D loss: 0.999968] [G loss: 1.000067]\n",
            "869 [D loss: 0.999975] [G loss: 1.000057]\n",
            "870 [D loss: 0.999971] [G loss: 1.000074]\n",
            "871 [D loss: 0.999969] [G loss: 1.000062]\n",
            "872 [D loss: 0.999970] [G loss: 1.000071]\n",
            "873 [D loss: 0.999971] [G loss: 1.000064]\n",
            "874 [D loss: 0.999972] [G loss: 1.000065]\n",
            "875 [D loss: 0.999975] [G loss: 1.000071]\n",
            "876 [D loss: 0.999976] [G loss: 1.000069]\n",
            "877 [D loss: 0.999967] [G loss: 1.000056]\n",
            "878 [D loss: 0.999965] [G loss: 1.000065]\n",
            "879 [D loss: 0.999972] [G loss: 1.000067]\n",
            "880 [D loss: 0.999973] [G loss: 1.000066]\n",
            "881 [D loss: 0.999976] [G loss: 1.000063]\n",
            "882 [D loss: 0.999974] [G loss: 1.000058]\n",
            "883 [D loss: 0.999969] [G loss: 1.000063]\n",
            "884 [D loss: 0.999966] [G loss: 1.000069]\n",
            "885 [D loss: 0.999967] [G loss: 1.000073]\n",
            "886 [D loss: 0.999963] [G loss: 1.000074]\n",
            "887 [D loss: 0.999975] [G loss: 1.000068]\n",
            "888 [D loss: 0.999977] [G loss: 1.000066]\n",
            "889 [D loss: 0.999968] [G loss: 1.000066]\n",
            "890 [D loss: 0.999967] [G loss: 1.000058]\n",
            "891 [D loss: 0.999970] [G loss: 1.000065]\n",
            "892 [D loss: 0.999969] [G loss: 1.000065]\n",
            "893 [D loss: 0.999971] [G loss: 1.000051]\n",
            "894 [D loss: 0.999973] [G loss: 1.000064]\n",
            "895 [D loss: 0.999969] [G loss: 1.000062]\n",
            "896 [D loss: 0.999965] [G loss: 1.000064]\n",
            "897 [D loss: 0.999973] [G loss: 1.000068]\n",
            "898 [D loss: 0.999971] [G loss: 1.000057]\n",
            "899 [D loss: 0.999968] [G loss: 1.000066]\n",
            "900 [D loss: 0.999973] [G loss: 1.000059]\n",
            "901 [D loss: 0.999974] [G loss: 1.000068]\n",
            "902 [D loss: 0.999967] [G loss: 1.000059]\n",
            "903 [D loss: 0.999970] [G loss: 1.000074]\n",
            "904 [D loss: 0.999961] [G loss: 1.000073]\n",
            "905 [D loss: 0.999970] [G loss: 1.000061]\n",
            "906 [D loss: 0.999966] [G loss: 1.000069]\n",
            "907 [D loss: 0.999973] [G loss: 1.000064]\n",
            "908 [D loss: 0.999977] [G loss: 1.000068]\n",
            "909 [D loss: 0.999961] [G loss: 1.000059]\n",
            "910 [D loss: 0.999976] [G loss: 1.000061]\n",
            "911 [D loss: 0.999968] [G loss: 1.000064]\n",
            "912 [D loss: 0.999970] [G loss: 1.000063]\n",
            "913 [D loss: 0.999972] [G loss: 1.000067]\n",
            "914 [D loss: 0.999969] [G loss: 1.000063]\n",
            "915 [D loss: 0.999972] [G loss: 1.000065]\n",
            "916 [D loss: 0.999971] [G loss: 1.000066]\n",
            "917 [D loss: 0.999966] [G loss: 1.000070]\n",
            "918 [D loss: 0.999967] [G loss: 1.000068]\n",
            "919 [D loss: 0.999972] [G loss: 1.000067]\n",
            "920 [D loss: 0.999966] [G loss: 1.000060]\n",
            "921 [D loss: 0.999969] [G loss: 1.000066]\n",
            "922 [D loss: 0.999978] [G loss: 1.000056]\n",
            "923 [D loss: 0.999967] [G loss: 1.000071]\n",
            "924 [D loss: 0.999973] [G loss: 1.000043]\n",
            "925 [D loss: 0.999970] [G loss: 1.000057]\n",
            "926 [D loss: 0.999967] [G loss: 1.000065]\n",
            "927 [D loss: 0.999971] [G loss: 1.000078]\n",
            "928 [D loss: 0.999968] [G loss: 1.000070]\n",
            "929 [D loss: 0.999969] [G loss: 1.000059]\n",
            "930 [D loss: 0.999976] [G loss: 1.000049]\n",
            "931 [D loss: 0.999969] [G loss: 1.000061]\n",
            "932 [D loss: 0.999975] [G loss: 1.000053]\n",
            "933 [D loss: 0.999962] [G loss: 1.000073]\n",
            "934 [D loss: 0.999982] [G loss: 1.000067]\n",
            "935 [D loss: 0.999976] [G loss: 1.000061]\n",
            "936 [D loss: 0.999983] [G loss: 1.000055]\n",
            "937 [D loss: 0.999968] [G loss: 1.000059]\n",
            "938 [D loss: 0.999969] [G loss: 1.000066]\n",
            "939 [D loss: 0.999967] [G loss: 1.000064]\n",
            "940 [D loss: 0.999964] [G loss: 1.000062]\n",
            "941 [D loss: 0.999974] [G loss: 1.000077]\n",
            "942 [D loss: 0.999979] [G loss: 1.000045]\n",
            "943 [D loss: 0.999967] [G loss: 1.000057]\n",
            "944 [D loss: 0.999975] [G loss: 1.000055]\n",
            "945 [D loss: 0.999967] [G loss: 1.000056]\n",
            "946 [D loss: 0.999968] [G loss: 1.000054]\n",
            "947 [D loss: 0.999963] [G loss: 1.000074]\n",
            "948 [D loss: 0.999971] [G loss: 1.000069]\n",
            "949 [D loss: 0.999969] [G loss: 1.000054]\n",
            "950 [D loss: 0.999977] [G loss: 1.000055]\n",
            "951 [D loss: 0.999969] [G loss: 1.000062]\n",
            "952 [D loss: 0.999970] [G loss: 1.000064]\n",
            "953 [D loss: 0.999971] [G loss: 1.000077]\n",
            "954 [D loss: 0.999971] [G loss: 1.000066]\n",
            "955 [D loss: 0.999972] [G loss: 1.000071]\n",
            "956 [D loss: 0.999960] [G loss: 1.000065]\n",
            "957 [D loss: 0.999970] [G loss: 1.000061]\n",
            "958 [D loss: 0.999965] [G loss: 1.000068]\n",
            "959 [D loss: 0.999971] [G loss: 1.000071]\n",
            "960 [D loss: 0.999967] [G loss: 1.000065]\n",
            "961 [D loss: 0.999967] [G loss: 1.000066]\n",
            "962 [D loss: 0.999969] [G loss: 1.000066]\n",
            "963 [D loss: 0.999967] [G loss: 1.000065]\n",
            "964 [D loss: 0.999970] [G loss: 1.000066]\n",
            "965 [D loss: 0.999972] [G loss: 1.000060]\n",
            "966 [D loss: 0.999970] [G loss: 1.000055]\n",
            "967 [D loss: 0.999967] [G loss: 1.000058]\n",
            "968 [D loss: 0.999968] [G loss: 1.000063]\n",
            "969 [D loss: 0.999970] [G loss: 1.000057]\n",
            "970 [D loss: 0.999972] [G loss: 1.000063]\n",
            "971 [D loss: 0.999975] [G loss: 1.000063]\n",
            "972 [D loss: 0.999974] [G loss: 1.000060]\n",
            "973 [D loss: 0.999972] [G loss: 1.000066]\n",
            "974 [D loss: 0.999956] [G loss: 1.000072]\n",
            "975 [D loss: 0.999958] [G loss: 1.000069]\n",
            "976 [D loss: 0.999969] [G loss: 1.000066]\n",
            "977 [D loss: 0.999974] [G loss: 1.000057]\n",
            "978 [D loss: 0.999974] [G loss: 1.000072]\n",
            "979 [D loss: 0.999970] [G loss: 1.000069]\n",
            "980 [D loss: 0.999968] [G loss: 1.000059]\n",
            "981 [D loss: 0.999971] [G loss: 1.000062]\n",
            "982 [D loss: 0.999970] [G loss: 1.000061]\n",
            "983 [D loss: 0.999972] [G loss: 1.000065]\n",
            "984 [D loss: 0.999967] [G loss: 1.000067]\n",
            "985 [D loss: 0.999963] [G loss: 1.000065]\n",
            "986 [D loss: 0.999973] [G loss: 1.000058]\n",
            "987 [D loss: 0.999970] [G loss: 1.000076]\n",
            "988 [D loss: 0.999976] [G loss: 1.000063]\n",
            "989 [D loss: 0.999969] [G loss: 1.000068]\n",
            "990 [D loss: 0.999959] [G loss: 1.000072]\n",
            "991 [D loss: 0.999972] [G loss: 1.000073]\n",
            "992 [D loss: 0.999975] [G loss: 1.000055]\n",
            "993 [D loss: 0.999967] [G loss: 1.000062]\n",
            "994 [D loss: 0.999968] [G loss: 1.000062]\n",
            "995 [D loss: 0.999965] [G loss: 1.000059]\n",
            "996 [D loss: 0.999966] [G loss: 1.000072]\n",
            "997 [D loss: 0.999974] [G loss: 1.000064]\n",
            "998 [D loss: 0.999969] [G loss: 1.000071]\n",
            "999 [D loss: 0.999969] [G loss: 1.000053]\n",
            "1000 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1001 [D loss: 0.999973] [G loss: 1.000068]\n",
            "1002 [D loss: 0.999967] [G loss: 1.000072]\n",
            "1003 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1004 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1005 [D loss: 0.999971] [G loss: 1.000048]\n",
            "1006 [D loss: 0.999971] [G loss: 1.000055]\n",
            "1007 [D loss: 0.999981] [G loss: 1.000059]\n",
            "1008 [D loss: 0.999966] [G loss: 1.000075]\n",
            "1009 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1010 [D loss: 0.999976] [G loss: 1.000056]\n",
            "1011 [D loss: 0.999985] [G loss: 1.000051]\n",
            "1012 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1013 [D loss: 0.999980] [G loss: 1.000063]\n",
            "1014 [D loss: 0.999964] [G loss: 1.000060]\n",
            "1015 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1016 [D loss: 0.999977] [G loss: 1.000065]\n",
            "1017 [D loss: 0.999959] [G loss: 1.000083]\n",
            "1018 [D loss: 0.999962] [G loss: 1.000051]\n",
            "1019 [D loss: 0.999974] [G loss: 1.000065]\n",
            "1020 [D loss: 0.999977] [G loss: 1.000074]\n",
            "1021 [D loss: 0.999983] [G loss: 1.000059]\n",
            "1022 [D loss: 0.999976] [G loss: 1.000059]\n",
            "1023 [D loss: 0.999980] [G loss: 1.000067]\n",
            "1024 [D loss: 0.999961] [G loss: 1.000065]\n",
            "1025 [D loss: 0.999982] [G loss: 1.000044]\n",
            "1026 [D loss: 0.999959] [G loss: 1.000069]\n",
            "1027 [D loss: 0.999971] [G loss: 1.000052]\n",
            "1028 [D loss: 0.999976] [G loss: 1.000071]\n",
            "1029 [D loss: 0.999977] [G loss: 1.000083]\n",
            "1030 [D loss: 0.999982] [G loss: 1.000052]\n",
            "1031 [D loss: 0.999961] [G loss: 1.000064]\n",
            "1032 [D loss: 0.999969] [G loss: 1.000075]\n",
            "1033 [D loss: 0.999961] [G loss: 1.000095]\n",
            "1034 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1035 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1036 [D loss: 0.999962] [G loss: 1.000062]\n",
            "1037 [D loss: 0.999977] [G loss: 1.000065]\n",
            "1038 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1039 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1041 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1042 [D loss: 0.999962] [G loss: 1.000064]\n",
            "1043 [D loss: 0.999961] [G loss: 1.000063]\n",
            "1044 [D loss: 0.999962] [G loss: 1.000065]\n",
            "1045 [D loss: 0.999969] [G loss: 1.000047]\n",
            "1046 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1047 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1048 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1049 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1050 [D loss: 0.999965] [G loss: 1.000068]\n",
            "1051 [D loss: 0.999970] [G loss: 1.000056]\n",
            "1052 [D loss: 0.999975] [G loss: 1.000050]\n",
            "1053 [D loss: 0.999975] [G loss: 1.000064]\n",
            "1054 [D loss: 0.999977] [G loss: 1.000060]\n",
            "1055 [D loss: 0.999972] [G loss: 1.000051]\n",
            "1056 [D loss: 0.999978] [G loss: 1.000051]\n",
            "1057 [D loss: 0.999961] [G loss: 1.000069]\n",
            "1058 [D loss: 0.999974] [G loss: 1.000055]\n",
            "1059 [D loss: 0.999967] [G loss: 1.000072]\n",
            "1060 [D loss: 0.999965] [G loss: 1.000067]\n",
            "1061 [D loss: 0.999977] [G loss: 1.000056]\n",
            "1062 [D loss: 0.999969] [G loss: 1.000054]\n",
            "1063 [D loss: 0.999976] [G loss: 1.000067]\n",
            "1064 [D loss: 0.999965] [G loss: 1.000063]\n",
            "1065 [D loss: 0.999964] [G loss: 1.000051]\n",
            "1066 [D loss: 0.999969] [G loss: 1.000073]\n",
            "1067 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1068 [D loss: 0.999963] [G loss: 1.000062]\n",
            "1069 [D loss: 0.999975] [G loss: 1.000070]\n",
            "1070 [D loss: 0.999962] [G loss: 1.000068]\n",
            "1071 [D loss: 0.999965] [G loss: 1.000073]\n",
            "1072 [D loss: 0.999974] [G loss: 1.000069]\n",
            "1073 [D loss: 0.999975] [G loss: 1.000057]\n",
            "1074 [D loss: 0.999972] [G loss: 1.000040]\n",
            "1075 [D loss: 0.999979] [G loss: 1.000061]\n",
            "1076 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1077 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1078 [D loss: 0.999970] [G loss: 1.000075]\n",
            "1079 [D loss: 0.999975] [G loss: 1.000055]\n",
            "1080 [D loss: 0.999963] [G loss: 1.000064]\n",
            "1081 [D loss: 0.999959] [G loss: 1.000057]\n",
            "1082 [D loss: 0.999984] [G loss: 1.000058]\n",
            "1083 [D loss: 0.999963] [G loss: 1.000058]\n",
            "1084 [D loss: 0.999958] [G loss: 1.000059]\n",
            "1085 [D loss: 0.999972] [G loss: 1.000092]\n",
            "1086 [D loss: 0.999968] [G loss: 1.000069]\n",
            "1087 [D loss: 0.999965] [G loss: 1.000058]\n",
            "1088 [D loss: 0.999975] [G loss: 1.000064]\n",
            "1089 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1090 [D loss: 0.999976] [G loss: 1.000048]\n",
            "1091 [D loss: 0.999965] [G loss: 1.000068]\n",
            "1092 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1093 [D loss: 0.999963] [G loss: 1.000068]\n",
            "1094 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1095 [D loss: 0.999977] [G loss: 1.000066]\n",
            "1096 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1097 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1098 [D loss: 0.999965] [G loss: 1.000046]\n",
            "1099 [D loss: 0.999965] [G loss: 1.000070]\n",
            "1100 [D loss: 0.999975] [G loss: 1.000064]\n",
            "1101 [D loss: 0.999968] [G loss: 1.000050]\n",
            "1102 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1103 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1104 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1105 [D loss: 0.999973] [G loss: 1.000055]\n",
            "1106 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1107 [D loss: 0.999975] [G loss: 1.000060]\n",
            "1108 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1109 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1110 [D loss: 0.999974] [G loss: 1.000055]\n",
            "1111 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1112 [D loss: 0.999974] [G loss: 1.000072]\n",
            "1113 [D loss: 0.999963] [G loss: 1.000072]\n",
            "1114 [D loss: 0.999977] [G loss: 1.000064]\n",
            "1115 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1116 [D loss: 0.999966] [G loss: 1.000077]\n",
            "1117 [D loss: 0.999972] [G loss: 1.000072]\n",
            "1118 [D loss: 0.999958] [G loss: 1.000058]\n",
            "1119 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1120 [D loss: 0.999965] [G loss: 1.000077]\n",
            "1121 [D loss: 0.999965] [G loss: 1.000073]\n",
            "1122 [D loss: 0.999966] [G loss: 1.000061]\n",
            "1123 [D loss: 0.999962] [G loss: 1.000049]\n",
            "1124 [D loss: 0.999964] [G loss: 1.000068]\n",
            "1125 [D loss: 0.999975] [G loss: 1.000065]\n",
            "1126 [D loss: 0.999965] [G loss: 1.000058]\n",
            "1127 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1128 [D loss: 0.999964] [G loss: 1.000062]\n",
            "1129 [D loss: 0.999975] [G loss: 1.000056]\n",
            "1130 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1131 [D loss: 0.999968] [G loss: 1.000076]\n",
            "1132 [D loss: 0.999974] [G loss: 1.000065]\n",
            "1133 [D loss: 0.999975] [G loss: 1.000075]\n",
            "1134 [D loss: 0.999964] [G loss: 1.000076]\n",
            "1135 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1136 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1137 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1138 [D loss: 0.999965] [G loss: 1.000074]\n",
            "1139 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1140 [D loss: 0.999970] [G loss: 1.000056]\n",
            "1141 [D loss: 0.999972] [G loss: 1.000068]\n",
            "1142 [D loss: 0.999971] [G loss: 1.000071]\n",
            "1143 [D loss: 0.999974] [G loss: 1.000066]\n",
            "1144 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1145 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1146 [D loss: 0.999971] [G loss: 1.000068]\n",
            "1147 [D loss: 0.999974] [G loss: 1.000070]\n",
            "1148 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1149 [D loss: 0.999969] [G loss: 1.000071]\n",
            "1150 [D loss: 0.999966] [G loss: 1.000066]\n",
            "1151 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1152 [D loss: 0.999970] [G loss: 1.000069]\n",
            "1153 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1154 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1155 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1156 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1157 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1158 [D loss: 0.999971] [G loss: 1.000049]\n",
            "1159 [D loss: 0.999966] [G loss: 1.000067]\n",
            "1160 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1161 [D loss: 0.999974] [G loss: 1.000068]\n",
            "1162 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1163 [D loss: 0.999975] [G loss: 1.000063]\n",
            "1164 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1165 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1166 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1167 [D loss: 0.999963] [G loss: 1.000065]\n",
            "1168 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1169 [D loss: 0.999971] [G loss: 1.000071]\n",
            "1170 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1171 [D loss: 0.999975] [G loss: 1.000073]\n",
            "1172 [D loss: 0.999969] [G loss: 1.000057]\n",
            "1173 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1174 [D loss: 0.999972] [G loss: 1.000069]\n",
            "1175 [D loss: 0.999961] [G loss: 1.000061]\n",
            "1176 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1177 [D loss: 0.999969] [G loss: 1.000043]\n",
            "1178 [D loss: 0.999963] [G loss: 1.000071]\n",
            "1179 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1180 [D loss: 0.999973] [G loss: 1.000053]\n",
            "1181 [D loss: 0.999968] [G loss: 1.000072]\n",
            "1182 [D loss: 0.999972] [G loss: 1.000070]\n",
            "1183 [D loss: 0.999980] [G loss: 1.000064]\n",
            "1184 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1185 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1186 [D loss: 0.999982] [G loss: 1.000060]\n",
            "1187 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1188 [D loss: 0.999973] [G loss: 1.000064]\n",
            "1189 [D loss: 0.999967] [G loss: 1.000059]\n",
            "1190 [D loss: 0.999978] [G loss: 1.000061]\n",
            "1191 [D loss: 0.999974] [G loss: 1.000079]\n",
            "1192 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1193 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1194 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1195 [D loss: 0.999970] [G loss: 1.000079]\n",
            "1196 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1197 [D loss: 0.999964] [G loss: 1.000067]\n",
            "1198 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1199 [D loss: 0.999966] [G loss: 1.000069]\n",
            "1200 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1201 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1202 [D loss: 0.999980] [G loss: 1.000065]\n",
            "1203 [D loss: 0.999964] [G loss: 1.000063]\n",
            "1204 [D loss: 0.999968] [G loss: 1.000053]\n",
            "1205 [D loss: 0.999974] [G loss: 1.000063]\n",
            "1206 [D loss: 0.999980] [G loss: 1.000071]\n",
            "1207 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1208 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1209 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1210 [D loss: 0.999970] [G loss: 1.000075]\n",
            "1211 [D loss: 0.999977] [G loss: 1.000057]\n",
            "1212 [D loss: 0.999970] [G loss: 1.000069]\n",
            "1213 [D loss: 0.999974] [G loss: 1.000070]\n",
            "1214 [D loss: 0.999976] [G loss: 1.000079]\n",
            "1215 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1216 [D loss: 0.999967] [G loss: 1.000081]\n",
            "1217 [D loss: 0.999977] [G loss: 1.000058]\n",
            "1218 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1219 [D loss: 0.999976] [G loss: 1.000074]\n",
            "1220 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1221 [D loss: 0.999973] [G loss: 1.000056]\n",
            "1222 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1223 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1224 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1225 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1226 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1227 [D loss: 0.999960] [G loss: 1.000059]\n",
            "1228 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1229 [D loss: 0.999972] [G loss: 1.000050]\n",
            "1230 [D loss: 0.999967] [G loss: 1.000060]\n",
            "1231 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1232 [D loss: 0.999964] [G loss: 1.000051]\n",
            "1233 [D loss: 0.999973] [G loss: 1.000060]\n",
            "1234 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1235 [D loss: 0.999967] [G loss: 1.000047]\n",
            "1236 [D loss: 0.999971] [G loss: 1.000048]\n",
            "1237 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1238 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1239 [D loss: 0.999975] [G loss: 1.000067]\n",
            "1240 [D loss: 0.999982] [G loss: 1.000054]\n",
            "1241 [D loss: 0.999973] [G loss: 1.000053]\n",
            "1242 [D loss: 0.999956] [G loss: 1.000064]\n",
            "1243 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1244 [D loss: 0.999966] [G loss: 1.000053]\n",
            "1245 [D loss: 0.999975] [G loss: 1.000060]\n",
            "1246 [D loss: 0.999977] [G loss: 1.000061]\n",
            "1247 [D loss: 0.999963] [G loss: 1.000053]\n",
            "1248 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1249 [D loss: 0.999969] [G loss: 1.000055]\n",
            "1250 [D loss: 0.999976] [G loss: 1.000060]\n",
            "1251 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1252 [D loss: 0.999970] [G loss: 1.000074]\n",
            "1253 [D loss: 0.999965] [G loss: 1.000069]\n",
            "1254 [D loss: 0.999974] [G loss: 1.000071]\n",
            "1255 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1256 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1257 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1258 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1259 [D loss: 0.999961] [G loss: 1.000058]\n",
            "1260 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1261 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1262 [D loss: 0.999976] [G loss: 1.000053]\n",
            "1263 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1264 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1265 [D loss: 0.999974] [G loss: 1.000068]\n",
            "1266 [D loss: 0.999968] [G loss: 1.000076]\n",
            "1267 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1268 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1269 [D loss: 0.999967] [G loss: 1.000068]\n",
            "1270 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1271 [D loss: 0.999962] [G loss: 1.000065]\n",
            "1272 [D loss: 0.999974] [G loss: 1.000070]\n",
            "1273 [D loss: 0.999971] [G loss: 1.000070]\n",
            "1274 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1275 [D loss: 0.999967] [G loss: 1.000057]\n",
            "1276 [D loss: 0.999969] [G loss: 1.000055]\n",
            "1277 [D loss: 0.999971] [G loss: 1.000051]\n",
            "1278 [D loss: 0.999976] [G loss: 1.000068]\n",
            "1279 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1280 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1281 [D loss: 0.999976] [G loss: 1.000054]\n",
            "1282 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1283 [D loss: 0.999965] [G loss: 1.000051]\n",
            "1284 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1285 [D loss: 0.999965] [G loss: 1.000052]\n",
            "1286 [D loss: 0.999976] [G loss: 1.000066]\n",
            "1287 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1288 [D loss: 0.999975] [G loss: 1.000065]\n",
            "1289 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1290 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1291 [D loss: 0.999964] [G loss: 1.000052]\n",
            "1292 [D loss: 0.999977] [G loss: 1.000057]\n",
            "1293 [D loss: 0.999964] [G loss: 1.000069]\n",
            "1294 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1295 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1296 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1297 [D loss: 0.999975] [G loss: 1.000054]\n",
            "1298 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1299 [D loss: 0.999971] [G loss: 1.000049]\n",
            "1300 [D loss: 0.999974] [G loss: 1.000061]\n",
            "1301 [D loss: 0.999972] [G loss: 1.000056]\n",
            "1302 [D loss: 0.999976] [G loss: 1.000073]\n",
            "1303 [D loss: 0.999964] [G loss: 1.000058]\n",
            "1304 [D loss: 0.999978] [G loss: 1.000057]\n",
            "1305 [D loss: 0.999974] [G loss: 1.000070]\n",
            "1306 [D loss: 0.999970] [G loss: 1.000056]\n",
            "1307 [D loss: 0.999959] [G loss: 1.000065]\n",
            "1308 [D loss: 0.999961] [G loss: 1.000070]\n",
            "1309 [D loss: 0.999962] [G loss: 1.000062]\n",
            "1310 [D loss: 0.999964] [G loss: 1.000071]\n",
            "1311 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1312 [D loss: 0.999968] [G loss: 1.000053]\n",
            "1313 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1314 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1315 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1316 [D loss: 0.999973] [G loss: 1.000066]\n",
            "1317 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1318 [D loss: 0.999976] [G loss: 1.000063]\n",
            "1319 [D loss: 0.999979] [G loss: 1.000057]\n",
            "1320 [D loss: 0.999972] [G loss: 1.000049]\n",
            "1321 [D loss: 0.999965] [G loss: 1.000055]\n",
            "1322 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1323 [D loss: 0.999977] [G loss: 1.000058]\n",
            "1324 [D loss: 0.999981] [G loss: 1.000066]\n",
            "1325 [D loss: 0.999964] [G loss: 1.000062]\n",
            "1326 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1327 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1328 [D loss: 0.999975] [G loss: 1.000061]\n",
            "1329 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1330 [D loss: 0.999964] [G loss: 1.000061]\n",
            "1331 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1332 [D loss: 0.999962] [G loss: 1.000065]\n",
            "1333 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1334 [D loss: 0.999974] [G loss: 1.000075]\n",
            "1335 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1336 [D loss: 0.999965] [G loss: 1.000067]\n",
            "1337 [D loss: 0.999969] [G loss: 1.000079]\n",
            "1338 [D loss: 0.999969] [G loss: 1.000070]\n",
            "1339 [D loss: 0.999976] [G loss: 1.000063]\n",
            "1340 [D loss: 0.999968] [G loss: 1.000054]\n",
            "1341 [D loss: 0.999977] [G loss: 1.000051]\n",
            "1342 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1343 [D loss: 0.999968] [G loss: 1.000074]\n",
            "1344 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1345 [D loss: 0.999972] [G loss: 1.000069]\n",
            "1346 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1347 [D loss: 0.999977] [G loss: 1.000064]\n",
            "1348 [D loss: 0.999976] [G loss: 1.000058]\n",
            "1349 [D loss: 0.999960] [G loss: 1.000059]\n",
            "1350 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1351 [D loss: 0.999974] [G loss: 1.000060]\n",
            "1352 [D loss: 0.999972] [G loss: 1.000073]\n",
            "1353 [D loss: 0.999964] [G loss: 1.000072]\n",
            "1354 [D loss: 0.999967] [G loss: 1.000056]\n",
            "1355 [D loss: 0.999973] [G loss: 1.000059]\n",
            "1356 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1357 [D loss: 0.999976] [G loss: 1.000071]\n",
            "1358 [D loss: 0.999968] [G loss: 1.000069]\n",
            "1359 [D loss: 0.999969] [G loss: 1.000071]\n",
            "1360 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1361 [D loss: 0.999974] [G loss: 1.000068]\n",
            "1362 [D loss: 0.999974] [G loss: 1.000071]\n",
            "1363 [D loss: 0.999968] [G loss: 1.000069]\n",
            "1364 [D loss: 0.999974] [G loss: 1.000067]\n",
            "1365 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1366 [D loss: 0.999964] [G loss: 1.000060]\n",
            "1367 [D loss: 0.999969] [G loss: 1.000069]\n",
            "1368 [D loss: 0.999968] [G loss: 1.000073]\n",
            "1369 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1370 [D loss: 0.999967] [G loss: 1.000076]\n",
            "1371 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1372 [D loss: 0.999974] [G loss: 1.000061]\n",
            "1373 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1374 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1375 [D loss: 0.999961] [G loss: 1.000056]\n",
            "1376 [D loss: 0.999967] [G loss: 1.000073]\n",
            "1377 [D loss: 0.999967] [G loss: 1.000070]\n",
            "1378 [D loss: 0.999963] [G loss: 1.000053]\n",
            "1379 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1381 [D loss: 0.999974] [G loss: 1.000053]\n",
            "1382 [D loss: 0.999970] [G loss: 1.000052]\n",
            "1383 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1384 [D loss: 0.999979] [G loss: 1.000066]\n",
            "1385 [D loss: 0.999965] [G loss: 1.000068]\n",
            "1386 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1387 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1388 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1389 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1390 [D loss: 0.999970] [G loss: 1.000075]\n",
            "1391 [D loss: 0.999974] [G loss: 1.000078]\n",
            "1392 [D loss: 0.999973] [G loss: 1.000054]\n",
            "1393 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1394 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1395 [D loss: 0.999973] [G loss: 1.000053]\n",
            "1396 [D loss: 0.999977] [G loss: 1.000066]\n",
            "1397 [D loss: 0.999966] [G loss: 1.000062]\n",
            "1398 [D loss: 0.999968] [G loss: 1.000082]\n",
            "1399 [D loss: 0.999962] [G loss: 1.000064]\n",
            "1400 [D loss: 0.999966] [G loss: 1.000078]\n",
            "1401 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1402 [D loss: 0.999977] [G loss: 1.000054]\n",
            "1403 [D loss: 0.999978] [G loss: 1.000060]\n",
            "1404 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1405 [D loss: 0.999967] [G loss: 1.000074]\n",
            "1406 [D loss: 0.999975] [G loss: 1.000067]\n",
            "1407 [D loss: 0.999961] [G loss: 1.000059]\n",
            "1408 [D loss: 0.999966] [G loss: 1.000067]\n",
            "1409 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1410 [D loss: 0.999964] [G loss: 1.000058]\n",
            "1411 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1412 [D loss: 0.999973] [G loss: 1.000066]\n",
            "1413 [D loss: 0.999973] [G loss: 1.000060]\n",
            "1414 [D loss: 0.999962] [G loss: 1.000070]\n",
            "1415 [D loss: 0.999973] [G loss: 1.000049]\n",
            "1416 [D loss: 0.999970] [G loss: 1.000056]\n",
            "1417 [D loss: 0.999973] [G loss: 1.000064]\n",
            "1418 [D loss: 0.999966] [G loss: 1.000056]\n",
            "1419 [D loss: 0.999965] [G loss: 1.000056]\n",
            "1420 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1421 [D loss: 0.999968] [G loss: 1.000068]\n",
            "1422 [D loss: 0.999963] [G loss: 1.000065]\n",
            "1423 [D loss: 0.999975] [G loss: 1.000078]\n",
            "1424 [D loss: 0.999973] [G loss: 1.000070]\n",
            "1425 [D loss: 0.999975] [G loss: 1.000070]\n",
            "1426 [D loss: 0.999965] [G loss: 1.000061]\n",
            "1427 [D loss: 0.999963] [G loss: 1.000061]\n",
            "1428 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1429 [D loss: 0.999974] [G loss: 1.000058]\n",
            "1430 [D loss: 0.999971] [G loss: 1.000050]\n",
            "1431 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1432 [D loss: 0.999967] [G loss: 1.000071]\n",
            "1433 [D loss: 0.999970] [G loss: 1.000052]\n",
            "1434 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1435 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1436 [D loss: 0.999960] [G loss: 1.000064]\n",
            "1437 [D loss: 0.999962] [G loss: 1.000066]\n",
            "1438 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1439 [D loss: 0.999967] [G loss: 1.000058]\n",
            "1440 [D loss: 0.999961] [G loss: 1.000061]\n",
            "1441 [D loss: 0.999970] [G loss: 1.000043]\n",
            "1442 [D loss: 0.999974] [G loss: 1.000046]\n",
            "1443 [D loss: 0.999958] [G loss: 1.000063]\n",
            "1444 [D loss: 0.999962] [G loss: 1.000059]\n",
            "1445 [D loss: 0.999978] [G loss: 1.000063]\n",
            "1446 [D loss: 0.999978] [G loss: 1.000053]\n",
            "1447 [D loss: 0.999964] [G loss: 1.000042]\n",
            "1448 [D loss: 0.999966] [G loss: 1.000056]\n",
            "1449 [D loss: 0.999959] [G loss: 1.000072]\n",
            "1450 [D loss: 0.999981] [G loss: 1.000066]\n",
            "1451 [D loss: 0.999959] [G loss: 1.000061]\n",
            "1452 [D loss: 0.999979] [G loss: 1.000056]\n",
            "1453 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1454 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1455 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1456 [D loss: 0.999977] [G loss: 1.000074]\n",
            "1457 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1458 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1459 [D loss: 0.999973] [G loss: 1.000045]\n",
            "1460 [D loss: 0.999974] [G loss: 1.000057]\n",
            "1461 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1462 [D loss: 0.999963] [G loss: 1.000047]\n",
            "1463 [D loss: 0.999974] [G loss: 1.000058]\n",
            "1464 [D loss: 0.999966] [G loss: 1.000056]\n",
            "1465 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1466 [D loss: 0.999965] [G loss: 1.000059]\n",
            "1467 [D loss: 0.999965] [G loss: 1.000055]\n",
            "1468 [D loss: 0.999968] [G loss: 1.000069]\n",
            "1469 [D loss: 0.999974] [G loss: 1.000051]\n",
            "1470 [D loss: 0.999969] [G loss: 1.000069]\n",
            "1471 [D loss: 0.999964] [G loss: 1.000070]\n",
            "1472 [D loss: 0.999963] [G loss: 1.000059]\n",
            "1473 [D loss: 0.999968] [G loss: 1.000052]\n",
            "1474 [D loss: 0.999960] [G loss: 1.000049]\n",
            "1475 [D loss: 0.999970] [G loss: 1.000069]\n",
            "1476 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1477 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1478 [D loss: 0.999965] [G loss: 1.000057]\n",
            "1479 [D loss: 0.999970] [G loss: 1.000070]\n",
            "1480 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1481 [D loss: 0.999978] [G loss: 1.000069]\n",
            "1482 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1483 [D loss: 0.999974] [G loss: 1.000057]\n",
            "1484 [D loss: 0.999965] [G loss: 1.000056]\n",
            "1485 [D loss: 0.999975] [G loss: 1.000055]\n",
            "1486 [D loss: 0.999969] [G loss: 1.000071]\n",
            "1487 [D loss: 0.999972] [G loss: 1.000049]\n",
            "1488 [D loss: 0.999981] [G loss: 1.000064]\n",
            "1489 [D loss: 0.999963] [G loss: 1.000062]\n",
            "1490 [D loss: 0.999964] [G loss: 1.000065]\n",
            "1491 [D loss: 0.999972] [G loss: 1.000068]\n",
            "1492 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1493 [D loss: 0.999976] [G loss: 1.000086]\n",
            "1494 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1495 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1496 [D loss: 0.999973] [G loss: 1.000055]\n",
            "1497 [D loss: 0.999967] [G loss: 1.000070]\n",
            "1498 [D loss: 0.999973] [G loss: 1.000065]\n",
            "1499 [D loss: 0.999975] [G loss: 1.000056]\n",
            "1500 [D loss: 0.999968] [G loss: 1.000056]\n",
            "1501 [D loss: 0.999973] [G loss: 1.000054]\n",
            "1502 [D loss: 0.999976] [G loss: 1.000063]\n",
            "1503 [D loss: 0.999961] [G loss: 1.000062]\n",
            "1504 [D loss: 0.999966] [G loss: 1.000058]\n",
            "1505 [D loss: 0.999966] [G loss: 1.000059]\n",
            "1506 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1507 [D loss: 0.999962] [G loss: 1.000059]\n",
            "1508 [D loss: 0.999975] [G loss: 1.000061]\n",
            "1509 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1510 [D loss: 0.999974] [G loss: 1.000068]\n",
            "1511 [D loss: 0.999970] [G loss: 1.000069]\n",
            "1512 [D loss: 0.999982] [G loss: 1.000065]\n",
            "1513 [D loss: 0.999974] [G loss: 1.000071]\n",
            "1514 [D loss: 0.999971] [G loss: 1.000054]\n",
            "1515 [D loss: 0.999973] [G loss: 1.000073]\n",
            "1516 [D loss: 0.999977] [G loss: 1.000062]\n",
            "1517 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1518 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1519 [D loss: 0.999966] [G loss: 1.000058]\n",
            "1520 [D loss: 0.999979] [G loss: 1.000060]\n",
            "1521 [D loss: 0.999979] [G loss: 1.000072]\n",
            "1522 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1523 [D loss: 0.999964] [G loss: 1.000063]\n",
            "1524 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1525 [D loss: 0.999963] [G loss: 1.000054]\n",
            "1526 [D loss: 0.999973] [G loss: 1.000071]\n",
            "1527 [D loss: 0.999974] [G loss: 1.000057]\n",
            "1528 [D loss: 0.999969] [G loss: 1.000054]\n",
            "1529 [D loss: 0.999965] [G loss: 1.000067]\n",
            "1530 [D loss: 0.999966] [G loss: 1.000049]\n",
            "1531 [D loss: 0.999969] [G loss: 1.000048]\n",
            "1532 [D loss: 0.999975] [G loss: 1.000069]\n",
            "1533 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1534 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1535 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1536 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1537 [D loss: 0.999978] [G loss: 1.000053]\n",
            "1538 [D loss: 0.999977] [G loss: 1.000059]\n",
            "1539 [D loss: 0.999965] [G loss: 1.000066]\n",
            "1540 [D loss: 0.999962] [G loss: 1.000062]\n",
            "1541 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1542 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1543 [D loss: 0.999969] [G loss: 1.000051]\n",
            "1544 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1545 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1546 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1547 [D loss: 0.999976] [G loss: 1.000057]\n",
            "1548 [D loss: 0.999971] [G loss: 1.000071]\n",
            "1549 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1550 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1551 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1552 [D loss: 0.999975] [G loss: 1.000067]\n",
            "1553 [D loss: 0.999959] [G loss: 1.000065]\n",
            "1554 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1555 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1556 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1557 [D loss: 0.999978] [G loss: 1.000057]\n",
            "1558 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1559 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1560 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1561 [D loss: 0.999976] [G loss: 1.000076]\n",
            "1562 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1563 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1564 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1565 [D loss: 0.999966] [G loss: 1.000058]\n",
            "1566 [D loss: 0.999960] [G loss: 1.000047]\n",
            "1567 [D loss: 0.999967] [G loss: 1.000053]\n",
            "1568 [D loss: 0.999978] [G loss: 1.000055]\n",
            "1569 [D loss: 0.999975] [G loss: 1.000064]\n",
            "1570 [D loss: 0.999965] [G loss: 1.000058]\n",
            "1571 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1572 [D loss: 0.999977] [G loss: 1.000072]\n",
            "1573 [D loss: 0.999969] [G loss: 1.000072]\n",
            "1574 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1575 [D loss: 0.999962] [G loss: 1.000055]\n",
            "1576 [D loss: 0.999965] [G loss: 1.000061]\n",
            "1577 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1578 [D loss: 0.999969] [G loss: 1.000076]\n",
            "1579 [D loss: 0.999973] [G loss: 1.000069]\n",
            "1580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1581 [D loss: 0.999964] [G loss: 1.000068]\n",
            "1582 [D loss: 0.999960] [G loss: 1.000067]\n",
            "1583 [D loss: 0.999962] [G loss: 1.000057]\n",
            "1584 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1585 [D loss: 0.999965] [G loss: 1.000058]\n",
            "1586 [D loss: 0.999966] [G loss: 1.000059]\n",
            "1587 [D loss: 0.999969] [G loss: 1.000072]\n",
            "1588 [D loss: 0.999981] [G loss: 1.000061]\n",
            "1589 [D loss: 0.999967] [G loss: 1.000073]\n",
            "1590 [D loss: 0.999975] [G loss: 1.000052]\n",
            "1591 [D loss: 0.999987] [G loss: 1.000053]\n",
            "1592 [D loss: 0.999976] [G loss: 1.000068]\n",
            "1593 [D loss: 0.999965] [G loss: 1.000045]\n",
            "1594 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1595 [D loss: 0.999973] [G loss: 1.000078]\n",
            "1596 [D loss: 0.999966] [G loss: 1.000073]\n",
            "1597 [D loss: 0.999975] [G loss: 1.000058]\n",
            "1598 [D loss: 0.999971] [G loss: 1.000073]\n",
            "1599 [D loss: 0.999966] [G loss: 1.000068]\n",
            "1600 [D loss: 0.999964] [G loss: 1.000060]\n",
            "1601 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1602 [D loss: 0.999976] [G loss: 1.000052]\n",
            "1603 [D loss: 0.999978] [G loss: 1.000057]\n",
            "1604 [D loss: 0.999968] [G loss: 1.000057]\n",
            "1605 [D loss: 0.999967] [G loss: 1.000044]\n",
            "1606 [D loss: 0.999980] [G loss: 1.000057]\n",
            "1607 [D loss: 0.999962] [G loss: 1.000060]\n",
            "1608 [D loss: 0.999970] [G loss: 1.000070]\n",
            "1609 [D loss: 0.999961] [G loss: 1.000055]\n",
            "1610 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1611 [D loss: 0.999963] [G loss: 1.000054]\n",
            "1612 [D loss: 0.999968] [G loss: 1.000042]\n",
            "1613 [D loss: 0.999973] [G loss: 1.000065]\n",
            "1614 [D loss: 0.999975] [G loss: 1.000056]\n",
            "1615 [D loss: 0.999966] [G loss: 1.000051]\n",
            "1616 [D loss: 0.999973] [G loss: 1.000079]\n",
            "1617 [D loss: 0.999977] [G loss: 1.000064]\n",
            "1618 [D loss: 0.999965] [G loss: 1.000065]\n",
            "1619 [D loss: 0.999975] [G loss: 1.000052]\n",
            "1620 [D loss: 0.999971] [G loss: 1.000054]\n",
            "1621 [D loss: 0.999972] [G loss: 1.000045]\n",
            "1622 [D loss: 0.999961] [G loss: 1.000061]\n",
            "1623 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1624 [D loss: 0.999976] [G loss: 1.000068]\n",
            "1625 [D loss: 0.999969] [G loss: 1.000053]\n",
            "1626 [D loss: 0.999967] [G loss: 1.000057]\n",
            "1627 [D loss: 0.999974] [G loss: 1.000068]\n",
            "1628 [D loss: 0.999969] [G loss: 1.000057]\n",
            "1629 [D loss: 0.999964] [G loss: 1.000066]\n",
            "1630 [D loss: 0.999976] [G loss: 1.000051]\n",
            "1631 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1632 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1633 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1634 [D loss: 0.999973] [G loss: 1.000053]\n",
            "1635 [D loss: 0.999965] [G loss: 1.000059]\n",
            "1636 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1637 [D loss: 0.999966] [G loss: 1.000060]\n",
            "1638 [D loss: 0.999978] [G loss: 1.000059]\n",
            "1639 [D loss: 0.999977] [G loss: 1.000062]\n",
            "1640 [D loss: 0.999965] [G loss: 1.000053]\n",
            "1641 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1642 [D loss: 0.999978] [G loss: 1.000049]\n",
            "1643 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1644 [D loss: 0.999973] [G loss: 1.000059]\n",
            "1645 [D loss: 0.999969] [G loss: 1.000069]\n",
            "1646 [D loss: 0.999966] [G loss: 1.000058]\n",
            "1647 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1648 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1649 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1650 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1651 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1652 [D loss: 0.999974] [G loss: 1.000058]\n",
            "1653 [D loss: 0.999969] [G loss: 1.000048]\n",
            "1654 [D loss: 0.999975] [G loss: 1.000057]\n",
            "1655 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1656 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1657 [D loss: 0.999973] [G loss: 1.000052]\n",
            "1658 [D loss: 0.999963] [G loss: 1.000061]\n",
            "1659 [D loss: 0.999974] [G loss: 1.000055]\n",
            "1660 [D loss: 0.999970] [G loss: 1.000079]\n",
            "1661 [D loss: 0.999970] [G loss: 1.000055]\n",
            "1662 [D loss: 0.999968] [G loss: 1.000068]\n",
            "1663 [D loss: 0.999967] [G loss: 1.000053]\n",
            "1664 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1665 [D loss: 0.999964] [G loss: 1.000056]\n",
            "1666 [D loss: 0.999977] [G loss: 1.000052]\n",
            "1667 [D loss: 0.999967] [G loss: 1.000075]\n",
            "1668 [D loss: 0.999968] [G loss: 1.000051]\n",
            "1669 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1670 [D loss: 0.999970] [G loss: 1.000055]\n",
            "1671 [D loss: 0.999977] [G loss: 1.000068]\n",
            "1672 [D loss: 0.999978] [G loss: 1.000055]\n",
            "1673 [D loss: 0.999972] [G loss: 1.000050]\n",
            "1674 [D loss: 0.999974] [G loss: 1.000065]\n",
            "1675 [D loss: 0.999964] [G loss: 1.000062]\n",
            "1676 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1677 [D loss: 0.999965] [G loss: 1.000052]\n",
            "1678 [D loss: 0.999981] [G loss: 1.000064]\n",
            "1679 [D loss: 0.999972] [G loss: 1.000073]\n",
            "1680 [D loss: 0.999962] [G loss: 1.000055]\n",
            "1681 [D loss: 0.999979] [G loss: 1.000068]\n",
            "1682 [D loss: 0.999965] [G loss: 1.000070]\n",
            "1683 [D loss: 0.999971] [G loss: 1.000052]\n",
            "1684 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1685 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1686 [D loss: 0.999983] [G loss: 1.000052]\n",
            "1687 [D loss: 0.999974] [G loss: 1.000048]\n",
            "1688 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1689 [D loss: 0.999963] [G loss: 1.000029]\n",
            "1690 [D loss: 0.999966] [G loss: 1.000042]\n",
            "1691 [D loss: 0.999974] [G loss: 1.000048]\n",
            "1692 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1693 [D loss: 0.999969] [G loss: 1.000044]\n",
            "1694 [D loss: 0.999970] [G loss: 1.000050]\n",
            "1695 [D loss: 0.999971] [G loss: 1.000047]\n",
            "1696 [D loss: 0.999973] [G loss: 1.000069]\n",
            "1697 [D loss: 0.999957] [G loss: 1.000049]\n",
            "1698 [D loss: 0.999970] [G loss: 1.000040]\n",
            "1699 [D loss: 0.999983] [G loss: 1.000042]\n",
            "1700 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1701 [D loss: 0.999970] [G loss: 1.000074]\n",
            "1702 [D loss: 0.999960] [G loss: 1.000041]\n",
            "1703 [D loss: 0.999963] [G loss: 1.000060]\n",
            "1704 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1705 [D loss: 0.999974] [G loss: 1.000053]\n",
            "1706 [D loss: 0.999966] [G loss: 1.000046]\n",
            "1707 [D loss: 0.999966] [G loss: 1.000049]\n",
            "1708 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1709 [D loss: 0.999978] [G loss: 1.000058]\n",
            "1710 [D loss: 0.999978] [G loss: 1.000050]\n",
            "1711 [D loss: 0.999968] [G loss: 1.000057]\n",
            "1712 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1713 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1714 [D loss: 0.999977] [G loss: 1.000056]\n",
            "1715 [D loss: 0.999970] [G loss: 1.000055]\n",
            "1716 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1717 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1718 [D loss: 0.999969] [G loss: 1.000052]\n",
            "1719 [D loss: 0.999964] [G loss: 1.000054]\n",
            "1720 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1721 [D loss: 0.999980] [G loss: 1.000063]\n",
            "1722 [D loss: 0.999968] [G loss: 1.000052]\n",
            "1723 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1724 [D loss: 0.999963] [G loss: 1.000072]\n",
            "1725 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1726 [D loss: 0.999971] [G loss: 1.000072]\n",
            "1727 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1728 [D loss: 0.999978] [G loss: 1.000067]\n",
            "1729 [D loss: 0.999966] [G loss: 1.000072]\n",
            "1730 [D loss: 0.999963] [G loss: 1.000062]\n",
            "1731 [D loss: 0.999977] [G loss: 1.000049]\n",
            "1732 [D loss: 0.999976] [G loss: 1.000069]\n",
            "1733 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1734 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1735 [D loss: 0.999976] [G loss: 1.000062]\n",
            "1736 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1737 [D loss: 0.999975] [G loss: 1.000048]\n",
            "1738 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1739 [D loss: 0.999964] [G loss: 1.000075]\n",
            "1740 [D loss: 0.999976] [G loss: 1.000060]\n",
            "1741 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1742 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1743 [D loss: 0.999973] [G loss: 1.000056]\n",
            "1744 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1745 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1746 [D loss: 0.999970] [G loss: 1.000070]\n",
            "1747 [D loss: 0.999977] [G loss: 1.000067]\n",
            "1748 [D loss: 0.999979] [G loss: 1.000065]\n",
            "1749 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1750 [D loss: 0.999961] [G loss: 1.000059]\n",
            "1751 [D loss: 0.999976] [G loss: 1.000057]\n",
            "1752 [D loss: 0.999975] [G loss: 1.000055]\n",
            "1753 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1754 [D loss: 0.999960] [G loss: 1.000059]\n",
            "1755 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1756 [D loss: 0.999973] [G loss: 1.000053]\n",
            "1757 [D loss: 0.999980] [G loss: 1.000061]\n",
            "1758 [D loss: 0.999969] [G loss: 1.000053]\n",
            "1759 [D loss: 0.999978] [G loss: 1.000058]\n",
            "1760 [D loss: 0.999974] [G loss: 1.000063]\n",
            "1761 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1762 [D loss: 0.999966] [G loss: 1.000060]\n",
            "1763 [D loss: 0.999976] [G loss: 1.000077]\n",
            "1764 [D loss: 0.999978] [G loss: 1.000055]\n",
            "1765 [D loss: 0.999970] [G loss: 1.000051]\n",
            "1766 [D loss: 0.999970] [G loss: 1.000054]\n",
            "1767 [D loss: 0.999979] [G loss: 1.000055]\n",
            "1768 [D loss: 0.999964] [G loss: 1.000064]\n",
            "1769 [D loss: 0.999959] [G loss: 1.000058]\n",
            "1770 [D loss: 0.999977] [G loss: 1.000048]\n",
            "1771 [D loss: 0.999976] [G loss: 1.000046]\n",
            "1772 [D loss: 0.999966] [G loss: 1.000049]\n",
            "1773 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1774 [D loss: 0.999974] [G loss: 1.000061]\n",
            "1775 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1776 [D loss: 0.999975] [G loss: 1.000047]\n",
            "1777 [D loss: 0.999975] [G loss: 1.000065]\n",
            "1778 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1779 [D loss: 0.999971] [G loss: 1.000055]\n",
            "1780 [D loss: 0.999966] [G loss: 1.000067]\n",
            "1781 [D loss: 0.999972] [G loss: 1.000050]\n",
            "1782 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1783 [D loss: 0.999964] [G loss: 1.000058]\n",
            "1784 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1785 [D loss: 0.999965] [G loss: 1.000067]\n",
            "1786 [D loss: 0.999973] [G loss: 1.000057]\n",
            "1787 [D loss: 0.999974] [G loss: 1.000077]\n",
            "1788 [D loss: 0.999965] [G loss: 1.000056]\n",
            "1789 [D loss: 0.999974] [G loss: 1.000061]\n",
            "1790 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1791 [D loss: 0.999968] [G loss: 1.000054]\n",
            "1792 [D loss: 0.999957] [G loss: 1.000070]\n",
            "1793 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1794 [D loss: 0.999961] [G loss: 1.000076]\n",
            "1795 [D loss: 0.999974] [G loss: 1.000069]\n",
            "1796 [D loss: 0.999969] [G loss: 1.000052]\n",
            "1797 [D loss: 0.999969] [G loss: 1.000053]\n",
            "1798 [D loss: 0.999976] [G loss: 1.000060]\n",
            "1799 [D loss: 0.999977] [G loss: 1.000064]\n",
            "1800 [D loss: 0.999974] [G loss: 1.000058]\n",
            "1801 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1802 [D loss: 0.999972] [G loss: 1.000072]\n",
            "1803 [D loss: 0.999974] [G loss: 1.000056]\n",
            "1804 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1805 [D loss: 0.999978] [G loss: 1.000071]\n",
            "1806 [D loss: 0.999981] [G loss: 1.000062]\n",
            "1807 [D loss: 0.999964] [G loss: 1.000065]\n",
            "1808 [D loss: 0.999975] [G loss: 1.000065]\n",
            "1809 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1810 [D loss: 0.999981] [G loss: 1.000052]\n",
            "1811 [D loss: 0.999965] [G loss: 1.000063]\n",
            "1812 [D loss: 0.999975] [G loss: 1.000057]\n",
            "1813 [D loss: 0.999966] [G loss: 1.000073]\n",
            "1814 [D loss: 0.999964] [G loss: 1.000075]\n",
            "1815 [D loss: 0.999973] [G loss: 1.000047]\n",
            "1816 [D loss: 0.999959] [G loss: 1.000056]\n",
            "1817 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1818 [D loss: 0.999973] [G loss: 1.000055]\n",
            "1819 [D loss: 0.999975] [G loss: 1.000059]\n",
            "1820 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1821 [D loss: 0.999978] [G loss: 1.000064]\n",
            "1822 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1823 [D loss: 0.999973] [G loss: 1.000059]\n",
            "1824 [D loss: 0.999979] [G loss: 1.000053]\n",
            "1825 [D loss: 0.999968] [G loss: 1.000052]\n",
            "1826 [D loss: 0.999969] [G loss: 1.000057]\n",
            "1827 [D loss: 0.999970] [G loss: 1.000052]\n",
            "1828 [D loss: 0.999979] [G loss: 1.000064]\n",
            "1829 [D loss: 0.999972] [G loss: 1.000074]\n",
            "1830 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1831 [D loss: 0.999975] [G loss: 1.000058]\n",
            "1832 [D loss: 0.999979] [G loss: 1.000063]\n",
            "1833 [D loss: 0.999965] [G loss: 1.000063]\n",
            "1834 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1835 [D loss: 0.999980] [G loss: 1.000057]\n",
            "1836 [D loss: 0.999986] [G loss: 1.000070]\n",
            "1837 [D loss: 0.999975] [G loss: 1.000056]\n",
            "1838 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1839 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1840 [D loss: 0.999973] [G loss: 1.000072]\n",
            "1841 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1842 [D loss: 0.999977] [G loss: 1.000060]\n",
            "1843 [D loss: 0.999965] [G loss: 1.000070]\n",
            "1844 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1845 [D loss: 0.999978] [G loss: 1.000056]\n",
            "1846 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1847 [D loss: 0.999965] [G loss: 1.000070]\n",
            "1848 [D loss: 0.999972] [G loss: 1.000055]\n",
            "1849 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1850 [D loss: 0.999980] [G loss: 1.000042]\n",
            "1851 [D loss: 0.999977] [G loss: 1.000058]\n",
            "1852 [D loss: 0.999964] [G loss: 1.000062]\n",
            "1853 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1854 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1855 [D loss: 0.999976] [G loss: 1.000057]\n",
            "1856 [D loss: 0.999971] [G loss: 1.000068]\n",
            "1857 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1858 [D loss: 0.999973] [G loss: 1.000047]\n",
            "1859 [D loss: 0.999970] [G loss: 1.000071]\n",
            "1860 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1861 [D loss: 0.999967] [G loss: 1.000052]\n",
            "1862 [D loss: 0.999972] [G loss: 1.000049]\n",
            "1863 [D loss: 0.999964] [G loss: 1.000070]\n",
            "1864 [D loss: 0.999975] [G loss: 1.000052]\n",
            "1865 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1866 [D loss: 0.999976] [G loss: 1.000046]\n",
            "1867 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1868 [D loss: 0.999968] [G loss: 1.000052]\n",
            "1869 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1870 [D loss: 0.999961] [G loss: 1.000065]\n",
            "1871 [D loss: 0.999974] [G loss: 1.000066]\n",
            "1872 [D loss: 0.999975] [G loss: 1.000066]\n",
            "1873 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1874 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1875 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1876 [D loss: 0.999974] [G loss: 1.000061]\n",
            "1877 [D loss: 0.999974] [G loss: 1.000050]\n",
            "1878 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1879 [D loss: 0.999978] [G loss: 1.000073]\n",
            "1880 [D loss: 0.999962] [G loss: 1.000059]\n",
            "1881 [D loss: 0.999978] [G loss: 1.000065]\n",
            "1882 [D loss: 0.999974] [G loss: 1.000060]\n",
            "1883 [D loss: 0.999973] [G loss: 1.000070]\n",
            "1884 [D loss: 0.999975] [G loss: 1.000058]\n",
            "1885 [D loss: 0.999974] [G loss: 1.000051]\n",
            "1886 [D loss: 0.999972] [G loss: 1.000054]\n",
            "1887 [D loss: 0.999977] [G loss: 1.000061]\n",
            "1888 [D loss: 0.999976] [G loss: 1.000061]\n",
            "1889 [D loss: 0.999975] [G loss: 1.000051]\n",
            "1890 [D loss: 0.999981] [G loss: 1.000044]\n",
            "1891 [D loss: 0.999970] [G loss: 1.000052]\n",
            "1892 [D loss: 0.999980] [G loss: 1.000064]\n",
            "1893 [D loss: 0.999973] [G loss: 1.000065]\n",
            "1894 [D loss: 0.999963] [G loss: 1.000042]\n",
            "1895 [D loss: 0.999967] [G loss: 1.000058]\n",
            "1896 [D loss: 0.999974] [G loss: 1.000055]\n",
            "1897 [D loss: 0.999960] [G loss: 1.000057]\n",
            "1898 [D loss: 0.999969] [G loss: 1.000069]\n",
            "1899 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1900 [D loss: 0.999971] [G loss: 1.000073]\n",
            "1901 [D loss: 0.999975] [G loss: 1.000068]\n",
            "1902 [D loss: 0.999976] [G loss: 1.000060]\n",
            "1903 [D loss: 0.999965] [G loss: 1.000059]\n",
            "1904 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1905 [D loss: 0.999976] [G loss: 1.000054]\n",
            "1906 [D loss: 0.999976] [G loss: 1.000057]\n",
            "1907 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1908 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1909 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1910 [D loss: 0.999964] [G loss: 1.000048]\n",
            "1911 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1912 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1913 [D loss: 0.999970] [G loss: 1.000053]\n",
            "1914 [D loss: 0.999960] [G loss: 1.000060]\n",
            "1915 [D loss: 0.999977] [G loss: 1.000049]\n",
            "1916 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1917 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1918 [D loss: 0.999976] [G loss: 1.000048]\n",
            "1919 [D loss: 0.999978] [G loss: 1.000060]\n",
            "1920 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1921 [D loss: 0.999968] [G loss: 1.000058]\n",
            "1922 [D loss: 0.999974] [G loss: 1.000059]\n",
            "1923 [D loss: 0.999965] [G loss: 1.000057]\n",
            "1924 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1925 [D loss: 0.999973] [G loss: 1.000055]\n",
            "1926 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1927 [D loss: 0.999991] [G loss: 1.000065]\n",
            "1928 [D loss: 0.999968] [G loss: 1.000076]\n",
            "1929 [D loss: 0.999972] [G loss: 1.000051]\n",
            "1930 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1931 [D loss: 0.999975] [G loss: 1.000054]\n",
            "1932 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1933 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1934 [D loss: 0.999974] [G loss: 1.000063]\n",
            "1935 [D loss: 0.999979] [G loss: 1.000058]\n",
            "1936 [D loss: 0.999980] [G loss: 1.000058]\n",
            "1937 [D loss: 0.999972] [G loss: 1.000044]\n",
            "1938 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1939 [D loss: 0.999965] [G loss: 1.000061]\n",
            "1940 [D loss: 0.999971] [G loss: 1.000068]\n",
            "1941 [D loss: 0.999975] [G loss: 1.000046]\n",
            "1942 [D loss: 0.999978] [G loss: 1.000056]\n",
            "1943 [D loss: 0.999974] [G loss: 1.000074]\n",
            "1944 [D loss: 0.999972] [G loss: 1.000052]\n",
            "1945 [D loss: 0.999966] [G loss: 1.000039]\n",
            "1946 [D loss: 0.999962] [G loss: 1.000039]\n",
            "1947 [D loss: 0.999979] [G loss: 1.000062]\n",
            "1948 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1949 [D loss: 0.999972] [G loss: 1.000076]\n",
            "1950 [D loss: 0.999962] [G loss: 1.000053]\n",
            "1951 [D loss: 0.999981] [G loss: 1.000075]\n",
            "1952 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1953 [D loss: 0.999959] [G loss: 1.000053]\n",
            "1954 [D loss: 0.999980] [G loss: 1.000070]\n",
            "1955 [D loss: 0.999963] [G loss: 1.000054]\n",
            "1956 [D loss: 0.999976] [G loss: 1.000061]\n",
            "1957 [D loss: 0.999967] [G loss: 1.000053]\n",
            "1958 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1959 [D loss: 0.999966] [G loss: 1.000068]\n",
            "1960 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1961 [D loss: 0.999972] [G loss: 1.000052]\n",
            "1962 [D loss: 0.999981] [G loss: 1.000061]\n",
            "1963 [D loss: 0.999959] [G loss: 1.000066]\n",
            "1964 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1965 [D loss: 0.999963] [G loss: 1.000060]\n",
            "1966 [D loss: 0.999978] [G loss: 1.000060]\n",
            "1967 [D loss: 0.999970] [G loss: 1.000071]\n",
            "1968 [D loss: 0.999964] [G loss: 1.000061]\n",
            "1969 [D loss: 0.999981] [G loss: 1.000072]\n",
            "1970 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1971 [D loss: 0.999965] [G loss: 1.000044]\n",
            "1972 [D loss: 0.999969] [G loss: 1.000075]\n",
            "1973 [D loss: 0.999982] [G loss: 1.000059]\n",
            "1974 [D loss: 0.999964] [G loss: 1.000049]\n",
            "1975 [D loss: 0.999979] [G loss: 1.000057]\n",
            "1976 [D loss: 0.999967] [G loss: 1.000058]\n",
            "1977 [D loss: 0.999974] [G loss: 1.000049]\n",
            "1978 [D loss: 0.999963] [G loss: 1.000067]\n",
            "1979 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1980 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1981 [D loss: 0.999972] [G loss: 1.000041]\n",
            "1982 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1983 [D loss: 0.999966] [G loss: 1.000052]\n",
            "1984 [D loss: 0.999968] [G loss: 1.000055]\n",
            "1985 [D loss: 0.999965] [G loss: 1.000052]\n",
            "1986 [D loss: 0.999977] [G loss: 1.000070]\n",
            "1987 [D loss: 0.999958] [G loss: 1.000061]\n",
            "1988 [D loss: 0.999963] [G loss: 1.000065]\n",
            "1989 [D loss: 0.999981] [G loss: 1.000058]\n",
            "1990 [D loss: 0.999973] [G loss: 1.000064]\n",
            "1991 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1992 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1993 [D loss: 0.999966] [G loss: 1.000055]\n",
            "1994 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1995 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1996 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1997 [D loss: 0.999966] [G loss: 1.000060]\n",
            "1998 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1999 [D loss: 0.999981] [G loss: 1.000051]\n",
            "2000 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2001 [D loss: 0.999971] [G loss: 1.000069]\n",
            "2002 [D loss: 0.999978] [G loss: 1.000053]\n",
            "2003 [D loss: 0.999970] [G loss: 1.000075]\n",
            "2004 [D loss: 0.999979] [G loss: 1.000051]\n",
            "2005 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2006 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2007 [D loss: 0.999977] [G loss: 1.000058]\n",
            "2008 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2009 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2010 [D loss: 0.999979] [G loss: 1.000065]\n",
            "2011 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2012 [D loss: 0.999963] [G loss: 1.000071]\n",
            "2013 [D loss: 0.999979] [G loss: 1.000055]\n",
            "2014 [D loss: 0.999976] [G loss: 1.000057]\n",
            "2015 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2016 [D loss: 0.999963] [G loss: 1.000069]\n",
            "2017 [D loss: 0.999961] [G loss: 1.000056]\n",
            "2018 [D loss: 0.999976] [G loss: 1.000060]\n",
            "2019 [D loss: 0.999972] [G loss: 1.000058]\n",
            "2020 [D loss: 0.999978] [G loss: 1.000072]\n",
            "2021 [D loss: 0.999964] [G loss: 1.000064]\n",
            "2022 [D loss: 0.999966] [G loss: 1.000052]\n",
            "2023 [D loss: 0.999966] [G loss: 1.000057]\n",
            "2024 [D loss: 0.999964] [G loss: 1.000066]\n",
            "2025 [D loss: 0.999967] [G loss: 1.000072]\n",
            "2026 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2027 [D loss: 0.999977] [G loss: 1.000063]\n",
            "2028 [D loss: 0.999978] [G loss: 1.000047]\n",
            "2029 [D loss: 0.999967] [G loss: 1.000052]\n",
            "2030 [D loss: 0.999977] [G loss: 1.000062]\n",
            "2031 [D loss: 0.999976] [G loss: 1.000049]\n",
            "2032 [D loss: 0.999970] [G loss: 1.000056]\n",
            "2033 [D loss: 0.999975] [G loss: 1.000061]\n",
            "2034 [D loss: 0.999971] [G loss: 1.000046]\n",
            "2035 [D loss: 0.999968] [G loss: 1.000059]\n",
            "2036 [D loss: 0.999965] [G loss: 1.000064]\n",
            "2037 [D loss: 0.999978] [G loss: 1.000058]\n",
            "2038 [D loss: 0.999989] [G loss: 1.000058]\n",
            "2039 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2040 [D loss: 0.999973] [G loss: 1.000073]\n",
            "2041 [D loss: 0.999967] [G loss: 1.000073]\n",
            "2042 [D loss: 0.999964] [G loss: 1.000066]\n",
            "2043 [D loss: 0.999974] [G loss: 1.000056]\n",
            "2044 [D loss: 0.999981] [G loss: 1.000057]\n",
            "2045 [D loss: 0.999967] [G loss: 1.000050]\n",
            "2046 [D loss: 0.999969] [G loss: 1.000074]\n",
            "2047 [D loss: 0.999976] [G loss: 1.000038]\n",
            "2048 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2049 [D loss: 0.999966] [G loss: 1.000055]\n",
            "2050 [D loss: 0.999975] [G loss: 1.000058]\n",
            "2051 [D loss: 0.999966] [G loss: 1.000061]\n",
            "2052 [D loss: 0.999975] [G loss: 1.000060]\n",
            "2053 [D loss: 0.999978] [G loss: 1.000054]\n",
            "2054 [D loss: 0.999968] [G loss: 1.000054]\n",
            "2055 [D loss: 0.999970] [G loss: 1.000035]\n",
            "2056 [D loss: 0.999964] [G loss: 1.000050]\n",
            "2057 [D loss: 0.999969] [G loss: 1.000058]\n",
            "2058 [D loss: 0.999967] [G loss: 1.000047]\n",
            "2059 [D loss: 0.999987] [G loss: 1.000049]\n",
            "2060 [D loss: 0.999963] [G loss: 1.000042]\n",
            "2061 [D loss: 0.999980] [G loss: 1.000065]\n",
            "2062 [D loss: 0.999973] [G loss: 1.000057]\n",
            "2063 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2064 [D loss: 0.999976] [G loss: 1.000040]\n",
            "2065 [D loss: 0.999977] [G loss: 1.000060]\n",
            "2066 [D loss: 0.999966] [G loss: 1.000050]\n",
            "2067 [D loss: 0.999971] [G loss: 1.000080]\n",
            "2068 [D loss: 0.999965] [G loss: 1.000054]\n",
            "2069 [D loss: 0.999973] [G loss: 1.000038]\n",
            "2070 [D loss: 0.999968] [G loss: 1.000046]\n",
            "2071 [D loss: 0.999975] [G loss: 1.000064]\n",
            "2072 [D loss: 0.999973] [G loss: 1.000050]\n",
            "2073 [D loss: 0.999968] [G loss: 1.000055]\n",
            "2074 [D loss: 0.999976] [G loss: 1.000069]\n",
            "2075 [D loss: 0.999969] [G loss: 1.000052]\n",
            "2076 [D loss: 0.999977] [G loss: 1.000061]\n",
            "2077 [D loss: 0.999970] [G loss: 1.000070]\n",
            "2078 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2079 [D loss: 0.999968] [G loss: 1.000055]\n",
            "2080 [D loss: 0.999976] [G loss: 1.000066]\n",
            "2081 [D loss: 0.999969] [G loss: 1.000059]\n",
            "2082 [D loss: 0.999978] [G loss: 1.000056]\n",
            "2083 [D loss: 0.999982] [G loss: 1.000062]\n",
            "2084 [D loss: 0.999977] [G loss: 1.000068]\n",
            "2085 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2086 [D loss: 0.999965] [G loss: 1.000055]\n",
            "2087 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2088 [D loss: 0.999969] [G loss: 1.000072]\n",
            "2089 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2090 [D loss: 0.999970] [G loss: 1.000050]\n",
            "2091 [D loss: 0.999975] [G loss: 1.000050]\n",
            "2092 [D loss: 0.999972] [G loss: 1.000053]\n",
            "2093 [D loss: 0.999981] [G loss: 1.000052]\n",
            "2094 [D loss: 0.999968] [G loss: 1.000059]\n",
            "2095 [D loss: 0.999975] [G loss: 1.000070]\n",
            "2096 [D loss: 0.999971] [G loss: 1.000053]\n",
            "2097 [D loss: 0.999976] [G loss: 1.000061]\n",
            "2098 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2099 [D loss: 0.999983] [G loss: 1.000065]\n",
            "2100 [D loss: 0.999976] [G loss: 1.000048]\n",
            "2101 [D loss: 0.999977] [G loss: 1.000055]\n",
            "2102 [D loss: 0.999980] [G loss: 1.000066]\n",
            "2103 [D loss: 0.999974] [G loss: 1.000047]\n",
            "2104 [D loss: 0.999965] [G loss: 1.000046]\n",
            "2105 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2106 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2107 [D loss: 0.999979] [G loss: 1.000042]\n",
            "2108 [D loss: 0.999972] [G loss: 1.000053]\n",
            "2109 [D loss: 0.999970] [G loss: 1.000048]\n",
            "2110 [D loss: 0.999966] [G loss: 1.000065]\n",
            "2111 [D loss: 0.999970] [G loss: 1.000053]\n",
            "2112 [D loss: 0.999975] [G loss: 1.000048]\n",
            "2113 [D loss: 0.999976] [G loss: 1.000057]\n",
            "2114 [D loss: 0.999973] [G loss: 1.000070]\n",
            "2115 [D loss: 0.999979] [G loss: 1.000054]\n",
            "2116 [D loss: 0.999955] [G loss: 1.000052]\n",
            "2117 [D loss: 0.999980] [G loss: 1.000060]\n",
            "2118 [D loss: 0.999974] [G loss: 1.000059]\n",
            "2119 [D loss: 0.999968] [G loss: 1.000061]\n",
            "2120 [D loss: 0.999977] [G loss: 1.000047]\n",
            "2121 [D loss: 0.999979] [G loss: 1.000053]\n",
            "2122 [D loss: 0.999979] [G loss: 1.000055]\n",
            "2123 [D loss: 0.999972] [G loss: 1.000075]\n",
            "2124 [D loss: 0.999978] [G loss: 1.000050]\n",
            "2125 [D loss: 0.999974] [G loss: 1.000062]\n",
            "2126 [D loss: 0.999972] [G loss: 1.000054]\n",
            "2127 [D loss: 0.999967] [G loss: 1.000057]\n",
            "2128 [D loss: 0.999981] [G loss: 1.000062]\n",
            "2129 [D loss: 0.999971] [G loss: 1.000052]\n",
            "2130 [D loss: 0.999964] [G loss: 1.000060]\n",
            "2131 [D loss: 0.999979] [G loss: 1.000055]\n",
            "2132 [D loss: 0.999980] [G loss: 1.000044]\n",
            "2133 [D loss: 0.999965] [G loss: 1.000059]\n",
            "2134 [D loss: 0.999981] [G loss: 1.000068]\n",
            "2135 [D loss: 0.999971] [G loss: 1.000056]\n",
            "2136 [D loss: 0.999976] [G loss: 1.000077]\n",
            "2137 [D loss: 0.999973] [G loss: 1.000048]\n",
            "2138 [D loss: 0.999969] [G loss: 1.000051]\n",
            "2139 [D loss: 0.999975] [G loss: 1.000072]\n",
            "2140 [D loss: 0.999974] [G loss: 1.000064]\n",
            "2141 [D loss: 0.999975] [G loss: 1.000053]\n",
            "2142 [D loss: 0.999965] [G loss: 1.000066]\n",
            "2143 [D loss: 0.999976] [G loss: 1.000039]\n",
            "2144 [D loss: 0.999976] [G loss: 1.000058]\n",
            "2145 [D loss: 0.999973] [G loss: 1.000046]\n",
            "2146 [D loss: 0.999974] [G loss: 1.000059]\n",
            "2147 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2148 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2149 [D loss: 0.999969] [G loss: 1.000054]\n",
            "2150 [D loss: 0.999981] [G loss: 1.000059]\n",
            "2151 [D loss: 0.999982] [G loss: 1.000063]\n",
            "2152 [D loss: 0.999968] [G loss: 1.000051]\n",
            "2153 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2154 [D loss: 0.999970] [G loss: 1.000055]\n",
            "2155 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2156 [D loss: 0.999990] [G loss: 1.000057]\n",
            "2157 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2158 [D loss: 0.999967] [G loss: 1.000059]\n",
            "2159 [D loss: 0.999977] [G loss: 1.000060]\n",
            "2160 [D loss: 0.999980] [G loss: 1.000055]\n",
            "2161 [D loss: 0.999973] [G loss: 1.000055]\n",
            "2162 [D loss: 0.999973] [G loss: 1.000055]\n",
            "2163 [D loss: 0.999983] [G loss: 1.000055]\n",
            "2164 [D loss: 0.999965] [G loss: 1.000061]\n",
            "2165 [D loss: 0.999974] [G loss: 1.000062]\n",
            "2166 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2167 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2168 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2169 [D loss: 0.999961] [G loss: 1.000060]\n",
            "2170 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2171 [D loss: 0.999972] [G loss: 1.000054]\n",
            "2172 [D loss: 0.999965] [G loss: 1.000070]\n",
            "2173 [D loss: 0.999978] [G loss: 1.000046]\n",
            "2174 [D loss: 0.999973] [G loss: 1.000052]\n",
            "2175 [D loss: 0.999973] [G loss: 1.000081]\n",
            "2176 [D loss: 0.999980] [G loss: 1.000062]\n",
            "2177 [D loss: 0.999976] [G loss: 1.000052]\n",
            "2178 [D loss: 0.999975] [G loss: 1.000053]\n",
            "2179 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2180 [D loss: 0.999982] [G loss: 1.000074]\n",
            "2181 [D loss: 0.999981] [G loss: 1.000052]\n",
            "2182 [D loss: 0.999981] [G loss: 1.000060]\n",
            "2183 [D loss: 0.999982] [G loss: 1.000055]\n",
            "2184 [D loss: 0.999969] [G loss: 1.000056]\n",
            "2185 [D loss: 0.999974] [G loss: 1.000063]\n",
            "2186 [D loss: 0.999982] [G loss: 1.000052]\n",
            "2187 [D loss: 0.999973] [G loss: 1.000072]\n",
            "2188 [D loss: 0.999968] [G loss: 1.000054]\n",
            "2189 [D loss: 0.999967] [G loss: 1.000038]\n",
            "2190 [D loss: 0.999985] [G loss: 1.000062]\n",
            "2191 [D loss: 0.999970] [G loss: 1.000053]\n",
            "2192 [D loss: 0.999967] [G loss: 1.000068]\n",
            "2193 [D loss: 0.999966] [G loss: 1.000049]\n",
            "2194 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2195 [D loss: 0.999985] [G loss: 1.000080]\n",
            "2196 [D loss: 0.999971] [G loss: 1.000070]\n",
            "2197 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2198 [D loss: 0.999974] [G loss: 1.000058]\n",
            "2199 [D loss: 0.999978] [G loss: 1.000054]\n",
            "2200 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2201 [D loss: 0.999967] [G loss: 1.000056]\n",
            "2202 [D loss: 0.999977] [G loss: 1.000052]\n",
            "2203 [D loss: 0.999956] [G loss: 1.000052]\n",
            "2204 [D loss: 0.999968] [G loss: 1.000051]\n",
            "2205 [D loss: 0.999974] [G loss: 1.000057]\n",
            "2206 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2207 [D loss: 0.999977] [G loss: 1.000063]\n",
            "2208 [D loss: 0.999982] [G loss: 1.000059]\n",
            "2209 [D loss: 0.999967] [G loss: 1.000053]\n",
            "2210 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2211 [D loss: 0.999981] [G loss: 1.000076]\n",
            "2212 [D loss: 0.999975] [G loss: 1.000048]\n",
            "2213 [D loss: 0.999978] [G loss: 1.000052]\n",
            "2214 [D loss: 0.999982] [G loss: 1.000040]\n",
            "2215 [D loss: 0.999965] [G loss: 1.000057]\n",
            "2216 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2217 [D loss: 0.999971] [G loss: 1.000089]\n",
            "2218 [D loss: 0.999989] [G loss: 1.000054]\n",
            "2219 [D loss: 0.999962] [G loss: 1.000067]\n",
            "2220 [D loss: 0.999978] [G loss: 1.000042]\n",
            "2221 [D loss: 0.999968] [G loss: 1.000038]\n",
            "2222 [D loss: 0.999968] [G loss: 1.000034]\n",
            "2223 [D loss: 0.999968] [G loss: 1.000061]\n",
            "2224 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2225 [D loss: 0.999979] [G loss: 1.000053]\n",
            "2226 [D loss: 0.999962] [G loss: 1.000069]\n",
            "2227 [D loss: 0.999967] [G loss: 1.000073]\n",
            "2228 [D loss: 0.999972] [G loss: 1.000055]\n",
            "2229 [D loss: 0.999968] [G loss: 1.000054]\n",
            "2230 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2231 [D loss: 0.999969] [G loss: 1.000056]\n",
            "2232 [D loss: 0.999983] [G loss: 1.000070]\n",
            "2233 [D loss: 0.999978] [G loss: 1.000075]\n",
            "2234 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2235 [D loss: 0.999985] [G loss: 1.000065]\n",
            "2236 [D loss: 0.999959] [G loss: 1.000055]\n",
            "2237 [D loss: 0.999976] [G loss: 1.000067]\n",
            "2238 [D loss: 0.999962] [G loss: 1.000053]\n",
            "2239 [D loss: 0.999969] [G loss: 1.000045]\n",
            "2240 [D loss: 0.999980] [G loss: 1.000070]\n",
            "2241 [D loss: 0.999967] [G loss: 1.000076]\n",
            "2242 [D loss: 0.999973] [G loss: 1.000069]\n",
            "2243 [D loss: 0.999966] [G loss: 1.000068]\n",
            "2244 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2245 [D loss: 0.999971] [G loss: 1.000045]\n",
            "2246 [D loss: 0.999973] [G loss: 1.000070]\n",
            "2247 [D loss: 0.999967] [G loss: 1.000053]\n",
            "2248 [D loss: 0.999973] [G loss: 1.000045]\n",
            "2249 [D loss: 0.999973] [G loss: 1.000044]\n",
            "2250 [D loss: 0.999977] [G loss: 1.000050]\n",
            "2251 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2252 [D loss: 0.999958] [G loss: 1.000043]\n",
            "2253 [D loss: 0.999962] [G loss: 1.000063]\n",
            "2254 [D loss: 0.999975] [G loss: 1.000060]\n",
            "2255 [D loss: 0.999980] [G loss: 1.000048]\n",
            "2256 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2257 [D loss: 0.999973] [G loss: 1.000062]\n",
            "2258 [D loss: 0.999965] [G loss: 1.000042]\n",
            "2259 [D loss: 0.999974] [G loss: 1.000054]\n",
            "2260 [D loss: 0.999973] [G loss: 1.000041]\n",
            "2261 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2262 [D loss: 0.999971] [G loss: 1.000055]\n",
            "2263 [D loss: 0.999974] [G loss: 1.000066]\n",
            "2264 [D loss: 0.999962] [G loss: 1.000069]\n",
            "2265 [D loss: 0.999976] [G loss: 1.000069]\n",
            "2266 [D loss: 0.999965] [G loss: 1.000050]\n",
            "2267 [D loss: 0.999955] [G loss: 1.000049]\n",
            "2268 [D loss: 0.999975] [G loss: 1.000049]\n",
            "2269 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2270 [D loss: 0.999981] [G loss: 1.000048]\n",
            "2271 [D loss: 0.999970] [G loss: 1.000035]\n",
            "2272 [D loss: 0.999987] [G loss: 1.000065]\n",
            "2273 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2274 [D loss: 0.999974] [G loss: 1.000057]\n",
            "2275 [D loss: 0.999972] [G loss: 1.000055]\n",
            "2276 [D loss: 0.999972] [G loss: 1.000035]\n",
            "2277 [D loss: 0.999963] [G loss: 1.000061]\n",
            "2278 [D loss: 0.999978] [G loss: 1.000057]\n",
            "2279 [D loss: 0.999967] [G loss: 1.000067]\n",
            "2280 [D loss: 0.999974] [G loss: 1.000072]\n",
            "2281 [D loss: 0.999975] [G loss: 1.000066]\n",
            "2282 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2283 [D loss: 0.999973] [G loss: 1.000069]\n",
            "2284 [D loss: 0.999975] [G loss: 1.000057]\n",
            "2285 [D loss: 0.999967] [G loss: 1.000043]\n",
            "2286 [D loss: 0.999967] [G loss: 1.000069]\n",
            "2287 [D loss: 0.999977] [G loss: 1.000074]\n",
            "2288 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2289 [D loss: 0.999972] [G loss: 1.000040]\n",
            "2290 [D loss: 0.999972] [G loss: 1.000058]\n",
            "2291 [D loss: 0.999970] [G loss: 1.000052]\n",
            "2292 [D loss: 0.999961] [G loss: 1.000062]\n",
            "2293 [D loss: 0.999974] [G loss: 1.000054]\n",
            "2294 [D loss: 0.999965] [G loss: 1.000059]\n",
            "2295 [D loss: 0.999967] [G loss: 1.000054]\n",
            "2296 [D loss: 0.999970] [G loss: 1.000053]\n",
            "2297 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2298 [D loss: 0.999973] [G loss: 1.000052]\n",
            "2299 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2300 [D loss: 0.999967] [G loss: 1.000062]\n",
            "2301 [D loss: 0.999967] [G loss: 1.000066]\n",
            "2302 [D loss: 0.999981] [G loss: 1.000057]\n",
            "2303 [D loss: 0.999977] [G loss: 1.000042]\n",
            "2304 [D loss: 0.999976] [G loss: 1.000060]\n",
            "2305 [D loss: 0.999958] [G loss: 1.000052]\n",
            "2306 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2307 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2308 [D loss: 0.999973] [G loss: 1.000045]\n",
            "2309 [D loss: 0.999967] [G loss: 1.000057]\n",
            "2310 [D loss: 0.999973] [G loss: 1.000053]\n",
            "2311 [D loss: 0.999982] [G loss: 1.000047]\n",
            "2312 [D loss: 0.999964] [G loss: 1.000050]\n",
            "2313 [D loss: 0.999964] [G loss: 1.000042]\n",
            "2314 [D loss: 0.999972] [G loss: 1.000049]\n",
            "2315 [D loss: 0.999973] [G loss: 1.000056]\n",
            "2316 [D loss: 0.999979] [G loss: 1.000051]\n",
            "2317 [D loss: 0.999971] [G loss: 1.000072]\n",
            "2318 [D loss: 0.999982] [G loss: 1.000051]\n",
            "2319 [D loss: 0.999977] [G loss: 1.000051]\n",
            "2320 [D loss: 0.999978] [G loss: 1.000055]\n",
            "2321 [D loss: 0.999971] [G loss: 1.000058]\n",
            "2322 [D loss: 0.999968] [G loss: 1.000049]\n",
            "2323 [D loss: 0.999966] [G loss: 1.000059]\n",
            "2324 [D loss: 0.999956] [G loss: 1.000047]\n",
            "2325 [D loss: 0.999973] [G loss: 1.000048]\n",
            "2326 [D loss: 0.999973] [G loss: 1.000053]\n",
            "2327 [D loss: 0.999971] [G loss: 1.000059]\n",
            "2328 [D loss: 0.999975] [G loss: 1.000066]\n",
            "2329 [D loss: 0.999981] [G loss: 1.000034]\n",
            "2330 [D loss: 0.999964] [G loss: 1.000054]\n",
            "2331 [D loss: 0.999964] [G loss: 1.000048]\n",
            "2332 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2333 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2334 [D loss: 0.999967] [G loss: 1.000061]\n",
            "2335 [D loss: 0.999976] [G loss: 1.000054]\n",
            "2336 [D loss: 0.999974] [G loss: 1.000066]\n",
            "2337 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2338 [D loss: 0.999974] [G loss: 1.000052]\n",
            "2339 [D loss: 0.999969] [G loss: 1.000051]\n",
            "2340 [D loss: 0.999983] [G loss: 1.000063]\n",
            "2341 [D loss: 0.999970] [G loss: 1.000051]\n",
            "2342 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2343 [D loss: 0.999980] [G loss: 1.000066]\n",
            "2344 [D loss: 0.999977] [G loss: 1.000053]\n",
            "2345 [D loss: 0.999980] [G loss: 1.000062]\n",
            "2346 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2347 [D loss: 0.999977] [G loss: 1.000064]\n",
            "2348 [D loss: 0.999974] [G loss: 1.000070]\n",
            "2349 [D loss: 0.999964] [G loss: 1.000053]\n",
            "2350 [D loss: 0.999962] [G loss: 1.000055]\n",
            "2351 [D loss: 0.999980] [G loss: 1.000046]\n",
            "2352 [D loss: 0.999978] [G loss: 1.000060]\n",
            "2353 [D loss: 0.999960] [G loss: 1.000065]\n",
            "2354 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2355 [D loss: 0.999971] [G loss: 1.000056]\n",
            "2356 [D loss: 0.999973] [G loss: 1.000056]\n",
            "2357 [D loss: 0.999965] [G loss: 1.000074]\n",
            "2358 [D loss: 0.999969] [G loss: 1.000059]\n",
            "2359 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2360 [D loss: 0.999967] [G loss: 1.000065]\n",
            "2361 [D loss: 0.999978] [G loss: 1.000062]\n",
            "2362 [D loss: 0.999968] [G loss: 1.000043]\n",
            "2363 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2364 [D loss: 0.999957] [G loss: 1.000065]\n",
            "2365 [D loss: 0.999972] [G loss: 1.000056]\n",
            "2366 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2367 [D loss: 0.999965] [G loss: 1.000057]\n",
            "2368 [D loss: 0.999980] [G loss: 1.000056]\n",
            "2369 [D loss: 0.999978] [G loss: 1.000055]\n",
            "2370 [D loss: 0.999969] [G loss: 1.000060]\n",
            "2371 [D loss: 0.999982] [G loss: 1.000058]\n",
            "2372 [D loss: 0.999976] [G loss: 1.000063]\n",
            "2373 [D loss: 0.999973] [G loss: 1.000056]\n",
            "2374 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2375 [D loss: 0.999966] [G loss: 1.000071]\n",
            "2376 [D loss: 0.999973] [G loss: 1.000061]\n",
            "2377 [D loss: 0.999980] [G loss: 1.000065]\n",
            "2378 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2379 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2380 [D loss: 0.999982] [G loss: 1.000064]\n",
            "2381 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2382 [D loss: 0.999973] [G loss: 1.000049]\n",
            "2383 [D loss: 0.999964] [G loss: 1.000049]\n",
            "2384 [D loss: 0.999977] [G loss: 1.000056]\n",
            "2385 [D loss: 0.999968] [G loss: 1.000061]\n",
            "2386 [D loss: 0.999977] [G loss: 1.000092]\n",
            "2387 [D loss: 0.999968] [G loss: 1.000060]\n",
            "2388 [D loss: 0.999970] [G loss: 1.000055]\n",
            "2389 [D loss: 0.999976] [G loss: 1.000046]\n",
            "2390 [D loss: 0.999976] [G loss: 1.000055]\n",
            "2391 [D loss: 0.999975] [G loss: 1.000043]\n",
            "2392 [D loss: 0.999972] [G loss: 1.000048]\n",
            "2393 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2394 [D loss: 0.999978] [G loss: 1.000048]\n",
            "2395 [D loss: 0.999971] [G loss: 1.000049]\n",
            "2396 [D loss: 0.999962] [G loss: 1.000062]\n",
            "2397 [D loss: 0.999975] [G loss: 1.000062]\n",
            "2398 [D loss: 0.999988] [G loss: 1.000045]\n",
            "2399 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2400 [D loss: 0.999966] [G loss: 1.000058]\n",
            "2401 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2402 [D loss: 0.999968] [G loss: 1.000055]\n",
            "2403 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2404 [D loss: 0.999969] [G loss: 1.000079]\n",
            "2405 [D loss: 0.999977] [G loss: 1.000060]\n",
            "2406 [D loss: 0.999980] [G loss: 1.000051]\n",
            "2407 [D loss: 0.999968] [G loss: 1.000051]\n",
            "2408 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2409 [D loss: 0.999982] [G loss: 1.000064]\n",
            "2410 [D loss: 0.999990] [G loss: 1.000066]\n",
            "2411 [D loss: 0.999971] [G loss: 1.000043]\n",
            "2412 [D loss: 0.999975] [G loss: 1.000058]\n",
            "2413 [D loss: 0.999967] [G loss: 1.000028]\n",
            "2414 [D loss: 0.999969] [G loss: 1.000054]\n",
            "2415 [D loss: 0.999975] [G loss: 1.000052]\n",
            "2416 [D loss: 0.999982] [G loss: 1.000056]\n",
            "2417 [D loss: 0.999974] [G loss: 1.000051]\n",
            "2418 [D loss: 0.999959] [G loss: 1.000056]\n",
            "2419 [D loss: 0.999972] [G loss: 1.000056]\n",
            "2420 [D loss: 0.999973] [G loss: 1.000038]\n",
            "2421 [D loss: 0.999981] [G loss: 1.000059]\n",
            "2422 [D loss: 0.999966] [G loss: 1.000042]\n",
            "2423 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2424 [D loss: 0.999980] [G loss: 1.000052]\n",
            "2425 [D loss: 0.999968] [G loss: 1.000037]\n",
            "2426 [D loss: 0.999967] [G loss: 1.000053]\n",
            "2427 [D loss: 0.999969] [G loss: 1.000058]\n",
            "2428 [D loss: 0.999963] [G loss: 1.000051]\n",
            "2429 [D loss: 0.999964] [G loss: 1.000059]\n",
            "2430 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2431 [D loss: 0.999962] [G loss: 1.000066]\n",
            "2432 [D loss: 0.999966] [G loss: 1.000064]\n",
            "2433 [D loss: 0.999968] [G loss: 1.000071]\n",
            "2434 [D loss: 0.999975] [G loss: 1.000060]\n",
            "2435 [D loss: 0.999976] [G loss: 1.000047]\n",
            "2436 [D loss: 0.999976] [G loss: 1.000061]\n",
            "2437 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2438 [D loss: 0.999974] [G loss: 1.000050]\n",
            "2439 [D loss: 0.999973] [G loss: 1.000062]\n",
            "2440 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2441 [D loss: 0.999967] [G loss: 1.000055]\n",
            "2442 [D loss: 0.999973] [G loss: 1.000070]\n",
            "2443 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2444 [D loss: 0.999976] [G loss: 1.000065]\n",
            "2445 [D loss: 0.999962] [G loss: 1.000074]\n",
            "2446 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2447 [D loss: 0.999971] [G loss: 1.000056]\n",
            "2448 [D loss: 0.999974] [G loss: 1.000066]\n",
            "2449 [D loss: 0.999966] [G loss: 1.000058]\n",
            "2450 [D loss: 0.999978] [G loss: 1.000049]\n",
            "2451 [D loss: 0.999969] [G loss: 1.000051]\n",
            "2452 [D loss: 0.999975] [G loss: 1.000059]\n",
            "2453 [D loss: 0.999975] [G loss: 1.000064]\n",
            "2454 [D loss: 0.999979] [G loss: 1.000066]\n",
            "2455 [D loss: 0.999967] [G loss: 1.000069]\n",
            "2456 [D loss: 0.999971] [G loss: 1.000054]\n",
            "2457 [D loss: 0.999973] [G loss: 1.000071]\n",
            "2458 [D loss: 0.999965] [G loss: 1.000062]\n",
            "2459 [D loss: 0.999975] [G loss: 1.000068]\n",
            "2460 [D loss: 0.999979] [G loss: 1.000056]\n",
            "2461 [D loss: 0.999971] [G loss: 1.000073]\n",
            "2462 [D loss: 0.999976] [G loss: 1.000073]\n",
            "2463 [D loss: 0.999978] [G loss: 1.000041]\n",
            "2464 [D loss: 0.999972] [G loss: 1.000070]\n",
            "2465 [D loss: 0.999971] [G loss: 1.000079]\n",
            "2466 [D loss: 0.999967] [G loss: 1.000039]\n",
            "2467 [D loss: 0.999979] [G loss: 1.000046]\n",
            "2468 [D loss: 0.999977] [G loss: 1.000066]\n",
            "2469 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2470 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2471 [D loss: 0.999967] [G loss: 1.000067]\n",
            "2472 [D loss: 0.999970] [G loss: 1.000056]\n",
            "2473 [D loss: 0.999967] [G loss: 1.000040]\n",
            "2474 [D loss: 0.999982] [G loss: 1.000047]\n",
            "2475 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2476 [D loss: 0.999973] [G loss: 1.000051]\n",
            "2477 [D loss: 0.999986] [G loss: 1.000065]\n",
            "2478 [D loss: 0.999964] [G loss: 1.000056]\n",
            "2479 [D loss: 0.999967] [G loss: 1.000061]\n",
            "2480 [D loss: 0.999966] [G loss: 1.000052]\n",
            "2481 [D loss: 0.999961] [G loss: 1.000055]\n",
            "2482 [D loss: 0.999979] [G loss: 1.000045]\n",
            "2483 [D loss: 0.999972] [G loss: 1.000058]\n",
            "2484 [D loss: 0.999968] [G loss: 1.000051]\n",
            "2485 [D loss: 0.999976] [G loss: 1.000037]\n",
            "2486 [D loss: 0.999965] [G loss: 1.000039]\n",
            "2487 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2488 [D loss: 0.999983] [G loss: 1.000047]\n",
            "2489 [D loss: 0.999970] [G loss: 1.000043]\n",
            "2490 [D loss: 0.999977] [G loss: 1.000052]\n",
            "2491 [D loss: 0.999977] [G loss: 1.000069]\n",
            "2492 [D loss: 0.999984] [G loss: 1.000039]\n",
            "2493 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2494 [D loss: 0.999965] [G loss: 1.000056]\n",
            "2495 [D loss: 0.999977] [G loss: 1.000055]\n",
            "2496 [D loss: 0.999980] [G loss: 1.000049]\n",
            "2497 [D loss: 0.999977] [G loss: 1.000066]\n",
            "2498 [D loss: 0.999970] [G loss: 1.000049]\n",
            "2499 [D loss: 0.999974] [G loss: 1.000057]\n",
            "2500 [D loss: 0.999979] [G loss: 1.000050]\n",
            "2501 [D loss: 0.999968] [G loss: 1.000047]\n",
            "2502 [D loss: 0.999981] [G loss: 1.000052]\n",
            "2503 [D loss: 0.999974] [G loss: 1.000057]\n",
            "2504 [D loss: 0.999967] [G loss: 1.000056]\n",
            "2505 [D loss: 0.999989] [G loss: 1.000055]\n",
            "2506 [D loss: 0.999963] [G loss: 1.000058]\n",
            "2507 [D loss: 0.999973] [G loss: 1.000057]\n",
            "2508 [D loss: 0.999975] [G loss: 1.000059]\n",
            "2509 [D loss: 0.999965] [G loss: 1.000047]\n",
            "2510 [D loss: 0.999974] [G loss: 1.000061]\n",
            "2511 [D loss: 0.999984] [G loss: 1.000051]\n",
            "2512 [D loss: 0.999979] [G loss: 1.000042]\n",
            "2513 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2514 [D loss: 0.999986] [G loss: 1.000047]\n",
            "2515 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2516 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2517 [D loss: 0.999962] [G loss: 1.000049]\n",
            "2518 [D loss: 0.999970] [G loss: 1.000055]\n",
            "2519 [D loss: 0.999966] [G loss: 1.000044]\n",
            "2520 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2521 [D loss: 0.999984] [G loss: 1.000044]\n",
            "2522 [D loss: 0.999975] [G loss: 1.000045]\n",
            "2523 [D loss: 0.999983] [G loss: 1.000063]\n",
            "2524 [D loss: 0.999964] [G loss: 1.000068]\n",
            "2525 [D loss: 0.999971] [G loss: 1.000041]\n",
            "2526 [D loss: 0.999973] [G loss: 1.000049]\n",
            "2527 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2528 [D loss: 0.999979] [G loss: 1.000058]\n",
            "2529 [D loss: 0.999970] [G loss: 1.000071]\n",
            "2530 [D loss: 0.999975] [G loss: 1.000052]\n",
            "2531 [D loss: 0.999981] [G loss: 1.000044]\n",
            "2532 [D loss: 0.999973] [G loss: 1.000038]\n",
            "2533 [D loss: 0.999978] [G loss: 1.000043]\n",
            "2534 [D loss: 0.999964] [G loss: 1.000063]\n",
            "2535 [D loss: 0.999963] [G loss: 1.000038]\n",
            "2536 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2537 [D loss: 0.999968] [G loss: 1.000050]\n",
            "2538 [D loss: 0.999977] [G loss: 1.000062]\n",
            "2539 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2540 [D loss: 0.999966] [G loss: 1.000060]\n",
            "2541 [D loss: 0.999985] [G loss: 1.000039]\n",
            "2542 [D loss: 0.999979] [G loss: 1.000053]\n",
            "2543 [D loss: 0.999976] [G loss: 1.000067]\n",
            "2544 [D loss: 0.999961] [G loss: 1.000053]\n",
            "2545 [D loss: 0.999970] [G loss: 1.000055]\n",
            "2546 [D loss: 0.999972] [G loss: 1.000046]\n",
            "2547 [D loss: 0.999970] [G loss: 1.000056]\n",
            "2548 [D loss: 0.999963] [G loss: 1.000040]\n",
            "2549 [D loss: 0.999973] [G loss: 1.000073]\n",
            "2550 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2551 [D loss: 0.999964] [G loss: 1.000051]\n",
            "2552 [D loss: 0.999968] [G loss: 1.000052]\n",
            "2553 [D loss: 0.999972] [G loss: 1.000050]\n",
            "2554 [D loss: 0.999983] [G loss: 1.000053]\n",
            "2555 [D loss: 0.999969] [G loss: 1.000050]\n",
            "2556 [D loss: 0.999962] [G loss: 1.000059]\n",
            "2557 [D loss: 0.999966] [G loss: 1.000040]\n",
            "2558 [D loss: 0.999964] [G loss: 1.000048]\n",
            "2559 [D loss: 0.999962] [G loss: 1.000059]\n",
            "2560 [D loss: 0.999974] [G loss: 1.000073]\n",
            "2561 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2562 [D loss: 0.999966] [G loss: 1.000043]\n",
            "2563 [D loss: 0.999971] [G loss: 1.000070]\n",
            "2564 [D loss: 0.999982] [G loss: 1.000041]\n",
            "2565 [D loss: 0.999974] [G loss: 1.000063]\n",
            "2566 [D loss: 0.999975] [G loss: 1.000056]\n",
            "2567 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2568 [D loss: 0.999971] [G loss: 1.000050]\n",
            "2569 [D loss: 0.999975] [G loss: 1.000067]\n",
            "2570 [D loss: 0.999974] [G loss: 1.000043]\n",
            "2571 [D loss: 0.999966] [G loss: 1.000047]\n",
            "2572 [D loss: 0.999981] [G loss: 1.000051]\n",
            "2573 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2574 [D loss: 0.999968] [G loss: 1.000056]\n",
            "2575 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2576 [D loss: 0.999970] [G loss: 1.000059]\n",
            "2577 [D loss: 0.999979] [G loss: 1.000065]\n",
            "2578 [D loss: 0.999974] [G loss: 1.000066]\n",
            "2579 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2580 [D loss: 0.999967] [G loss: 1.000058]\n",
            "2581 [D loss: 0.999974] [G loss: 1.000055]\n",
            "2582 [D loss: 0.999982] [G loss: 1.000049]\n",
            "2583 [D loss: 0.999968] [G loss: 1.000056]\n",
            "2584 [D loss: 0.999962] [G loss: 1.000073]\n",
            "2585 [D loss: 0.999975] [G loss: 1.000052]\n",
            "2586 [D loss: 0.999961] [G loss: 1.000065]\n",
            "2587 [D loss: 0.999981] [G loss: 1.000051]\n",
            "2588 [D loss: 0.999968] [G loss: 1.000069]\n",
            "2589 [D loss: 0.999977] [G loss: 1.000058]\n",
            "2590 [D loss: 0.999972] [G loss: 1.000041]\n",
            "2591 [D loss: 0.999973] [G loss: 1.000054]\n",
            "2592 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2593 [D loss: 0.999969] [G loss: 1.000072]\n",
            "2594 [D loss: 0.999980] [G loss: 1.000056]\n",
            "2595 [D loss: 0.999972] [G loss: 1.000050]\n",
            "2596 [D loss: 0.999971] [G loss: 1.000049]\n",
            "2597 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2598 [D loss: 0.999966] [G loss: 1.000057]\n",
            "2599 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2600 [D loss: 0.999970] [G loss: 1.000047]\n",
            "2601 [D loss: 0.999980] [G loss: 1.000053]\n",
            "2602 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2603 [D loss: 0.999970] [G loss: 1.000049]\n",
            "2604 [D loss: 0.999978] [G loss: 1.000062]\n",
            "2605 [D loss: 0.999974] [G loss: 1.000069]\n",
            "2606 [D loss: 0.999977] [G loss: 1.000057]\n",
            "2607 [D loss: 0.999968] [G loss: 1.000059]\n",
            "2608 [D loss: 0.999984] [G loss: 1.000055]\n",
            "2609 [D loss: 0.999971] [G loss: 1.000058]\n",
            "2610 [D loss: 0.999977] [G loss: 1.000051]\n",
            "2611 [D loss: 0.999965] [G loss: 1.000059]\n",
            "2612 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2613 [D loss: 0.999971] [G loss: 1.000052]\n",
            "2614 [D loss: 0.999987] [G loss: 1.000046]\n",
            "2615 [D loss: 0.999970] [G loss: 1.000056]\n",
            "2616 [D loss: 0.999978] [G loss: 1.000067]\n",
            "2617 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2618 [D loss: 0.999977] [G loss: 1.000062]\n",
            "2619 [D loss: 0.999972] [G loss: 1.000056]\n",
            "2620 [D loss: 0.999982] [G loss: 1.000062]\n",
            "2621 [D loss: 0.999957] [G loss: 1.000060]\n",
            "2622 [D loss: 0.999979] [G loss: 1.000050]\n",
            "2623 [D loss: 0.999984] [G loss: 1.000048]\n",
            "2624 [D loss: 0.999968] [G loss: 1.000044]\n",
            "2625 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2626 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2627 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2628 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2629 [D loss: 0.999987] [G loss: 1.000060]\n",
            "2630 [D loss: 0.999975] [G loss: 1.000033]\n",
            "2631 [D loss: 0.999965] [G loss: 1.000043]\n",
            "2632 [D loss: 0.999980] [G loss: 1.000054]\n",
            "2633 [D loss: 0.999973] [G loss: 1.000056]\n",
            "2634 [D loss: 0.999962] [G loss: 1.000066]\n",
            "2635 [D loss: 0.999983] [G loss: 1.000055]\n",
            "2636 [D loss: 0.999968] [G loss: 1.000057]\n",
            "2637 [D loss: 0.999962] [G loss: 1.000047]\n",
            "2638 [D loss: 0.999975] [G loss: 1.000047]\n",
            "2639 [D loss: 0.999964] [G loss: 1.000059]\n",
            "2640 [D loss: 0.999972] [G loss: 1.000053]\n",
            "2641 [D loss: 0.999974] [G loss: 1.000041]\n",
            "2642 [D loss: 0.999964] [G loss: 1.000062]\n",
            "2643 [D loss: 0.999968] [G loss: 1.000059]\n",
            "2644 [D loss: 0.999968] [G loss: 1.000041]\n",
            "2645 [D loss: 0.999961] [G loss: 1.000078]\n",
            "2646 [D loss: 0.999963] [G loss: 1.000070]\n",
            "2647 [D loss: 0.999964] [G loss: 1.000054]\n",
            "2648 [D loss: 0.999964] [G loss: 1.000039]\n",
            "2649 [D loss: 0.999978] [G loss: 1.000060]\n",
            "2650 [D loss: 0.999979] [G loss: 1.000059]\n",
            "2651 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2652 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2653 [D loss: 0.999976] [G loss: 1.000037]\n",
            "2654 [D loss: 0.999975] [G loss: 1.000034]\n",
            "2655 [D loss: 0.999965] [G loss: 1.000052]\n",
            "2656 [D loss: 0.999960] [G loss: 1.000069]\n",
            "2657 [D loss: 0.999978] [G loss: 1.000055]\n",
            "2658 [D loss: 0.999961] [G loss: 1.000058]\n",
            "2659 [D loss: 0.999962] [G loss: 1.000051]\n",
            "2660 [D loss: 0.999965] [G loss: 1.000063]\n",
            "2661 [D loss: 0.999961] [G loss: 1.000055]\n",
            "2662 [D loss: 0.999969] [G loss: 1.000061]\n",
            "2663 [D loss: 0.999974] [G loss: 1.000055]\n",
            "2664 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2665 [D loss: 0.999958] [G loss: 1.000062]\n",
            "2666 [D loss: 0.999970] [G loss: 1.000052]\n",
            "2667 [D loss: 0.999967] [G loss: 1.000061]\n",
            "2668 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2669 [D loss: 0.999971] [G loss: 1.000049]\n",
            "2670 [D loss: 0.999969] [G loss: 1.000060]\n",
            "2671 [D loss: 0.999962] [G loss: 1.000056]\n",
            "2672 [D loss: 0.999969] [G loss: 1.000078]\n",
            "2673 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2674 [D loss: 0.999974] [G loss: 1.000063]\n",
            "2675 [D loss: 0.999968] [G loss: 1.000074]\n",
            "2676 [D loss: 0.999972] [G loss: 1.000070]\n",
            "2677 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2678 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2679 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2680 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2681 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2682 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2683 [D loss: 0.999964] [G loss: 1.000066]\n",
            "2684 [D loss: 0.999974] [G loss: 1.000064]\n",
            "2685 [D loss: 0.999960] [G loss: 1.000069]\n",
            "2686 [D loss: 0.999975] [G loss: 1.000052]\n",
            "2687 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2688 [D loss: 0.999976] [G loss: 1.000045]\n",
            "2689 [D loss: 0.999976] [G loss: 1.000079]\n",
            "2690 [D loss: 0.999961] [G loss: 1.000066]\n",
            "2691 [D loss: 0.999979] [G loss: 1.000066]\n",
            "2692 [D loss: 0.999968] [G loss: 1.000061]\n",
            "2693 [D loss: 0.999974] [G loss: 1.000053]\n",
            "2694 [D loss: 0.999955] [G loss: 1.000050]\n",
            "2695 [D loss: 0.999971] [G loss: 1.000054]\n",
            "2696 [D loss: 0.999976] [G loss: 1.000064]\n",
            "2697 [D loss: 0.999977] [G loss: 1.000045]\n",
            "2698 [D loss: 0.999967] [G loss: 1.000060]\n",
            "2699 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2700 [D loss: 0.999976] [G loss: 1.000069]\n",
            "2701 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2702 [D loss: 0.999970] [G loss: 1.000049]\n",
            "2703 [D loss: 0.999968] [G loss: 1.000045]\n",
            "2704 [D loss: 0.999968] [G loss: 1.000058]\n",
            "2705 [D loss: 0.999978] [G loss: 1.000049]\n",
            "2706 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2707 [D loss: 0.999978] [G loss: 1.000057]\n",
            "2708 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2709 [D loss: 0.999968] [G loss: 1.000034]\n",
            "2710 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2711 [D loss: 0.999973] [G loss: 1.000073]\n",
            "2712 [D loss: 0.999979] [G loss: 1.000057]\n",
            "2713 [D loss: 0.999972] [G loss: 1.000053]\n",
            "2714 [D loss: 0.999966] [G loss: 1.000051]\n",
            "2715 [D loss: 0.999958] [G loss: 1.000053]\n",
            "2716 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2717 [D loss: 0.999960] [G loss: 1.000059]\n",
            "2718 [D loss: 0.999973] [G loss: 1.000054]\n",
            "2719 [D loss: 0.999978] [G loss: 1.000059]\n",
            "2720 [D loss: 0.999967] [G loss: 1.000063]\n",
            "2721 [D loss: 0.999964] [G loss: 1.000046]\n",
            "2722 [D loss: 0.999963] [G loss: 1.000055]\n",
            "2723 [D loss: 0.999959] [G loss: 1.000055]\n",
            "2724 [D loss: 0.999976] [G loss: 1.000061]\n",
            "2725 [D loss: 0.999961] [G loss: 1.000062]\n",
            "2726 [D loss: 0.999971] [G loss: 1.000058]\n",
            "2727 [D loss: 0.999988] [G loss: 1.000063]\n",
            "2728 [D loss: 0.999984] [G loss: 1.000060]\n",
            "2729 [D loss: 0.999966] [G loss: 1.000075]\n",
            "2730 [D loss: 0.999978] [G loss: 1.000054]\n",
            "2731 [D loss: 0.999977] [G loss: 1.000058]\n",
            "2732 [D loss: 0.999976] [G loss: 1.000039]\n",
            "2733 [D loss: 0.999980] [G loss: 1.000077]\n",
            "2734 [D loss: 0.999973] [G loss: 1.000058]\n",
            "2735 [D loss: 0.999973] [G loss: 1.000053]\n",
            "2736 [D loss: 0.999971] [G loss: 1.000052]\n",
            "2737 [D loss: 0.999975] [G loss: 1.000051]\n",
            "2738 [D loss: 0.999961] [G loss: 1.000065]\n",
            "2739 [D loss: 0.999961] [G loss: 1.000060]\n",
            "2740 [D loss: 0.999975] [G loss: 1.000065]\n",
            "2741 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2742 [D loss: 0.999966] [G loss: 1.000061]\n",
            "2743 [D loss: 0.999971] [G loss: 1.000087]\n",
            "2744 [D loss: 0.999976] [G loss: 1.000063]\n",
            "2745 [D loss: 0.999984] [G loss: 1.000063]\n",
            "2746 [D loss: 0.999973] [G loss: 1.000052]\n",
            "2747 [D loss: 0.999979] [G loss: 1.000040]\n",
            "2748 [D loss: 0.999959] [G loss: 1.000043]\n",
            "2749 [D loss: 0.999963] [G loss: 1.000057]\n",
            "2750 [D loss: 0.999978] [G loss: 1.000048]\n",
            "2751 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2752 [D loss: 0.999961] [G loss: 1.000066]\n",
            "2753 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2754 [D loss: 0.999973] [G loss: 1.000046]\n",
            "2755 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2756 [D loss: 0.999979] [G loss: 1.000039]\n",
            "2757 [D loss: 0.999979] [G loss: 1.000062]\n",
            "2758 [D loss: 0.999974] [G loss: 1.000070]\n",
            "2759 [D loss: 0.999961] [G loss: 1.000059]\n",
            "2760 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2761 [D loss: 0.999976] [G loss: 1.000077]\n",
            "2762 [D loss: 0.999970] [G loss: 1.000047]\n",
            "2763 [D loss: 0.999977] [G loss: 1.000067]\n",
            "2764 [D loss: 0.999966] [G loss: 1.000039]\n",
            "2765 [D loss: 0.999972] [G loss: 1.000046]\n",
            "2766 [D loss: 0.999966] [G loss: 1.000062]\n",
            "2767 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2768 [D loss: 0.999960] [G loss: 1.000058]\n",
            "2769 [D loss: 0.999975] [G loss: 1.000058]\n",
            "2770 [D loss: 0.999973] [G loss: 1.000074]\n",
            "2771 [D loss: 0.999958] [G loss: 1.000061]\n",
            "2772 [D loss: 0.999979] [G loss: 1.000049]\n",
            "2773 [D loss: 0.999977] [G loss: 1.000049]\n",
            "2774 [D loss: 0.999963] [G loss: 1.000053]\n",
            "2775 [D loss: 0.999980] [G loss: 1.000037]\n",
            "2776 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2777 [D loss: 0.999968] [G loss: 1.000056]\n",
            "2778 [D loss: 0.999976] [G loss: 1.000071]\n",
            "2779 [D loss: 0.999981] [G loss: 1.000065]\n",
            "2780 [D loss: 0.999975] [G loss: 1.000052]\n",
            "2781 [D loss: 0.999969] [G loss: 1.000058]\n",
            "2782 [D loss: 0.999971] [G loss: 1.000056]\n",
            "2783 [D loss: 0.999976] [G loss: 1.000053]\n",
            "2784 [D loss: 0.999967] [G loss: 1.000051]\n",
            "2785 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2786 [D loss: 0.999977] [G loss: 1.000063]\n",
            "2787 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2788 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2789 [D loss: 0.999978] [G loss: 1.000067]\n",
            "2790 [D loss: 0.999984] [G loss: 1.000060]\n",
            "2791 [D loss: 0.999959] [G loss: 1.000075]\n",
            "2792 [D loss: 0.999982] [G loss: 1.000064]\n",
            "2793 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2794 [D loss: 0.999972] [G loss: 1.000056]\n",
            "2795 [D loss: 0.999985] [G loss: 1.000055]\n",
            "2796 [D loss: 0.999968] [G loss: 1.000078]\n",
            "2797 [D loss: 0.999972] [G loss: 1.000043]\n",
            "2798 [D loss: 0.999966] [G loss: 1.000061]\n",
            "2799 [D loss: 0.999976] [G loss: 1.000058]\n",
            "2800 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2801 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2802 [D loss: 0.999963] [G loss: 1.000071]\n",
            "2803 [D loss: 0.999973] [G loss: 1.000053]\n",
            "2804 [D loss: 0.999962] [G loss: 1.000053]\n",
            "2805 [D loss: 0.999975] [G loss: 1.000062]\n",
            "2806 [D loss: 0.999966] [G loss: 1.000068]\n",
            "2807 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2808 [D loss: 0.999971] [G loss: 1.000057]\n",
            "2809 [D loss: 0.999978] [G loss: 1.000064]\n",
            "2810 [D loss: 0.999979] [G loss: 1.000055]\n",
            "2811 [D loss: 0.999975] [G loss: 1.000063]\n",
            "2812 [D loss: 0.999966] [G loss: 1.000065]\n",
            "2813 [D loss: 0.999965] [G loss: 1.000054]\n",
            "2814 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2815 [D loss: 0.999967] [G loss: 1.000064]\n",
            "2816 [D loss: 0.999979] [G loss: 1.000073]\n",
            "2817 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2818 [D loss: 0.999972] [G loss: 1.000057]\n",
            "2819 [D loss: 0.999982] [G loss: 1.000062]\n",
            "2820 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2821 [D loss: 0.999976] [G loss: 1.000042]\n",
            "2822 [D loss: 0.999977] [G loss: 1.000071]\n",
            "2823 [D loss: 0.999968] [G loss: 1.000052]\n",
            "2824 [D loss: 0.999968] [G loss: 1.000048]\n",
            "2825 [D loss: 0.999977] [G loss: 1.000057]\n",
            "2826 [D loss: 0.999967] [G loss: 1.000070]\n",
            "2827 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2828 [D loss: 0.999981] [G loss: 1.000042]\n",
            "2829 [D loss: 0.999970] [G loss: 1.000079]\n",
            "2830 [D loss: 0.999969] [G loss: 1.000070]\n",
            "2831 [D loss: 0.999974] [G loss: 1.000066]\n",
            "2832 [D loss: 0.999981] [G loss: 1.000050]\n",
            "2833 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2834 [D loss: 0.999971] [G loss: 1.000051]\n",
            "2835 [D loss: 0.999973] [G loss: 1.000045]\n",
            "2836 [D loss: 0.999976] [G loss: 1.000065]\n",
            "2837 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2838 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2839 [D loss: 0.999974] [G loss: 1.000048]\n",
            "2840 [D loss: 0.999975] [G loss: 1.000065]\n",
            "2841 [D loss: 0.999965] [G loss: 1.000052]\n",
            "2842 [D loss: 0.999967] [G loss: 1.000078]\n",
            "2843 [D loss: 0.999976] [G loss: 1.000047]\n",
            "2844 [D loss: 0.999972] [G loss: 1.000059]\n",
            "2845 [D loss: 0.999968] [G loss: 1.000053]\n",
            "2846 [D loss: 0.999968] [G loss: 1.000060]\n",
            "2847 [D loss: 0.999986] [G loss: 1.000031]\n",
            "2848 [D loss: 0.999959] [G loss: 1.000068]\n",
            "2849 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2850 [D loss: 0.999961] [G loss: 1.000059]\n",
            "2851 [D loss: 0.999955] [G loss: 1.000064]\n",
            "2852 [D loss: 0.999978] [G loss: 1.000054]\n",
            "2853 [D loss: 0.999965] [G loss: 1.000052]\n",
            "2854 [D loss: 0.999973] [G loss: 1.000051]\n",
            "2855 [D loss: 0.999981] [G loss: 1.000055]\n",
            "2856 [D loss: 0.999964] [G loss: 1.000061]\n",
            "2857 [D loss: 0.999983] [G loss: 1.000071]\n",
            "2858 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2859 [D loss: 0.999973] [G loss: 1.000071]\n",
            "2860 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2861 [D loss: 0.999967] [G loss: 1.000045]\n",
            "2862 [D loss: 0.999986] [G loss: 1.000051]\n",
            "2863 [D loss: 0.999976] [G loss: 1.000067]\n",
            "2864 [D loss: 0.999975] [G loss: 1.000075]\n",
            "2865 [D loss: 0.999980] [G loss: 1.000066]\n",
            "2866 [D loss: 0.999979] [G loss: 1.000055]\n",
            "2867 [D loss: 0.999973] [G loss: 1.000052]\n",
            "2868 [D loss: 0.999969] [G loss: 1.000048]\n",
            "2869 [D loss: 0.999971] [G loss: 1.000051]\n",
            "2870 [D loss: 0.999977] [G loss: 1.000073]\n",
            "2871 [D loss: 0.999976] [G loss: 1.000055]\n",
            "2872 [D loss: 0.999977] [G loss: 1.000049]\n",
            "2873 [D loss: 0.999965] [G loss: 1.000052]\n",
            "2874 [D loss: 0.999969] [G loss: 1.000040]\n",
            "2875 [D loss: 0.999982] [G loss: 1.000065]\n",
            "2876 [D loss: 0.999967] [G loss: 1.000055]\n",
            "2877 [D loss: 0.999965] [G loss: 1.000059]\n",
            "2878 [D loss: 0.999972] [G loss: 1.000058]\n",
            "2879 [D loss: 0.999971] [G loss: 1.000043]\n",
            "2880 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2881 [D loss: 0.999965] [G loss: 1.000062]\n",
            "2882 [D loss: 0.999975] [G loss: 1.000066]\n",
            "2883 [D loss: 0.999972] [G loss: 1.000054]\n",
            "2884 [D loss: 0.999971] [G loss: 1.000073]\n",
            "2885 [D loss: 0.999979] [G loss: 1.000049]\n",
            "2886 [D loss: 0.999964] [G loss: 1.000058]\n",
            "2887 [D loss: 0.999977] [G loss: 1.000085]\n",
            "2888 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2889 [D loss: 0.999969] [G loss: 1.000050]\n",
            "2890 [D loss: 0.999970] [G loss: 1.000058]\n",
            "2891 [D loss: 0.999976] [G loss: 1.000053]\n",
            "2892 [D loss: 0.999968] [G loss: 1.000074]\n",
            "2893 [D loss: 0.999973] [G loss: 1.000061]\n",
            "2894 [D loss: 0.999974] [G loss: 1.000040]\n",
            "2895 [D loss: 0.999963] [G loss: 1.000059]\n",
            "2896 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2897 [D loss: 0.999974] [G loss: 1.000042]\n",
            "2898 [D loss: 0.999966] [G loss: 1.000060]\n",
            "2899 [D loss: 0.999966] [G loss: 1.000056]\n",
            "2900 [D loss: 0.999979] [G loss: 1.000045]\n",
            "2901 [D loss: 0.999969] [G loss: 1.000057]\n",
            "2902 [D loss: 0.999965] [G loss: 1.000065]\n",
            "2903 [D loss: 0.999968] [G loss: 1.000058]\n",
            "2904 [D loss: 0.999967] [G loss: 1.000065]\n",
            "2905 [D loss: 0.999979] [G loss: 1.000047]\n",
            "2906 [D loss: 0.999975] [G loss: 1.000060]\n",
            "2907 [D loss: 0.999982] [G loss: 1.000051]\n",
            "2908 [D loss: 0.999972] [G loss: 1.000044]\n",
            "2909 [D loss: 0.999972] [G loss: 1.000053]\n",
            "2910 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2911 [D loss: 0.999959] [G loss: 1.000058]\n",
            "2912 [D loss: 0.999982] [G loss: 1.000051]\n",
            "2913 [D loss: 0.999966] [G loss: 1.000069]\n",
            "2914 [D loss: 0.999982] [G loss: 1.000069]\n",
            "2915 [D loss: 0.999971] [G loss: 1.000053]\n",
            "2916 [D loss: 0.999969] [G loss: 1.000058]\n",
            "2917 [D loss: 0.999970] [G loss: 1.000058]\n",
            "2918 [D loss: 0.999968] [G loss: 1.000044]\n",
            "2919 [D loss: 0.999977] [G loss: 1.000045]\n",
            "2920 [D loss: 0.999986] [G loss: 1.000043]\n",
            "2921 [D loss: 0.999966] [G loss: 1.000065]\n",
            "2922 [D loss: 0.999969] [G loss: 1.000053]\n",
            "2923 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2924 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2925 [D loss: 0.999968] [G loss: 1.000057]\n",
            "2926 [D loss: 0.999968] [G loss: 1.000045]\n",
            "2927 [D loss: 0.999964] [G loss: 1.000059]\n",
            "2928 [D loss: 0.999972] [G loss: 1.000059]\n",
            "2929 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2930 [D loss: 0.999974] [G loss: 1.000058]\n",
            "2931 [D loss: 0.999973] [G loss: 1.000062]\n",
            "2932 [D loss: 0.999972] [G loss: 1.000085]\n",
            "2933 [D loss: 0.999971] [G loss: 1.000039]\n",
            "2934 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2935 [D loss: 0.999960] [G loss: 1.000051]\n",
            "2936 [D loss: 0.999957] [G loss: 1.000057]\n",
            "2937 [D loss: 0.999967] [G loss: 1.000060]\n",
            "2938 [D loss: 0.999966] [G loss: 1.000055]\n",
            "2939 [D loss: 0.999973] [G loss: 1.000074]\n",
            "2940 [D loss: 0.999970] [G loss: 1.000059]\n",
            "2941 [D loss: 0.999973] [G loss: 1.000072]\n",
            "2942 [D loss: 0.999984] [G loss: 1.000052]\n",
            "2943 [D loss: 0.999971] [G loss: 1.000053]\n",
            "2944 [D loss: 0.999979] [G loss: 1.000051]\n",
            "2945 [D loss: 0.999975] [G loss: 1.000058]\n",
            "2946 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2947 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2948 [D loss: 0.999981] [G loss: 1.000051]\n",
            "2949 [D loss: 0.999975] [G loss: 1.000065]\n",
            "2950 [D loss: 0.999967] [G loss: 1.000055]\n",
            "2951 [D loss: 0.999981] [G loss: 1.000069]\n",
            "2952 [D loss: 0.999979] [G loss: 1.000049]\n",
            "2953 [D loss: 0.999970] [G loss: 1.000052]\n",
            "2954 [D loss: 0.999978] [G loss: 1.000057]\n",
            "2955 [D loss: 0.999974] [G loss: 1.000068]\n",
            "2956 [D loss: 0.999974] [G loss: 1.000062]\n",
            "2957 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2958 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2959 [D loss: 0.999969] [G loss: 1.000052]\n",
            "2960 [D loss: 0.999960] [G loss: 1.000065]\n",
            "2961 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2962 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2963 [D loss: 0.999966] [G loss: 1.000068]\n",
            "2964 [D loss: 0.999980] [G loss: 1.000052]\n",
            "2965 [D loss: 0.999959] [G loss: 1.000056]\n",
            "2966 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2967 [D loss: 0.999970] [G loss: 1.000057]\n",
            "2968 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2969 [D loss: 0.999973] [G loss: 1.000060]\n",
            "2970 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2971 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2972 [D loss: 0.999977] [G loss: 1.000064]\n",
            "2973 [D loss: 0.999979] [G loss: 1.000071]\n",
            "2974 [D loss: 0.999970] [G loss: 1.000072]\n",
            "2975 [D loss: 0.999968] [G loss: 1.000056]\n",
            "2976 [D loss: 0.999976] [G loss: 1.000053]\n",
            "2977 [D loss: 0.999973] [G loss: 1.000069]\n",
            "2978 [D loss: 0.999972] [G loss: 1.000058]\n",
            "2979 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2980 [D loss: 0.999982] [G loss: 1.000046]\n",
            "2981 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2982 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2983 [D loss: 0.999956] [G loss: 1.000044]\n",
            "2984 [D loss: 0.999962] [G loss: 1.000054]\n",
            "2985 [D loss: 0.999975] [G loss: 1.000049]\n",
            "2986 [D loss: 0.999960] [G loss: 1.000062]\n",
            "2987 [D loss: 0.999965] [G loss: 1.000070]\n",
            "2988 [D loss: 0.999974] [G loss: 1.000057]\n",
            "2989 [D loss: 0.999978] [G loss: 1.000061]\n",
            "2990 [D loss: 0.999972] [G loss: 1.000075]\n",
            "2991 [D loss: 0.999973] [G loss: 1.000071]\n",
            "2992 [D loss: 0.999970] [G loss: 1.000070]\n",
            "2993 [D loss: 0.999954] [G loss: 1.000065]\n",
            "2994 [D loss: 0.999976] [G loss: 1.000064]\n",
            "2995 [D loss: 0.999965] [G loss: 1.000065]\n",
            "2996 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2997 [D loss: 0.999959] [G loss: 1.000059]\n",
            "2998 [D loss: 0.999969] [G loss: 1.000056]\n",
            "2999 [D loss: 0.999978] [G loss: 1.000056]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoYAAAHwCAYAAAA7Ga44AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXdYFNfXx7+XLogNsaICdkEEBRvB\nrsQWjdGYaKzRxMRojNFooonGNGts8afxNRqTmGhiLIk9FuwN7L2iYhcEBKTf94+dGWZnZyu7sOD5\nPI+P7MydO3dmd+5877nnnMs45yAIgiAIgiAIh8JuAEEQBEEQBGEfkDAkCIIgCIIgAJAwJAiCIAiC\nIARIGBIEQRAEQRAASBgSBEEQBEEQAiQMCYIgCIIgCAAkDAmCIAiCIAgBEoYEQRAAGGOfMsa2KrZd\n1bPtDeFvxhj7gDF2hjGWxhh7wBiLEvcrjvuZMZbNGKus2D6VMcYZY6/LtjkJ23z1tDWKMZbOGEth\njD1hjK2T1yur80PFcR8K26fKtn3GGLsp1BXHGFtjwu0iCKKYQsKQIAhCwz4ALRljjgAgCC1nACGK\nbbWEsgCwAMAYAB8D8AJQFcBkAC/LK2aMeQB4DUASgLdUzp0A4EvxPCbyAee8pNCekgBmK/ZfATBQ\nsW2QsF1s1yAAAwB0EOoKBbDLjDYQBFHMIGFIEASh4Tg0QjBY+BwBYA+Ay4pt1znn9xhjdQC8D+AN\nzvl/nPPnnPMczvkBzvlgRd2vAUgEMA0acaZkG4BMqItGg3DOEwFskLVRfj3ujLEAABD+dxO2i4QB\n2M45vy7U9YBzvtTcNhAEUXwgYUgQBAGAc54J4CiAVsKmVgD2Azig2CZaC9sBuMM5jzah+kEA/gCw\nGkA9xlgT5ekBfA5gCmPM2Zx2M8a8APQCcE1l96/IsxoOEj7LOQJgIGNsPGMs1EyLJUEQxRAShgRB\nEHnsRZ4IjIBGGO5XbNsr/F0ewAP5wYKPXqLg/1dD2FYdQFsAv3POH0IzVauc4gXn/B8AjwEMM7Gt\nCxhjSQCeCG0ZpVLmNwBvCmLzDeGz/Jy/CcdFCtf1iDE2wcTzEwRRDCFhSBAEkcc+AC8xxsoB8Oac\nXwVwCBrfw3IAApFnMYwHoBVIwjn3gUakuQJgwuYBAC5yzk8Jn1cB6KfHMjgZwCRopnyNMZpzXhpA\nEICyAHyUBTjnt6GxJH4L4Crn/I5KmVWc8w4AygAYAeArxlikCecnCKIYQsKQIAgij8MASgMYDuAg\nAHDOkwHcE7bd45zfFMruBuDDGAs1UudAAP5CxPIDAN9DIx67KAtyzv+DRsi9b2qDOednAXwNYBFj\njKkU+QWa4JhfjNSTxTn/C8AZaAQwQRAvICQMCYIgBDjnzwFEAxgLzRSyyAFh2z5Z2csAfgSwmjHW\nkTFWQvDRaymWYYy1AFATQFNogkOCoRFdv0NlOllgEoBPzGz6SgAVAbyism8NgE4A/lTuYIwNZox1\nZYx5MsYcGGOdAQRA42tJEMQLCAlDgiAIbfYCqACNGBTZL2zbpyg7EpqUNd9Dk3ImDsBXAPoCuA1N\nwMdGzvlZIeL3Aef8AYD5ALoJ09NacM4PAjhmToOFwJn50ASwKPc955zvFESvkmQAnwltTQQwE8B7\nnPMDKmUJgngBYJzzwm4DQRAEQRAEYQeQxZAgCIIgCIIAQMKQIAiCIAiCECBhSBAEQRAEQQAgYUgQ\nBEEQBEEIkDAkCIIgCIIgAABOhd2Aokr58uW5r69vYTeDIAiCIAjCKDExMU84597GypEwtBBfX19E\nR0cXdjMIgiAIgiCMwhi7ZUo5mkomCIIgCIIgAJAwJAiCIAiCIARIGBIEQRAEQRAAyMeQIAiCIIot\nWVlZiIuLQ3p6emE3hSgg3Nzc4OPjA2dnZ4uOJ2FIEARBEMWUuLg4eHp6wtfXF4yxwm4OYWM454iP\nj0dcXBz8/PwsqoOmkgmCIAiimJKeng4vLy8ShS8IjDF4eXnly0JMwpAgCIIgijEkCl8s8vt9kzAk\nCIIgCMJmODo6Ijg4GAEBAWjUqBHmzJmD3NxcAEB0dDRGjx6d73MsWbIEv/zyi1nHtGzZ0uLz/fzz\nz7h3757FxwPA1KlTMXv27HzVYQvIx5AgCIIgCJtRokQJnDp1CgDw6NEj9OvXD8nJyfjyyy8RGhqK\n0NDQfNWfnZ2NESNGmH3coUOHLD7nzz//jMDAQFSpUsXkY3JycuDo6GjxOQsKshgSBEEQBFEgVKhQ\nAUuXLsUPP/wAzjmioqLQrVs3AMDevXsRHByM4OBghISE4NmzZwCAGTNmoGHDhmjUqBEmTpwIAGjT\npg3GjBmD0NBQzJ8/X8v61qZNG3z00UcIDQ1F/fr1cfz4cfTq1Qu1a9fG5MmTpbaULFkSABAVFYU2\nbdqgd+/eqFevHvr37w/OOQBg2rRpCAsLQ2BgIN555x1wzrF27VpER0ejf//+CA4OxvPnz7Fr1y6E\nhISgYcOGGDp0KDIyMgBoVkmbMGECGjdujL/++kvvfTl16hSaN2+OoKAgvPrqq3j69CkAYMGCBWjQ\noAGCgoLwxhtvGLxP1oIshgRBEATxAvDlv+dx4V6yVetsUKUUpnQPMOsYf39/5OTk4NGjR1rbZ8+e\njUWLFiE8PBwpKSlwc3PD1q1bsXHjRhw9ehTu7u5ISEiQymdmZkpL006dOlWrLhcXF0RHR2P+/Pno\n0aMHYmJiUK5cOdSsWRMfffQRvLy8tMqfPHkS58+fR5UqVRAeHo6DBw/ipZdewgcffIAvvvgCADBg\nwABs2rQJvXv3xg8//IDZs2cjNDQU6enpGDx4MHbt2oU6depg4MCBWLx4McaMGQMA8PLywokTJwze\nk4EDB2LhwoVo3bo1vvjiC3z55ZeYN28epk+fjps3b8LV1RWJiYl675M1IYshQRAEQRCFTnh4OMaO\nHYsFCxYgMTERTk5O2LlzJ4YMGQJ3d3cAQLly5aTyffv21VvXK6+8AgBo2LAhAgICULlyZbi6usLf\n3x937tzRKd+0aVP4+PjAwcEBwcHBiI2NBQDs2bMHzZo1Q8OGDbF7926cP39e59jLly/Dz88PderU\nAQAMGjQI+/btM6mdAJCUlITExES0bt1a5/igoCD0798fv/32G5ycnPTeJ2tCFkOCIAiCeAEw17Jn\nK27cuAFHR0dUqFABFy9elLZPnDgRXbt2xZYtWxAeHo7t27cbrMfDw0PvPldXVwCAg4OD9Lf4OTs7\nW295QBMsk52djfT0dLz//vuIjo5GtWrVMHXqVIvSwBhqpzE2b96Mffv24d9//8U333yDs2fPqt6n\nevXqWXwOJWQxJAiCIAiiQHj8+DFGjBiBDz74QCetyvXr19GwYUNMmDABYWFhuHTpEjp27IgVK1Yg\nLS0NALSmkm2NKALLly+PlJQUrF27Vtrn6ekp+fbVrVsXsbGxuHbtGgDg119/lax/plC6dGmULVsW\n+/fv1zo+NzcXd+7cQdu2bTFjxgwkJSUhJSVF9T5ZE7IYEgRBEARhM54/f47g4GBkZWXByckJAwYM\nwNixY3XKzZs3D3v27IGDgwMCAgLQuXNnuLq64tSpUwgNDYWLiwu6dOmCb7/9tkDaXaZMGQwfPhyB\ngYGoVKkSwsLCpH2DBw/GiBEjUKJECRw+fBgrVqxAnz59kJ2djbCwMLOjpFeuXIkRI0YgLS0N/v7+\nWLFiBXJycvDWW28hKSkJnHOMHj0aZcqUweeff65zn6wJEyNvCPMIDQ3lotMrQRAEQdgjFy9eRP36\n9Qu7GUQBo/a9M8ZiOOdGcwPRVDJBEARBEAQBgIShXbNk73X4TtyM9Kycwm4KQRAEQRAvACQM7Zgl\ne68DANIySRgSBEEQBGF7SBjaMaL7Jy1/ThAEQRBEQUDC0I7JFZQhhQcRBEEQBFEQkDC0UxJSM/Es\nXZOEMyeXpCFBEARBELaHhKGd0nPRQenvlAzdLO0EQRAEURR4+PAh+vXrB39/fzRp0gQtWrTA+vXr\nC609UVFROHToUL7r6Natm5VaZF+QMLRTbiekSX+/sfRwIbaEIAiCICyDc46ePXuiVatWuHHjBmJi\nYrB69WrExcXZ9Lxqy96JWCIMDdVX3CBhWAR4mJxR2E0gCIIgCLPZvXs3XFxctFYCqVGjBkaNGgUA\nyMnJwfjx4xEWFoagoCD8+OOPADTirU2bNujduzfq1auH/v37Q1yQIyYmBq1bt0aTJk0QGRmJ+/fv\nAwDatGmDMWPGIDQ0FPPnz8e///6LZs2aISQkBB06dMDDhw8RGxuLJUuWYO7cuQgODsb+/fsRGxuL\ndu3aISgoCO3bt8ft27cB5K1u0qxZM3zyySd6rzEhIQE9e/ZEUFAQmjdvjjNnzgAA9u7di+DgYAQH\nByMkJATPnj3D/fv30apVKwQHByMwMFBaBs+eoCXxCIIgCOJFYOtE4MFZ69ZZqSHQebre3efPn0fj\nxo317v/pp59QunRpHD9+HBkZGQgPD0enTp0AACdPnsT58+dRpUoVhIeH4+DBg2jWrBlGjRqFjRs3\nwtvbG2vWrMGkSZOwfPlyAEBmZibEVcmePn2KI0eOgDGGZcuWYebMmZgzZw5GjBiBkiVLYty4cQCA\n7t27Y9CgQRg0aBCWL1+O0aNHY8OGDQCAuLg4HDp0CI6OjnqvYcqUKQgJCcGGDRuwe/duDBw4EKdO\nncLs2bOxaNEihIeHIyUlBW5ubli6dCkiIyMxadIk5OTkSGtA2xMkDO0UxvLS1RAEQRBEcWDkyJE4\ncOAAXFxccPz4cezYsQNnzpzB2rVrAQBJSUm4evUqXFxc0LRpU/j4+AAAgoODERsbizJlyuDcuXPo\n2LEjAI3FsXLlylL9ffv2lf6Oi4tD3759cf/+fWRmZsLPz0+1TYcPH8a6desAAAMGDNCyDvbp08eg\nKASAAwcO4O+//wYAtGvXDvHx8UhOTkZ4eDjGjh2L/v37o1evXvDx8UFYWBiGDh2KrKws9OzZE8HB\nwebeQptDwtBOcWQM2aQMCYIgCGthwLJnKwICAiTRBACLFi3CkydPEBqqWbKXc46FCxciMjJS67io\nqCi4urpKnx0dHZGdnQ3OOQICAnD4sLrvvYeHh/T3qFGjMHbsWLzyyiuIiorC1KlTzW6/vD5zmThx\nIrp27YotW7YgPDwc27dvR6tWrbBv3z5s3rwZgwcPxtixYzFw4ECLz2ELyMfQTiFJSBAEQRR12rVr\nh/T0dCxevFjaJp8+jYyMxOLFi5GVlQUAuHLlClJTU/XWV7duXTx+/FgShllZWTh//rxq2aSkJFSt\nWhUAsHLlSmm7p6cnnj17Jn1u2bIlVq9eDQBYtWoVIiIizLrGiIgIrFq1CoBG0JYvXx6lSpXC9evX\n0bBhQ0yYMAFhYWG4dOkSbt26hYoVK2L48OEYNmwYTpw4Yda5CgIShnaKMndhYlpmIbWEIAiCICyD\nMYYNGzZg79698PPzQ9OmTTFo0CDMmDEDADBs2DA0aNAAjRs3RmBgIN59912DEcAuLi5Yu3YtJkyY\ngEaNGiE4OFhvhPHUqVPRp08fNGnSBOXLl5e2d+/eHevXr5eCTxYuXIgVK1YgKCgIv/76K+bPn2/W\nNU6dOhUxMTEICgrCxIkTJRE6b948BAYGIigoCM7OzujcuTOioqLQqFEjhISEYM2aNfjwww/NOldB\nwDhNV1pEaGgoFx1cbUGDL7ZprZE8PrIuRratZbPzEQRBEMWPixcvon79+oXdDKKAUfveGWMxnPNQ\nY8eSxdBOmfFaECqWyvOvKFXCuRBbQxAEQRDEiwAJQzule6Mq+KpHoPT5XFxSIbaGIAiCIIgXARKG\ndkwJl7wQ+TXRdwqxJQRBEARBvAiQMLRjmvl5FXYTCIIgCIJ4gSBhaMe4ONHXQxAEQRBEwUHKgyAI\ngiAIggBAwpAgCIIgCBvy8OFD9OvXD/7+/mjSpAlatGiB9evXF1p7oqKi9OY+NKeObt26WalF9gUJ\nwyIE5ZwkCIIgihKcc/Ts2ROtWrXCjRs3EBMTg9WrVyMuLs6m5zWUJNsSYWiovuIGCcMiRFYOCUOC\nIAii6LB79264uLhgxIgR0rYaNWpg1KhRAICcnByMHz8eYWFhCAoKwo8//ghAI97atGmD3r17o169\neujfv79kHImJiUHr1q3RpEkTREZG4v79+wCANm3aYMyYMQgNDcX8+fPx77//olmzZggJCUGHDh3w\n8OFDxMbGYsmSJZg7d6608klsbCzatWuHoKAgtG/fHrdv3wYADB48GCNGjECzZs3wySef6L3GhIQE\n9OzZE0FBQWjevDnOnDkDANi7dy+Cg4MRHByMkJAQPHv2DPfv30erVq0QHByMwMBA7N+/3/o3PZ84\nFXYDCMPM6xuMMWtOAQCyc3PhQlqeIAiCsIAZx2bgUsIlq9ZZr1w9TGg6Qe/+8+fPo3Hjxnr3//TT\nTyhdujSOHz+OjIwMhIeHo1OnTgCAkydP4vz586hSpQrCw8Nx8OBBNGvWDKNGjcLGjRvh7e2NNWvW\nYNKkSVi+fDkAIDMzE+KqZE+fPsWRI0fAGMOyZcswc+ZMzJkzByNGjEDJkiUxbtw4AJol8gYNGoRB\ngwZh+fLlGD16NDZs2AAAiIuLw6FDh+Do6KjSeg1TpkxBSEgINmzYgN27d2PgwIE4deoUZs+ejUWL\nFiE8PBwpKSlwc3PD0qVLERkZiUmTJiEnJ0dr3Wh7gYShndMjuIokDKNjn6JVHe9CbhFBEARBWMbI\nkSNx4MABuLi44Pjx49ixYwfOnDmDtWvXAgCSkpJw9epVuLi4oGnTpvDx8QEABAcHIzY2FmXKlMG5\nc+fQsWNHABqLY+XKlaX6+/btK/0dFxeHvn374v79+8jMzISfn59qmw4fPox169YBAAYMGKBlHezT\np49BUQgABw4cwN9//w0AaNeuHeLj45GcnIzw8HCMHTsW/fv3R69eveDj44OwsDAMHToUWVlZ6Nmz\nJ4KDg829hTaHhKGdwxiT/r72KIWEIUEQBGERhix7tiIgIEASTQCwaNEiPHnyBKGhmiV7OedYuHAh\nIiMjtY6LioqCq2vesrCOjo7Izs4G5xwBAQE4fPiw6vk8PDykv0eNGoWxY8filVdeQVRUFKZOnWp2\n++X1mcvEiRPRtWtXbNmyBeHh4di+fTtatWqFffv2YfPmzRg8eDDGjh2LgQMHWnwOW0DzkkWIaZsu\nFHYTCIIgCMJk2rVrh/T0dCxevFjaJp8+jYyMxOLFi5GVlQUAuHLlClJTU/XWV7duXTx+/FgShllZ\nWTh//rxq2aSkJFStWhUAsHLlSmm7p6cnnj17Jn1u2bIlVq9eDQBYtWoVIiIizLrGiIgIrFq1CoBG\n0JYvXx6lSpXC9evX0bBhQ0yYMAFhYWG4dOkSbt26hYoVK2L48OEYNmwYTpw4Yda5CgKyGBIEQRAE\nYRMYY9iwYQM++ugjzJw5E97e3vDw8MCMGTMAAMOGDUNsbCwaN24Mzjm8vb0l/z41XFxcsHbtWowe\nPRpJSUnIzs7GmDFjEBAQoFN26tSp6NOnD8qWLYt27drh5s2bADQ+hb1798bGjRuxcOFCLFy4EEOG\nDMGsWbPg7e2NFStWmHWNU6dOxdChQxEUFAR3d3dJhM6bNw979uyBg4MDAgIC0LlzZ6xevRqzZs2C\ns7MzSpYsiV9++cWscxUEjFKgWEZoaCgXHVxtje/EzdLfsdO7Fsg5CYIgiKLPxYsXUb9+/cJuBlHA\nqH3vjLEYznmosWMLdSqZMfYyY+wyY+waY2yiyn5XxtgaYf9RxpivbN+nwvbLjLFIY3UyxvyEOq4J\ndboI2wczxh4zxk4J/4bZ9qoJgiAIgiDsk0IThowxRwCLAHQG0ADAm4yxBopibwN4yjmvBWAugBnC\nsQ0AvAEgAMDLAP7HGHM0UucMAHOFup4KdYus4ZwHC/+W2eByCYIgCIIg7J7CtBg2BXCNc36Dc54J\nYDWAHooyPQCIHqNrAbRnmjDdHgBWc84zOOc3AVwT6lOtUzimnVAHhDp72vDarMrglr6F3QSCIAiC\nIF4AClMYVgVwR/Y5TtimWoZzng0gCYCXgWP1bfcCkCjUoXau1xhjZxhjaxlj1fJzUbbgrebVC7sJ\nBEEQRBGFYgleLPL7fVO6GuBfAL6c8yAA/yHPQqkDY+wdxlg0Yyz68ePHBdZAB1kuw9xcesAJgiAI\n03Bzc0N8fDyJwxcEzjni4+Ph5uZmcR2Fma7mLgC5dc5H2KZWJo4x5gSgNIB4I8eqbY8HUIYx5iRY\nDaXynPN4WfllAGbqazDnfCmApYAmKtn4JVoHJ4c8/Z6dy+HiwAyUJgiCIAgNPj4+iIuLQ0EaM4jC\nxc3NTVoxxhIKUxgeB1CbMeYHjUh7A0A/RZl/AAwCcBhAbwC7OeecMfYPgN8ZY98DqAKgNoBjAJha\nncIxe4Q6Vgt1bgQAxlhlzvl94XyvALhoqwu2FJnBELk06iMIgiBMxNnZWe9ScAShRqFNJQuWuw8A\nbIdGjP3JOT/PGJvGGHtFKPYTAC/G2DUAYwFMFI49D+BPABcAbAMwknOeo69Ooa4JAMYKdXkJdQPA\naMbYecbYaQCjAQy25XVbQkZ2rvR3Dk0l2wUjfz+BrWfvGy9IEARBEEUISnBtIQWZ4PpOQhoiZu4B\nAJz4vCPKebgUyHkJ/YhJxynhOEEQBFEUKBIJrgnTqFbOXfp7bcwdAyUJgiCIogznHIlpmYXdDLvm\nTkJagQRi3opPxZ7Lj2xW/1/RdzDgp6M2q99SSBgWEV4N0WTX8fJwLeSWEARBvJiM/P0Efjtyy6bn\n+OXwLQRP+w83n6Ta9DxFlRuPUxAxcw9+2HPN5udqPSsKQ1Yct1n949eewf6rT2xWv6WQMCwiDAn3\nBQCULuFcuA0hkJqRbbxQMSM3l+OLjedw+cGzwm6KWbz983H0+78jhd0Mopiw+cx9TN5wzuzjcnM5\nfjpwE2mZxvuOnRcfAtBYqwzx7ZaLmPj3GbPbUtS5n5QOADhyI95ISfvGnt34SBgWEZwdNV9VVk6u\nkZJFh0fJ6dh05p5N6uacIzPbNvfqg99P2KRee+bhs3T8cviWXU57GGLXpUc4dL1ov0CKGnFP03D9\ncUphN8Ou2HHhAb7adAEztl4y+Rh5/lo1lu67gdXH1V2LomMT8OhZulltLCyuPUrBogKw/hUkubkc\nKw7eRHpWjur+mFsJ8Pt0SwG3ynRIGBYRnB01nUSWGX4VD5PTMfe/KzYbmfT630GET98NALgdn4Zz\nd5PMOn7g8mP44PeTSLGCBW7vlcfwnbgZT1IyAADTt11CnclbkZGt/mDmh5N3Eq1ep73jIgxMnuvp\n6Ijiw6qjt/BntOW+zC/N2IP2c/ZasUXmk52Ta1eD6PQsTVuepmUZLSt210Z0oUF6LzmMLvMPWF5B\nAfL6j4cxa/tlk6ypcuzY4IaNp+/iy38vYMGuq6r7D12z78EqCcMigmQxNMMKNvqPk5i/6yq6LDhg\nk07yxO1E3E18DgBoNWsPui00ryO6+1RzrDVyMy7bfwMAcOFeMgDgt8MaPyBbWA0LIr345QfPsO3c\ngwI4k2mI39DzzPwLw9N3EjHur9M6zuODVxxD5Nx9+a7fHolPycDjZxmF3QyTmLT+HD5ZW7SnKFvP\nikKDL7YVdjMkHIRFCQz1dUv3Xce+K4/BhaeNqfQ0vRcfwoqDN006pzhItneM9Sl/HLuNe8J7BtDt\nf689SjGprzx9J9EmhgI1ElI1A4A0PdfmYOeLVJAwLCI4CcIwO9d0oSNa4i7eT0Z07FObtKuwyc7J\nxfPMHEn4OgkPXI4Nh5MsP0N5E4mctw8jfoux+XlMRXyhZVshErDHooNYGxOHpOfa1pOoy49x+WHh\n+TDuOP8AW2yUm7LJ1zsR9s1OzN95NV8W/KycXHz852mj/mcFyT+n72HS+rOFcu70rBzcUJm2vpv4\nHFk59mNSEnWAoa/+2y2XMHD5MYhdvFo3E33rKb7894JV2tTkq//w+o+HrVKXNVC7N0nPs/DpurN4\na5muC4sooDt8vxcjfovBsv03tITfnYQ0aSo39kkqeiw6iK82ae7dP6fv4U890/CWkPQ8C9vP54lT\nsR2uznkS69qjFIR+vRMPk9PhqBCGCan2FYVOwrCIIE4lZ5rR2SmTYR+89gTZVrAcnrqTKOXxyw+G\nrmTezivw+1T3HPEpGaj12RYcvKaJ5Bq9+iTqf7FNulbxgRM7V/t5NRRt5J32PsW0PQAMWxmNVjP3\nwHfiZpyJM22q3Vaj5uycXPwdE2d2Oot3fo3B+6vU/UdXH7uNzy0IOlAyd+cVnLituT97rzzGsJXH\nzRKK0bFP8feJOIxfewZT/zmP/y48zHeb9HHKgMtEbi7H7O2X8ehZOkb/cRKrjt7WW3bOjsuqL3Zz\nSM3IVv0+x/55Cu3m7EWPHw7g7Z9tFz2aX0R/QaXF8Psdl9Fprva0u2QxtPH4Mz41E8duJtj2JCZg\n6DrFZyNeLpz0lP9680Us3XtD+hwxc480uE4Q0v+cu6uZURr9x0l8YiRw505CmrGmS4z+4yTe/TVG\nmkHLEFwH3JwcAWielw7f78WTlAxsO/cAjoqLbi3kKbYXSBgWEdycNT+wowYisRLTMjHur9NIy8xG\n7JNUrVHI4etP0H/ZUczcfhkX7yfnqy2HjTjz5+RyLWtQTi7Hr4djDU7r3opPRci0HdLDOG/nVdUR\n5N4rj5Gdy/H7Mc2LaMtZzShNtA4oLatcOGVmdq7B84//6zRmbDPNMdy+JwGArzZdMEm4Z2bnGlxJ\nJzeXY/rWS7iX+Fzru1h2QDOVdVbmU7rz4kPcFr67/VefICeX43a8bseqdT4LVfuj5HSDA5zlB2/i\n479OY21MnGUnUGHiurP41cI0JfN3avsZifdg6M/HsfPiI7OssHkDH46fD8Vi+C/RSEzLRN8fD0sv\nJWvRc9FBvfuOxybghz3XTJpyXrj7Gg5cMz8lR8vvduGH3VeRnJ6FgCnb8f1/V3TKiKk+TsclYdcl\n2+Wbyy/iGEgpDBfsvoYrD7UtnuLPQW0q2RiDlh/Dp+sKx3prKfrGRd//dwXB0/4TymgKXbyfLBkF\n1HielYPcXC5Z7KIua9aHjrGrsnt3AAAgAElEQVRgxizDDDekWMGCL75jlIaKw7L3di7nOhbDZ3aW\n6YKEYRHB01WzrPWmM/dVX7gAsGDXNayNicPvR2+jzewoPJL5ND1M1vy9dN8NdJ6/X2cazxyMjWRn\nbLuERl/ukKaya362BZ9vPI8f917XKic+7DwXWBsTh6dpWVh/8q5WGeU0kThl7C4IZRExhYyT5MsD\n4X/NH6Ff/4eQaTv0tvmvmDgsjrqud78+fjtyC0kyh/KM7ByzpwqX7ruOa4+sF8X50wHTfJDqTN6K\nmp9twRU907dn7yZhyd7rGP3HSa0Xmnh9+qImZ22/jG82X0SrWXtwN/E54p6mSR21XJxb4lualJaF\npt/uwteb9S9pLvryPVUkCc7IzrGKxRzQ+G+9tvgQHiRpR37eTXyu49s1d6euoAHy7qP8PiSlZalO\njYoI4x4tV4l/T9/D0ZsJ+GF3XmRn/2VH4P/pZnDObZII+F3BEmPIP8wcf66cXK4TfHAvKR2zd1zB\nU2GAu/H0Xd0Dzbi0G49TEB1rewvZnYQ0nYA6xrT7JYOoBJ88zzStX9l75TH+OKbfemspqRnZJp0/\nKycX8Xp8G0/cfooePxzQG6kr1p6elYOzcUmqgRud5+/Hoj36++lbCWmY8PcZ1J2c51+akpGNb7Zo\n+gtbWWHFW6NvEiQzR97vQUcY2hskDIsIcr+2+FT1B48Z8GPhih5UDPywBsqX7boTmg78kGJk9/ux\n26oO0bmcSyJDzOEl0m7OXiw/cBOpGdlIy8yWpgLEaxWfL1HoKh988QWanJ6N1Mwcs6w+lx4kY+Dy\nY4iOTUD49N3SOeR3cvKGc2g0bQf2X32MlIxs1J28Df8zQ2BmZOfg2y2X0GfJIQAaS5DcqlYQua4G\nLT+mul0ULFk5uQphqPnfUNe2XHCQv5/4HC/N2IOP/zyNcQqrrCXCMDld8x1YMoVad/I29Fp8CJ3m\n7sWoP07qLSfe/0V7rum1vP4ZfQcxt55ixSFtER4+fTdCv95psB3SgEj4LHcb7vbDfrQzENHroCIw\ndl7UWMrEewMAB6/FI5cD83ddhf9nW/S+jEUS0zK1HPyNkSgMhgx9hUdumC7CJv59Bg2+2I5Oc/di\nxK/avrV5vzftX9yTlAyzLC3t5uxF7yW296mLmLkHvRcf0tomfm+cc0RdfoQ3lh7WK9iPCeJVPOZu\n4nPU/2KbxRZrYzx6lm4wP+n1xykImLJd1QJ/PDZBS9B/tOYUmny9EydvP8XgFce0fnfDVkbjdFyS\nzkCUyaypbWdHod7n29D9B+1AxuT0bJMs4pvP3MdfinYGTtmut/z5e0n4v303VPeZIyJzFYNlnXex\nfKKEcwo+IayPPmuJo4HAC6WhRP5STsvMxqNky3Ne1Zq0VeuzKP6UUxr3k9JVs8jLheGZON2UN9M2\nXcDXmy9i1O8ndTpHdxeNJVW0jubkcq0OLPTrnRi8Ik/4mOMn9tm6s9h35TGG/RKNu4nPpWl8NbEW\nHftUsvptPqMJYMjMzsVDA/d16j/n8We0pq1i9Nqb/3cENT/Ly2+Vnctx43EKxqw+aTSy/NC1J1rT\nLKZaiuSX03vxISkAQz4YkZcxxwdKdGfYfekR1sbE4edDsdK+Kw9TJGur/D7NVZkytBZn4pJw5WEK\n/j2tP39msjAAmLX9MgD1+yhapsWy6Vk5GLvmlLS/w/d78d+Fh6rPVUJqJo7HJkj3VP683knQvPzm\n7LisdczjZxn4dN1ZadpZ3qa9VzTTZeLLR/77FFfpkItGNcKn70bL6btV7wvnHPuvPlZ1O1AOOOWY\n8+oTX+ZXHqZg23ntCFPxDMrfW9g3ugKcc27RoCEzOxfHrWhNvKQQWnlTycDIVSdw5EYC6suipndf\n0m1zamY24lMycEtYAcVYYJSxqPfdlx5i3F+ndba3nRWFyHn7EPc0Dc2/3aXjWye6HimXhnv0LB19\nlhzGx39q6rwdn4ZNQt/31rKjiLr8WCu5vNgX6Jsi33jqnsHVXt5cqp2o3pIx88nb2n6zXRcckKyJ\nSozlkQQ0z156Vo5OW5RHyp+TXM6l/sNeIWFYBIm5pe4vIf6Q1Trwv09oj6LkP+Q+Sw6j6be7TDp3\nbi43q8NXvlTvPE3Dn9F38P1/V/IsJjxvikzTNt32P0nJQLTKdbu7aE8p77/6RKfzE/1MzEVshWgd\nMRTl+CQlA8+El2+6MIX28V+n0ezbXThyIx6+Ezfj2iPtl8XPh2IloSp+d0cVzuBPUjLQbs5ebDh1\nD6f1BAPcik/FnYQ09Ft2FP1lTv5T/z2vM6009Z/z2HdF/X5wzhF96yneX3UCF+8nY4/MZ0s+kDgo\n5OAyxQdKnEJRS9vw5v8dQc//afzYOs/fL22fv+uqqoUr5lYC3hBeDnLrgb48mPJ+3ZBfkhKlVTtL\nJROA+H39cUwT2bjt3AOsk7lBXHuUguG/RKs+V++tOoE+MstVTi7HtH8vaN3vhcK08C+HY7HvymNM\n23QBfxy7jd1CGbVBgpOjaJXK25b8XLg3ip/uubtJeHnePqRmZCM7JxepwvejZknddfERBvx0DDU/\n26IzTajsauRJlRkzbzpZTsytvOdAFBTKF7WaMFh34i6G/xKtsz3WyPJy07deQp8lh6V0V8bIyeVG\nrbBy5H2zOOCS+7CJMyFyhqw4jiZf75S+On1CRfz9T/nH8KB36M/RqlY/8btfGxOHB8np+EuRwzIt\nQ7PfySGvk3775+NYKQzyzt1LQmpGNlrNygugEOs8cVu3z1JehvjxhJ73mkhBp3sy1rudv5eEQcuP\nYdqmC3nuNXoEn/y3msvt30+dhGExQvI/MsFSlCss1H4rPhXnhc5Qn1N/p7l7NX6Ls/bA/7Mt+M5I\n9n7x2Sjj7ox5Cj+R3FyOT9ae0fIfuXg/WeuB+l3FR0afb5hSGObHd1LOhXvJOqNLQxa7VUdvY8BP\nGsvkjcepuBWfim3nNKPnjac0Vhj5mpjKKUp9A0hxNK4Pzjlaz4pChEpU2y+Hb+HTdWfxw+68e/3z\noVgM1DN1LP/ddJ6/H/Nl35HaT0pss6HfmzErp2ghUKZrqPf5Nh2fz0/WntGZTvrz+B0ETtmOSevP\n6gxCLt3PE+L9jUTFyn0FHz3L0ErWni0bEOTmcmw8dVfHepMf36XcXI7lB29iiEpU7Rcbz2Pg8mPI\nEESImM9ULZBKnDG4IRNBojBv+u0urXs3feslXHrwDPuvPsak9YYFhdxndfYObWuufMBw7GYCmn6T\nJ4RvPknV8vVSwjkH5xpRrOSD3/ME6mvCtKwpt/hjFYsYAPRekje1+93Wi+CcY/OZ+/CduBmpGdlY\nf1IjmExNGzLur9Oo97npeRLl06Vq12EwCIzrWujlAz5xqjQ+Rb3tKRnZ0qDVYBuFlilbIkbvOstG\n77suPZJ8/RJSMq3iI23Ke8tWqBkj5Pdb+V5JzciWfP2vPUqRBQwBUZcfYYdgtRYthdrCkNs84jy/\nOBV2AwjrIYbAm7LIe0JaJnooog6bfrsLfUOr4d3W/vD3Liltv/IwBZ+ZmKfs+x2X0dSvHI7cSEBo\njXI6DsTJ6brWnYHLj2FQixrSZ7XRfVYO13l4/xd1DbGKQBxTAy8AoM2sPQjyKYMFb4ZI23JyNRFj\nB67pWtXMSRJ+S9YucbpRbL5aMlZ9uREfyKYih/x8HENa+mLoS34o4+6ChbuuYo4J066zd1zBe21q\nab2QuqskI9eX+5FD3R9Q9O/6cZ9+n8ovNp432j59FswZ2y7hvTY1DR4rdsCrjt5GRG1vvBxYSdq3\n7qRKsIKMu4nP0e//jmh9V4BGdMmTtcutjePWnpZ8aEVaz9qDsR3rGDyXIW48UX+pyqfVxN+es4Ou\ntUnkysNnBqPRt5y5j+Gt/LW2jfjN8PKOT1MztSIq/zh2G2uO54kS+c9CmYNSOZ16L/E5PFyd8Cg5\nHZ/8fQZPUzN1nl8RVed8pvlunB2Z3rRC+pALvh/33sCgFr4YKSxteeXhM2lFkrF/nsLCN0PQzN9L\n6/gDV5+ghpe79FkZJCci/y1n5eRKYirPxxCqCteQv624S7L+QtdNB9C/KpEhHzt959t46i5a1iwP\nb09Xabu+qfbUzBydd4mpyK2uxoShsou0pozk3PDgrtGXOxA7vSv+OHYbVcuUwPi1p6WATs659P1x\nAINV3KXkbbXnFVtEyGJYhKhYytXgftHq9sgEk/sHejrWNdF3DDq/G2PB7muS0/kaI8tqyacXVx7O\nE7Nq1pDM7FydB2rmtss65UxBFJix8Wn4R+FTlZim3w/GnMTWZ+ISpfZuFqxLYuehFhnJGFSXhHKW\nTd88S8/Ggt3XpBQOapZVfVx/nKLl0H1WZflCfR3Wmbgk1RH1u7/GIDsn12DQwjOVgYCSd37VnfoT\nyc7JxcZTd/HD7qs6EbDKF4k4pWZqxxs+fbeOKASg4xf6jiwYQikKAajWYQ6vLVYPiJBHGT8WpnDF\ngYCaMFSbjpTzzZaL+HD1SY2lzsTXqtp5tDIOyW52CUWmAOXT0m3hAbT8bhc6zt2Hk7cT9YpCQNs6\nJXLjcSrqTN6KGdsuY6uZqwIpr1a+3rk8XdCjZxmSRf37/65Ibjtv/XRU1Sq/bP8NjP/rtHQf5NZ4\n0Vfv5O2nkrVWY73TfSYMCkPhf7VnViQnl6v+7tWC/fQFtIkpvhLSMvHh6lN4e6W2wLmdoPFB3JPP\ntEBiN3rtUQrqfb5NmnbebMSH0horZOnj7ZXHcUA2o5OQmol3f9VdYODTdWcxcPkxSRQCwPHYp9I7\nV3lvxY/y7bbIEmBtSBgWIQ5OaAcA6NSgoup+ZdJMQ6QaWYZo1dFbCJ62wyrrGJuLWvRZuhWXMoq6\nrO5ID2hSizxISlcdPUqrF5hwjtk7rujtyJapWDWfpWej3//pTnfq81nxnbgZ95NMDxiavvWS5DKg\n5EFyOnwnbjaYekRfX7bn8mP8diR/6TEM5Zfs939H8eHqU5i94wruKa5XY0Xjis/5Z6IFeeBssfap\n3C9YKfoyLXweNp66h4iZe5CVbdrLacBPhqfgT8uCxZS+vUqfuITUTKP9joihrsycdCy+EzfjTkKa\njmiS+76pBWQAwIJdV6VpbH18vfki/oqJUxVl4vW/+r9DkoVPX9JwQwnCTU0Toyb2f5EFe4k8TM5Q\ndc0RBwGiC829RN3+5UFyuqrLgyEOXnuC+0l5fXp2DseRG/F602TpQ6fPtqK+2nP5sdas2M8Hb+pY\nvE3hl8Pas3ViBhEtiyEsy1FZkNBUchHCydEBDSqX0vuStmYIvOh3ZO40hDUQ02/IyczO1Xq48vNg\nxT3V5LpS47cjt7Hr4iP0Ca2ms2/sn6ex8+JDky1Syu/p680XDfriqL00rJWuxpR1U0O++k/vPn1R\nvIv2XFPdbg4OjOkV0acMrKKSmZOrZdGyxXrgpmLMOm4q5TxckJCaidIlnA36y2bm41rjnj6X3BuM\ncTUfvmM7Lli+1rehiFBz/YiN+ZcqLb4Z2blmW3VyOIeDok9yYMzkvImJafqvKTXDuJjWBEDobl+w\nW/f5bP7dLgRWLaWzXZzWFaOQn6RkWGWFq/7LjmoJ/a83X8Dx2KcY18k89wtlIvhjsQlmBQCJpOox\ndjgwzcpaTg4OcHe1TBotVaS++e3IbXQLqqK1qIQtLZ/WgoRhEcPFyUFvpJ+9J83MD5qpkrwHKj8v\n4s+N+L3dT0pXTa4KaFZacXWy3NC+2sz1Oa2xNjGgmYbLDwtVXjCA4WXTTEUjAtSv05DYC5qqnbD8\n680XTfJptGdEX7iI2uWl1B9yqpYpgbuJz5GelT8R7KQyVWtt5NNt5mLNrsySdWjlkeiGLNoif0XH\n4evN2kE0jMEqeRNH/m7cn/L3o7dRv7Ku2NOHmtuB0tplTeRa6LiwCom5Yxs1PWXJWs8BeowdsfFp\naCLkIP2qR4DOflN899V4Q5Fm5/D1eJNcbAoTEoZFDFcnB70dlTlTyUWNLFlKDWtj7qjTnKWS8ou1\nVkQpDJcAUzFk/TJncG3KC7yooO+6rbXsnb3nUbOmG5Yl0a4jV+VFRdeZvNVASQ1qwXkF3R0XRDJ8\na7L5rP5coqailvfWGjg66A6cJluwVnqDyqVwQbEErTIlmT1CPoZFDI3FUP0FaO/Z1PODIUf1/GJO\n2gmCKAhsnbrDlAC1wsSa022WuBgoV2CyhILWaYWZ7sUSlGtE2xOmBmcZw6uki1XqKWhIGBYxXJ0c\nDVgMC7gxBEHYhCM3rR/MUqSwosaxljuGuahFMduS/PiDEtoYy+1pKvLctUUJEoZFDFcnB1y4n4w3\nlx7RcsLecvY+frBCIABBEIWPoWCEF4EbRlYqIQjCdpAwLGKIgQ+Hb8RjqCxtwPurTuCJnsz3BEEQ\nBEEQpkDCsIjhIouI1bdmMkEQRYOmvuUKuwkEQRBakDAsYtg6urRJjbI2rZ8wjQ711ZOYFzSerk5a\ngxHCujiRYzBBEHYG9fhFDF8vD7OPMUfsLXmrCQCgTsWSeDWkqtnnyi8tFGuUFjXGR9a1Sj3t61ew\nSj35pV5lT+wa27qwm1FgFLQgNzdZM0EQhK0hYVjE6NKwstnHNKxaGue+jFTdFxlQEd0bVZE+e3m4\nIHZ6V+z4qDXm9g3GxM718P3rjbDgzRCpTL1KngCAmb2D9J7TV7bgvD7a19MWP7HTu+L1MB+tbZO7\n1jdajz0xqKUvgPznMHOxcQLi8iUNr7stwsBQrZzx79LWvNvK3+xjSrmZn6bVkmPU2Dm2lUnllOsL\nW0qbut5WqYcg7BFPC1ciISyDhGERw9JpvZKuTvAvr2tt/HFAKMZ2zFuaSJkLcUTrmujV2AevyMQj\nE1RPdQOC4b02NY22aVH/xpjSvYHWtuTn2lPlwyL8Ua1cCaN15YcxHWrrbPt9WDOL6hIFndzyVLqE\ns8lCQcTWU4wTXrbMshlcrYzJZee/EWzROdTq6d3Ex3hBGX+/1wKz+jTS2ubmbPzZcTZRkIf5lkWd\niiX17n+emZdS6vq3XeCpEJz9m1XHgQltzbqfhmhV23RhWLm0m1XOqca3rzbET4NCbVb/i4yt+0F7\nZu8nbXFx2suF3QyTmdK9AaYq3m1FCRKGRQxzlmNTTgWP0CPWaphoEVo5tCnGR9aVRm+GluAzljps\nxZAwuDk7Yki4n9b2VxpVQTM/bYf8rGxNZZ0a2GaaT20pqZa1yltUl4uTA/aMa4OFMgvru639UauC\np6oA1YebhZakzaNfMqlcn9Bq+LC96e1ZNjAU/ZtVx4aR4dK245M6GDymShnzX2SfdakHAPD2dMWx\nSe2x46NW6BFcFTlmZgtuUqMcPFy0xZi4/u5shWCUoxTkL6n8Dga1qIE/322BHR+pT7G3r1cBFUtr\nLLIBVUrB0YEhMqCSVpkp3QPgU9bdpOtSW54rP4intIXQ8C3vjvZ24h9b3DBH/Bcn/n6vBcp5uKCE\ni6NNBzXWZEi4n8V9uD1AwrCIoRSGxwwsr1PCRfuHqW/JPFNXTGldxxsj29bCwn4h+LhjHTSpXhad\nA7VfeLUqaKwoZd2dVeuY8VpDrBrWDG3rqvvQlfVwwZp3W+DXt5tiyVuNAQDZwrqlNSvot9AoaeRT\n2uSy8nvKGLQsqCL1K5fC1O4N0LqObufsrBATfuU94ObsiDmCAAkTIk/HdNDU29TPeCRqQJVS+LB9\nbRyc2M7k69Acp3vdxya1Vy1rztraHRpUxDevNtTa5u1peDra1GXXVgwOk6Jzg3zKIHZ6Vxyf1AEV\nPN1Qp6LGbaFOBU8Mj/DDisFh2PGRadZXpcgr665ZhaBMCfXfJqBrMZzVR9ddIsinjGQ1V2N2n0ao\n4OmGS1+9jE2j1IW6eG9MWa2iX7MaRsuY47rg6eakeQ7fbq66v4yeZ9cYM18LQsua5g+oXhZEs7Iv\nIbQpxiuealHLQD8//40QvfvsjaL8fZEwLGoofmwDfjqqt6hoGRB/oNaanqxYyg2j2teGgwPDYiFY\nReT9NjXxQ78QRAZUwsVpL2PrhxFa+/uGVUe4wgrTN7Qa3n5J23IYUdsbLwdq/CnFJQCdzRAyvw5r\nhsCqpVBXEBb66N3ER+slH1HbG6MVlrRq5Upg1bBmGBzuhxoqvpMTXq6nWnevxlVxYEJbSRgCGj/K\nuX21p1jV/ChLujrho451UFVmdds3vq1J07P/fdQKm0a9BPF2ebg4YcvoCLzWWHs6VhSGhqb91fwL\nlw5oIvmXGvpKnBTrjcqtqHLa1quA1oKPnE9ZdSuWgwPDpK4N0LZeBUksGkMp8r58RWN5C6xaGmem\ndtIS+aIYUrpqVC6d1x6fsiUwu08jo0FZoqXAzdlRr4B0UBGGrep444O2tdAzuIp2WVkVWz+M0Dvo\n0sf/DdSe2s3lHH3DqktWTSXK702Op5sTvu4ZqLqvbqW876WH4hoMMaGz5vnpE6rrLvBVjwDVZ644\no8+fLj3LdmuB/zG8Of4zccBlawz18obcl6zN8Ag/o2XM+Z0bwt58KEkYFjHKlNBee9GcNUUdZL6B\nX/UI0BIZ4yPrWmVqydGBoVtQFTDGUMLFUYqintSlPjbKpiHlzOgdhM+76ffHENc6NWdpq1Juztg0\nKgK/vt1U2qb2oHdsUBFBVfOsbGoL0bevVxHlPFyEtuju1/cVMMbgU9Z4RzYswh+j29fWEgRl3HXX\n2Kzu5Y4ewVV1hEErhRWzdkVPBFYtjZJCZ1PC2RENqpTCnNcbYf8nbfHXiBYAIInOSqXUp2fm9Q3G\nVz11pzE7BVTC66HVpGvUR43y2teunAr7X//GGBLuCwB4r3VNRE/uYNL9MkTnwEpYMSQMgLYl958P\nwtGhQUXETu+KSqXdUMrNGSuHNpWmxkVrlVoHPblrfczp0wibR0egdxMfVQv7on6Npb+VFmRAv/VU\nLgw55xgXWRcfd9L2/5Tf4/qVS8FRIdzqVfJUfZl2aVgJ+8a3RUfhuvPOo/nfxdFB1Uqn1n6RzaMi\n9LqzyAX7nD6NVF001PAr74HY6V1VZxEGtPDF3vFtTapHjjkzBkqm92povJCM1e+oW17Npbywru5P\ng8O0tk/uWh/bxkTg7tPnVjmPGi1qeqF2RU8EVDHtO7Mlul1K3oZKFk4lv64y6FA/Vx5OJvgbe3no\nnzVhBiWuNua6ytgaEoZFDBcnB61OXk2oiCh/9BUFAdC9UWUMaOGLHsF5lo+RbWth/yfmTVuaQgkX\nR8RO74rhrfzRyEJHe1FQyCNpTR05lpQ5/U/qqis+XRwdUNbDBb8M1QhI+fOpJlbV+goOjpjJHfRO\n2SpRs3yO7VgHrwkBFkq/tsOftsP+T/JejuvfD8fXPQNR1t0Zk7vWl9reuLr2/V0/MhzfvBqoJWSq\nlXOXLJg9gqvgp0GhGNC8BhpULoXBQkS1SKeAinB3MTyS1df1xU7vilJuzjg7tRMiAyoiZnIHlCqh\nXVeXhpUxpbtGeDo4MJMjpQ2x+K0mksAQLaL1KnkiyEf9txdcTTN1PfWVAHzUoQ6Gq0Q/D4vwx2tN\nfFDawBR016DKWPBmCCJql1edou8jCGlA+3uKkIll8bdnzLWjX1NNXeM61YGvlzv+fq+l3rLVZdY2\n8aXvJQgQxphWtgERfS4G/+vfGNW93OGhx7ohd11xcnTQmrJXWmIjautOORsaZOwd3wbvtjY9Mt3b\n0zIB0b5eBbzRtLpZFpySVrL26EtFNjTcD/UqlUJ6do7FdX/QtpZJ5Wqb4a5jKxwUvwN9A5VBLYy7\nWIh80V3dT9fNSb8foCmyzhzDjD5CqpfRa1woLOzLfklYFeWPralfOfz2djM08y9aqy0seDMEj5Iz\nULVsCfiV90DLWl6IfZKGyHn7jB6rTAcy87UgfPL3GQDAyLY1JWub2Blx5N00F6FDkndM4zvVw7m7\nyTh1J1HalssBLzNETQU9Frow33JoWdMLXyii2eTTmQDgW94DvuU98FbzvI4xenIHnRdUTe+SqOmt\nv6NnjEmBAluEKf+fD8UCAMp5uMDVQKeZV4fh/Z5uzvhxgO2iVAe2qIFfDt9S3ScKHFM6XVcnR3wo\nBAdVLVMCdxNNs86UdXfGU2Fd41caVdGK3pcjzyU6VOY20TWoMhwdGmPEbyekl4yaLtv9cWtJkH3U\nsQ5Gta8NZ0cHfNBON4DovTY1sTjqus51bx4dgTXHb2sFhzg7OmDXx63Rfs5eaZv4nS55qwmeZ2Xj\nozWnAeSlypJPGYuE19LNPyp3Xfm/gaEYtPyY9Lm5vxf2X32ic0zs9K7Yf/UxBvx0TGt7DS8PfNq5\nPn7ce0PnGDVyctWnXaf3aog+odUQc+spXv/xsM5+MXrcnPe0UsjoI7BqKZy7m6x3v3hOeXXRkztI\nA4XMbNOmkl2cHHTKMqZxeZmx7ZLBY7/rFYQNp+6ZdJ6C4PNuDdCwqrb1t6SrE1IysvH2S/5YqefZ\nV+Kmx8rt7MjQuWFVrDtx16L2KdumheJnUbqEs07e0tjpXcE5NzgoKgzIYlgM+PP4HYP75Sbtl2qX\nNzklh73g5uyI6l7ucHRgaFuvAlydHA3mf5M/Y8oH7vWwahj2kh/m9GmE8ZH1JPEgFpO/T14Pq4bh\nEX74sENeMEppd2esf7+l1hSctUZ7bs6O+H14c5N96OSUL+lq1Si4/Z+0NSk4xZzpEkAzpWtNDN17\nsW3mjur3jm9jctnDn7bHpa/MS6PRLUhbPHq6aSxrYjPlQWJvCtZBf++SksWfMWbwGe4oRO+r/Y76\nhlXXsczqGzzUr+yJV0N0p+BqepfEic87Sp/PTO2EFYOb6pST/35a1/HGkU9Ns6ibKrTqqQhUEX1u\nJ0E+ZeDowLQs//JgB3GaXs2lRB8GXDK1UEbJm4KzrHJThWGT6mXxzavafqCcAyVMSNdUwsVRr2tJ\nQeHAGOa/EYw+TXzw9kt+On34/k/aYv8nbVHdyx03v+uCz7rUw9HP2qumctv6YQROft5R77Sws6MD\nZr4WZDTDgpI949rg2EXjD6IAACAASURBVGft0atxVZMt2eLjoAxIszdRCJAwLBaIFjAl4tRXOQ/L\nogztmepe7pjXVz0Q4/dhhn1+JndrIE3bioiPptxi6OrkiEldG+hY4hhjGBaR1xmIU3PFCVNfzmbq\nQr1TuuYiBtJwcEnoKHNFih2xucLQFN8iETdnx3wLcvFWi789sbUVPF3xXS/9SeS168j7IhpXL4u/\n32upE0RldruEL/fNptV1UkWV83DBH8Ob45tXA1HKzVn1paz0q6xU2k3rJTqmQ238OKCJ8jCTozm/\n0OOXXKdiSZ1o7wvTIhEzuQMaSD50eSeRi0BxNkXtFzO5a338NaIFfh+uneNUX7YHJcp7JE8XtWxg\nqNTPyAU1kx1iyq+4nIcLxkXWxZth1U1qkxoHJrTFtW86I3Z6VylivCBhDOgRXFUnD6lIWQ8XKSiO\nMYZ3WtWUBk2AxgovUq2cO8oK/uFq6cIcHRicHB1U3UQMfa2+Xu6oUMoNjDF0aqB+j5SHixH7vir5\nhO0NEobFmF6Nq2J6r4Z4t7XxZNPWwNfLXTWdi63oqRIdemZqJ7SoqT2t1aWh8c5NfLFaYv3rY2by\nZTm7PrbP5eZMfTn/9rZlicDzS3C1vCmc9e+3xMzeQahVQduC5GDGVLKSxtXLWLTaiiWIrgKi76co\nasxJJ6SkSY2y+ToeyBOq3/VqiKUDdd0BWtT0Qn8DqXTE88un0eUDjjEd6ujkdwQ0fp91KpbEuvf1\n+05O7lofLWp6obl/OUTULo+BLWporbYkivWqZUqgUbUycHdx0nL3kP++5b6RvcUBh8pvZki4n+Du\noe0fqWbxWaMSkKK08srFa4cGFTG7TyOM61QHITJfbPn9+nFAE7yj+E02qlZG8g12dGA48XlHNKlR\nVsdPlTH9lillH+Tk6CANjrhZk+r6GSrLV/vjgCaIGtdGb1mTB6VKhKZ+LEs3Jhfto1XcLqRyZj4r\n8nup71Dl/Z7zeiPsHNvaaj6ptoSEYRFFLdeekurl3PFG0+o2nzouXcIZnRpURNT4tqrRtAVJKTfd\nkd//+jfBze+6GDxOstqY2Q8GVzOc004fDSqXgqebk0EfwMLE1I7SlJyMtiBQ8O1pWbM8qpVzl6Kk\n5YgWK1cLLHrr3g/Hp12suxzjhpHhmKRSp195D0SNayPluRSjfg36LynQlxfUVOQBbdZiVLvaqFza\nzeyVUNxdnLDjo9ZoXF3/Gu/DIvzBGMPqd1rg17ebYVqPQCktj7OjA2b2DsK4TnVwYEJb1WwI8l+3\n3AdWfJRFC5I8Qln+TLSvVwHOjpopTzVrk1qgXStFwI0yEtXb0xUftKutV3TU9C6JzxS/n40jw/Fp\nF/V0WXKcHBxUB3sd6le0Wh80rpP6O6lJjbIYK+yb0r0BIgMqGbSa5XdmVS4s5dP8Dg4Muz5urSWE\nuXRMXrk5fRph1bBmqm4yar60+vp/+dZjn7WHm7OjwRyN9oT9S1dCFVMiOAsq8/rpKZ0K5Dxq/DG8\nObJycjFw+TGD5YyJN3G5wF6NDeeoExFT+5haXsnm0S8ZXR2mMMlP37xzrO2toCHVy+L0F51Q2kBO\nv+rl3DGuUx2t6PvCJLhaGb1L4MlflF4lXbF2RAuT070A2tHH9kJg1dI4rPArtKU3Va0KJfFB21ro\nG1YN5Uu6qgbmSO0Q+oNGPqW1coWK21e/0xx7Lj/CG02rY+K6szrHK1PKhPmWxfHYp9JnN2dHfPtq\nQ3y2Pu/YQS190czfC53n7wdgmouDmvWsfElXPEnJQHNh2lsUMPru7Tut/DG8lR/+jonT2RdkJK1P\n58DK2H7+oda2t5pXx29HbuuUHRbhj9k7ruhsf62xD0q6Opk8+Mjvb0R+y5TT/EoRnJfrV1OuW1Bl\nyc3oyI14nbqXDQzD42cZWtsaVNasbqQvWf2rIVV1Ag7/GN7crhNgkzAsopiy7uuLgHLa2FIqlHLD\nze+6mGz9q+DphmvfdLZ4uo4xBhsvh2wR+z9pi6M3E8zys5PTpWGlAhsVGxKFgOYeGxIH9kyob8Fb\nYj3dnPAsPRv+5UviTsJzk6LS7QnGGMZFmrcGuD5p5lveA0PKa6Y/VwwJ0woCUePD9nXwlrDYwEQh\nYXe/ZtWx4eRdHItNkNonF/tqsxtK1Lqj6MnagRJiH6RcOODYpPZgYHpXKGrqVw4jjaSx6RlSFWPW\nnNLapqZ/lNbttnW9UbVsCXzVQz0ZuiEsDsYQDpOLaXP659NTOsHdRfc33y2oMjaduQ9A43agHIS5\nODng+rddkJiWieBp/+U1x8CprfXeshUkDIso3RtVwdg/Txd2M4oV5nZIloone6ZaOXfV1U5M4de3\nm2qt8kIULfaNb4uUjGyUKuGM6NgEi5MJFwWkYDMTrPbmTNO3rOmFESb6dA+P8Mes7ZcNljHF385R\nmCKtokhrVUGZy1FRV92KnmYJp7Z1vbHn8mPViO2lA5totXXFEN0odVOZ1du0gCsdZCt9iSmb9PXp\naqmD9OUprV3BE8B9o6dXulE189eIvzfCdN1c7J3i92Z7QShqKWeI4k9EbW+T3Be6BlU2KzktUTCI\n0Z6lSzhr5Tq0Beakg7EFBTWNZyh4Qy2SW4mpgRg1vUtqBdGooazJ3Gj94UImBuXg7/LXL6Ny6RKS\nn56p91aeTFtcPaapbznUtiBdFwBkCbnGXBwdMOHleganrlcKwlWZdF+OWqYKc6hapgRip3eVBGJR\ngiyGRLFg/fstkZph+coAhOUMbumLCqVMT/AtXz6OeLGwF78q0S/PWlG3+pjUtQHG/nlKWp1ISQt/\nL4NuQfkMLNdCee/NFYYta5VH7PSuOBOXqLVddDkQrXMtTZwm3fphBGpN2gpAZrnNx/VKSz2aILi9\nSrpiSvcGUs5PVezlx1oIkDAkigUhBiIYCdsy9RX15aYIQokYDFDYSX0tzUJgLsHVymD3x2307v/D\nyDrLtrhPjGmuW8/iMDq4ODoguLrx/KOODgzbx7SCT9kSRssCtnPFMUUYApr0Q4YY3NIXZ+MSMbCF\nL47eSMCjZ+km1dvBxtb2goCEIUEQBFEgvNO6Jp6mZWFIuG9hNwWA9YShtfVbl4aVsOXsA6vWKVpJ\nHRhDDucmWwyvfNNZ777vZOl8APXlEk1BXIqwuoX+zXJcrCQ4y3m4SL6SxgS8yI1vuxQLQyMJQ4Ig\niGKAoZQZ9kJJVyd81dP8SFVbId6tTaNewtGbCYXaFjnz+obgqx5Zxguagbh2tYujA57n5ujkUTSX\nwKql8GZTy1dYATRrgCc9z0Jg1dJY8lYTtKpT3vhBRijMoEBlYvGiCglDgiCIYsCJzzvavTC0F5RW\nncCqpaWk6faAi5OD1kot1qBncFVcefAMPmVLYOq/F5BrB78Vf1lewZcD87f8XsVSrniYnGG8IGEU\nEoYEQRDFAH3pNghdvAXR1aZuwS3hKbLgzRDceJxS4Od1cXLA5G4NsPHUXQDq+QhNQUyD06ZO/lbb\nsTb/fvASbj5JLexmFAtIGBIEQRAvFBVKueHoZ+1NWkHKFKoIq6cok0yr8UqjKlY5p6V4uGhe+4ZS\ntRiiUmnr3jtrUaGUm84KI4RlkDAkCIIgXjgqWlFE+JX3wKGJ7VCpCAiT9vUrYEr3Bqrri5uKNe8d\nYX+QMCyGnPy8Y7GIjCIIgigqVCljWpqWwoYxZjRVC/FiQ8KwGFLWw8V4IYIgiCLGplEvUf9GEDaG\nhGExo0oxXt+UIIgXG3uKHCaI4gotuFvM2PjBS4XdBIIgCIIgiigkDIswfVWch7097StSjCAIgiCI\nogMJwyLMtJ60Ri1BEARBENaDhGERxoFCjwmCIAiCsCIkDIswJAwJgiAIgrAmJAyLMMVkvW6CIAiC\nIOyEQhWGjLGXGWOXGWPXGGMTVfa7MsbWCPuPMsZ8Zfs+FbZfZoxFGquTMeYn1HFNqNPF2DnsHcYY\nTk/pVNjNIAiCIAiimFBowpAx5ghgEYDOABoAeJMx1kBR7G0ATznntQDMBTBDOLYBgDcABAB4GcD/\nGGOORuqcAWCuUNdToW695ygquDmT0ZcgCIIgCOtQmKqiKYBrnPMbnPNMAKsB9FCU6QFgpfD3WgDt\nGWNM2L6ac57BOb8J4JpQn2qdwjHthDog1NnTyDmKBORnSBAEQRCEtSjMlU+qArgj+xwHoJm+Mpzz\nbMZYEgAvYfsRxbFVhb/V6vQCkMg5z1Ypr+8cTyy+Misw4r8RuJ503XhBDnjUei597Lh2ng1bRRAE\nQRCENXF1dMWmVzcVdjMkaEk8M2CMvQPgHQCoXr26Tc/V0LshvN29jZbjHFh7L08LN6+vm/SaIAiC\nIAj7xNnBubCboEVhCsO7AOQqxkfYplYmjjHmBKA0gHgjx6ptjwdQhjHmJFgN5eX1nUMHzvlSAEsB\nIDQ0lJt8pRYwMnikSeU45/ht0xbp81fhXW3VJIIgCIIgijmF6WN4HEBtIVrYBZpgkn8UZf4BMEj4\nuzeA3ZxzLmx/Q4go9gNQG8AxfXUKx+wR6oBQ50Yj5ygSFCF3SIIgCIIg7JxCsxgK/nwfANgOwBHA\ncs75ecbYNADRnPN/APwE4FfG2DUACdAIPQjl/gRwAUA2gJGc8xwAUKtTOOUEAKsZY18DOCnUDX3n\nIAiCIAiCeNFgRcg4ZleEhoby6Ojowm4GAMB34mbp79jpNJVMEARBEIQ2jLEYznmosXKUBI8gCIIg\nCIIAQMKwWDC6Xa3CbgJBEARBEMUAEobFgI861insJhAEQRAEUQwgYVgMoMhkgiAIgiCsAQlDgiAI\ngiAIAgAJw2JFTW+Pwm4CQRAEQRBFGFoSr5iw/v2WqOFFwpAgCIIgCMshYVhMCKletrCbQBAEQRBE\nEYemkgmCIAiCIAgAJAwJgiAIgiAIARKGBPH/7d13uBTV+Qfw73sLHS5cQECKgmKvSOwaVEQkiT2W\nWNDYe4wm0Z8aUWNi1JhEMRpEFLvGbtQAYhdRQFHEBiJSpCNFpNxyfn+cmTuzs2dmZ3Znd3b3fj/P\ns8/Ozs7OnJ2dnXnnVCIiIgLAwJCIiIiILAwMiYiIiAgAA0MiIiIisjAwJCIiIiIADAyJiIiIyMLA\nkIiIiIgAMDAkIiIiIgsDQyIiIiICwMCQiIiIiCwMDImIiIgIAANDIiIiIrIwMCQiIiIiAAwMiYiI\niMjCwJCIiIiIADAwJCIiIiILA0MiIiIiAsDAkIiIiIgsDAyJiIiICAADQyIiIiKyMDAkIiIiIgAM\nDImIiIjIwsCQiIiIiAAwMCQiImpeXr0eePSEpFNBRaoq6QQQEVGRWD4LqO0HVFQmnRLKp3duTzoF\nVMSYY0hERMCKr4GRA4HX/pR0SogoQQwMiYgIWLtYP897L9l0EBWL5bOSTkEiGBgSEREg1uVANSab\nDqJi8NkLOgf98xeTTknBMTAkIiJXYKiSTQdRMVjyqfU8M9l0JICBIRERASL6mTmGlIQVXwMb1yad\nCgIDQyIiApwcQzDHkBJw5wDg/mHRPlO3HvhxZX7S04wxMCQiIgDMMSyYh44BPn069/XcezAw/trc\n11MsFn/iTI+/RtfzCzJmKHBL3/ykpRlXqWBgSEREzbcoefEMYEQNMOfNwm3z64nAU78OXmbaWOCr\n8cHLLJwGTLojvnQVk0l3Ak+eGrzMoukFSIgUYBvFhR1cExFR8218Mvcd/fzFS0C/n+Z/ew31mZcZ\n+wvgm7f09IjV+U0PkQdzDImIyJVj2MwCw0LXrazfkHkZOygsJoU8LhrqCrctSsPAkIiImnHjkwIX\noTds0s8V1YXZXpA1i4Cln2debuKNwPUdCxew1f0Y/zrnTwEePi5cji2A5vc/cDAwJCIiNMvGJz+u\nBJZ/paeVAsZdDUwdE9/6pz0AjB6cOs/OMaxqGd92snX7dsC/9s683JR79bNfdzIbfwCeOEXX14xD\nY0M863F75mxg9gRg1bfRPiesY0hERM1ZqRQlf/060KoD0HOP7NcxapArUFDAeyP15MAMDUPcGhuA\nikrzey9emj7PDgwriyDHMKyq1gBW6+5hTMYcpjuE/vxF4KoFQMv2BU1ek40/AKoBaFVjeLOAx/WP\nK3XuavtuhdtmjJhjSERELiUSGD50lO6uJRfu3KNsAuKZzwI31ALLZ4dbXiln/N3KFtG3V0jLZzs5\ngNWt9fPfdwB+WJq+rD1KCAA8dlLu2862C5rbtgFu7uO8/n4uMOU+3er8+7nR1pXLDdItfYG/bZP9\n5xPGwJCICmPdcn2CtluBUpGxLoR164H13yeblCSsWRj9M589r5/Ddpsy9T7g0eP1dGXIouQNa4C1\ni6OnLVcj9wDu2V/3k+gu9v7iv8Gfm/u2zil96tfApnX5TaNXnbW9ug3Af04H/rkr8NJvc1ypqyh5\n3QpgzXd6uqEe+OLl0slhj4CBIREVxvwP9POkO5NNB5nZF7hV3wJ/3TLRpORk/DXAS1dE/9ysDH0G\nGkWsf/bte8502KLkkQOBv20bbTu5cjfQmHRHcPBjGnlk2gO6A+9b+wOrF2Tennv9Cz8MnUxfs8br\n3FyTXOoM3toPuH17Pf3u34HHTwK+fDn79RUpBoZEVGDNrzJ3afBc/CfekEwycjXpTqexRNFx7WNT\n45M3bgZu6JI674cl+U2SySdPpL5WAY1Bgop969YBz5zrvH7kl+mNcYDUwPDeg8KlMUhgMX3A+adu\ng1PUn8n3VjWEdctCJ6tUMDCk5mHd8nhbG1IWyq/IJZIln+kLTyEoBXz0SLSiPG+u0Nt/izdNXutW\nAJt8uiWZ8RRwUw+gfmN+0xBW/UZgycz0+bn0/WgKXt74C9Do0yXM+lXAcxcCK7+Jr/Wvn3pPIxNT\nNzWT707NAfXjDipnjQcWTDEtFCl5GWXTsKehDnjqDJ1Du/GH8J9btzz6toocA0NqHp4+E/jvZcCy\nL5NOSXFa813m4bfikmv3D/M/KK16inUbgBs6A3fvAzx/YWG2+e27wPMXAK/8IcKHChy439rPP3do\n/DW6L7tiuei+fAVw976637+4VHg6BXn3n8HLv3M7MP1h4I7ddN2/vPL8R02B4f+uBO4fmnlV897z\nb81si7uLpKAcQ/v8s3YJsNpVr3TUQU6xsN3XpJ/6Tc56XrvRmf/Jf4DvCjFMX34xMKTmwb7AFEsO\nRLEZPRh49Je5rUMpoDHgBB9XJe37DgUe+Fk86yqEH5YAjVadrXkhcljiYPc3Z2pB6ieJSvTLvjCn\nY21AANbYALx5a/7SZDLvff28YZUzz1S3LhP3PnbfIDU2ABP+GP6zQRZ/qht5rZrnzJs1Qc/LtlGR\nXy5mWKZue9z8+kfMVpii5L9to1tZ25ZEyIW9Z7/U11Pu03UjnzkLGBVyWMViuekxYGBIpW/9qnC9\n95M/d4vMuvXh69m4vXM7cEOngJO8fWFrZnUMU+qSidXHWdjRF7Kwcg7w2InW5qKc4oukqH/BVNcL\nQ5q+eAl4/U+G+S8DN2+RnzR5x5H+brquW/fp0/GsP1LObgbTHtDPX77izHvnH/r5s+ezuwHIdcST\nTEXf2XZPky9SgcD/w/KvUvfjS7+NVjdy9qvArVvpgL0IMTCk0nf/sHC99wPNrxf7lXOiFW0oBTx1\npq5nEzV3deoD+vnHFf7rBprfb+AOzhrr9EUw5y40DDatAz5+PLXRyLLPgVmvhvt8kcSFmYvxfI7L\n8dek5ujFqSkwtHLE0wKdDDuvKYjwLNdQBzx+csjGMjn8QHYH3C9eCky7P/rnc92vQeeSqIHqph/1\neW3eZP91BDWWCbMfXx3hpFmg+3SMM4dvwTT9PP/9+NYZI458QsXvh2XA4k+ArQ8xv7/UUCm8kDau\nBea+C2wbor5Nod2xu34esTrc8o0NwDdv6umGTVkO2xUh8PvmbaDnAKBF2yy2UyLc9afsoOfTp4Ej\n7oh3O6/8HvjoYaD95s687+cCjxwb8vfPMvDYsFoHZUNu0iOR5CrjjUMMEezaJdFuUOxzjF9duEzB\nrFLm7S39PHO/gEHr9Fr2lSuNru25G2Ok5MgWiF9guOY7/xtJk5VznHOa25JPU4P1oDqLYQLRafcD\nLV2jp4zcA2gR42guuTRaKgDmGFJ2Nv1YuIP6wSOAh4/JPH7muhXAhOvSl3v9L6k988ft+QuBx04A\nVnydv20UimpMLzaLvpJw87//Fhj7c+CFi7PcTgZrFkVrXZgv7ouUN+cpF+tXpV5w7U6Qs62vZfq9\n14W4aE8aCXz4IDBldHbbbajXuZzrDblSkY7BkMsu/FDXL7utf4R1Z9iGu1HRxrXAS5d7jj3rc2k5\nWzkcB/WeFu5LZgJ3/UR3ou0125VrnOt5e92K6PUr/QLn27eP1pBm2Vfm+ffsDzx3vvM66FoRdp97\ncx03xVgPcvZEeyPxrTNGDAwpug1rgD/3AF6/qTDbs1sSZ/pDv3wF8O4/0uttvHmzM+13Umxs0K1y\nszlp2sNhZWp5F1XUIZzioBrQlNOQ00VrI3B9J1206WXfLW9co5+XGhogxOH27XIfMi0OKceU9d3r\nfLppieKvWwAPH+tatR10ZriB8mU49t8L0Rm5/V0mXm++IN/UA7jnAP/P39hZd43z6ghrRoacPL//\naNj/btR+8txd6jTUAe/8PT0oc5t8tw6SJ//LkDbvsZBDYDDyJ6mvV81Pfe2bI2rY5tx3UlvoBnn9\nT9HrBDZ4cgxH1ABv3x5tHUD2QV02NuXxpnK+VQwed2vsmCQSGIpIrYhMEJFZ1nMnn+WGW8vMEpHh\nrvl7iMgMEZktIneI6H+A33pFu8Na/hMRGeBaV4OITLceL+T7u5cFO+v/rVt1kBinxgbd36C7srOE\nDFTsk3U2J4X3RupWuZ+/GP2zdroiVfTPYOazejin2SHrh8VFNYbf3wAw+R7d51wK0fVxVKPrYo/U\nC/fsiU6wEOd+81peBN0TpezHLAKBGU/59xc3921nOmxupFLAnDfSW5CbglVvP4PfvJ2ei+jOtTT1\n9Vf3o64KkomxuFHpOrIv/04HEz8k0Jnwe3c506MP0cf0W7dl/lxKLplfMJtDYLB6fuZljNs0pOWB\nnwH/2if7tGRi+m3D7MM0If8/gb0jFFEwxqLkFFcCmKiU6g9govU6hYjUArgOwF4A9gRwnSuAvBvA\n2QD6Ww+7cpffeg93LXuO9XnbeqXUbtbjiPi+YjlzHcxPnRHvqj96WPc3OMld/yqGHKymVfncRdu5\nc9mMMpCPwPDbSfrZzo0slMaGaHWv/vcH3UckgMwnbVer5Ocvcl67Nzf9UXMfheOuDt+Ioti4j1v3\nhSDMRUEpvX/D9BcXtgrAly8DDx4JvH9P6vynfp2+rDtYbGzQRf8PHZW6jN2wAYinYZF7HUrp7j8+\nGKVfr5yD0FUV4mJqAb0hoM6mvT/cN7f2mMr2sy1SSXmW3+9/V3lXZF5u42rdCfsTp+beCtmrYZOu\n1+lWF7Lz9YZ6ZzjN0DmGBeg2Kw5F2n1aUoHhkQDGWtNjARxlWOYwABOUUiuVUt8DmABgqIj0ANBB\nKTVZKaUAPOj6vN96jwTwoNImA+horYdyteSzeNe3zup3zdTfVhyBYaZiqI1rg0/6xs+GCAzvG6L7\nugrLrp/Usl20tGTNuhjPfDbeenB+UtbtCgSeO9/cR+F7I3UjinyZ/qhTb2re+8Drf85+Xd+8BTxy\nvJNr4ZdjOOlOq7+5+cCYw831tqJcoDMVJW9YrbdnBwor56S+b2oEYPe/2NgA3FCrp725gtmMMhEk\n5T9q+L/mWpQchV9duqBSCbvj6kZXl0RNN0/e9RQg98pdpA0E76fnLwQ+fyFcDm8UqlHX68zGazfq\nvku/mx7+N861VXIKn5udsC2KP3se+PeB5rS/f3f6vCKQVGDYTSll92C6GEA3wzI9AbjzyhdY83pa\n0975Qev1WxcAtBKRqSIyWURMASp5pXTSGvMh9Jrr7nzhh3pb9p/8pcuBh47x/+ya79LTF5Z9gp54\nPXBzn+w+G3QHPP99/y5Kvpuuh7lys9dV3SZaWrJl59K8eAmaToSZGvuEsXYR8MmTejqluxrXbxT3\nRQgAFkVY5/LZOiB9+iz9eswQ4M2/Zr/tx08BZo3TOTDTH/Nv+DThWv389m3AvEmGInkg0kUsUxUA\ne9SOVd+mzv/4CeDr18yf+eYt/ezO2fDmCrpH8FiRQw73uqV6yDf3/yjtv6xQsAr737zlX5cuqBWy\nvT/CBH2JFGsG7L+qVvo5qA5lodn/n3XLEL4oOYbGJza/XPDlPg1hvJ4+G1j0cXHt0wzy1l2NiLwK\noLvhravdL5RSSkRi/6dHWO8WSqmFItIPwGsiMkMpZWxeKiLnQBdFo0+fiMFDucpXn3Rz3tA5KkP/\n6vyRP34s+DOLchiKKJcTtP3ZUYOAXz0JbHNYtM/bPeW7uxSxi5zsi8z6VcCnTwEDz8x9n791m87l\n2c81GoFUpO+DKPtk1XynzpM38HvmbKDnHq7B5iX/F8R/BzR2APQ4wm1qgW0Pd07Y3tE27C5GRtQA\nP70SOMhbJOejwlWk+9x5ERJtyh1z7acNq/Xx4Ne1T1BR8pKZhmDGWu7Zc/yTZHd87pfD694uAPzn\ndGDHo/3XF8SuTzv9YVcSvcdk0Cnd8N43bwEznwP2OB3osUv4tCyYFr2XgR9X6mPKlGMIAP813BjG\n9T8YUZPavUqQ1Qv837P/C8VUxGm3slcq/P7y3vy4NdYH10FMWz6HG+QF05yGN/UbgerW2a+rgPIW\nGCqlBvu9JyJLRKSHUmqRVaRrGrdpIYBBrte9ALxhze/lmW83p/Jb70IAvU2fUUrZz3NE5A0AuwMw\nnhGUUqMAjAKAgQMHFlFFhQLzG9YpTivn6mdTH4XffQRstiNQFTTsUVQ+P+e65frPHNTPnvtk9e27\n0QPDIHZ9pf9eBsx8Bui2kw6wnjgFuGga0GXr6Ou0x/b0BoZN7K41IpwQ/7FT6mvvBfzOAc60SPL1\nfJ6/wJk+713z+acKQAAAIABJREFUMu4Lwps3hw8M7X1puqCYvnfQvnAfWzf3Adp0Bn4/x7xs029o\nWN/d+wK9PZ3AL/3CGREjyGMnAf0Gubbj+s8v+0q3KPaadCew7TCg81bOvK9fB/rsrf9PUS7MbhNv\n0LmrXvOn6PFrvcb+Qj9PvS98X54AsGpuhmHVDG7pq7dh/2e9ozGZupGJ8wZpo+f7+Z2bg4ZltNNT\nTLlb7iLbsOeNcf/n/949+wPb/Tz89nPpfWO0q1eETH1dFpGkipJfAGC3Mh4O4HnDMuMADBGRTlaj\nkyEAxllFxWtEZG+rNfJprs/7rfcFAKdZrZP3BrDaCh47iUhLABCRLgD2AxBzpblyFKIoecPqHCsw\nB5wARg0Cxl/t//4TJ+v+DOPY3K1bAXfv5/Om/VnXhyuz6RA6gFgXmR+tXvfr1ju5id99GOeGnEn7\n4pDpouXXx92de2TeVpQLYrZBhJdSqePH2h7yyeFqrAu+QCoFvPHX9Jwl+zcL/R19+rgzzbPrAZr2\nycxngzczf3Lq62/fAV4N8T/58mXdeXYT17HyiaE7oroNusPrew8G3v2nM/+ho4CbrEKkbBsRmILC\n5bOA+wYDPyxOnW8KWMMOQbZqfvTA0GbnGLpbjJt4c9YzsRvg5JP9nYup5a6bPdxfrrLtVDwXt/XX\nObvFlBvrI6nA8GYAh4rILACDrdcQkYEiMhoAlFIrAdwIYIr1uMGaBwAXABgNYDZ07t4rQesF8DKA\nOdby91qfB4DtAUwVkY8BvA7gZqUUA8NMguoYNjYAL/9e5248/qvg9eTS79/CDEHRuz45Ie/f43MB\nDjgRfv+N/3vez1a10HW5vhof/Jmwmu76XRfjfDQOcecu2A1/MhWh3NrPPL9hU3oOYhrDb/Dfy3wW\n9UmHtxHEyjnA3funB6x24PfeSOAfO6c3mLIbPCml6wPZGlyBoekGaNHHwBt/Tk93YCOQHHIMU+bH\nUP8zW/axMnqwOfiyv+OGVcCEP5rXETb9YY7xkQPN891DA9oeOS7cdl+9zqkSEFXYetfrlkfLOS9I\njlPATUrilDMqU5xMfa3m09LiDzESGRJPKbUCQNr4ZkqpqQDOcr0eA2CMz3JpV56A9SoAFxrmTwKw\nc8TkkzfHsKFeP1dU6GKKD/6t35sVEBzN/0C3NDviTmDAabmlIYrpjwB7nQv02NWzuhjqGAI6x3DM\nYbqOi110lUuOl6lj3E+eSN9urkwXsyUzgdo8DG4vPjmGU9P+6pq7rtYLlzjTHz0MDP2LvsDevj1Q\nu5UeG/hzTwHE16/p4LDCaj3rW/9IATOeTN2uHRhWGE6Vj53oTP/7QF1UO+wWc3clrk2kzzP8xiu+\n1ts31UlavQBo08XnOxSCFRgumGJ++8VLzfPdQtcVM+TwFkq2wVHYvkdXfp3fTpSjDE1p8xvTuZw9\ney7wxl8Kt72gBpRFgiOfUG6kQo9ccJ9VpdQ7rFVDffpnAB0UAsCnz5jft09QHz5ofn/DGuDNW3SL\nXj9rF5vne9O06BNzkVhYKTmGLZ3Aw85xi5K7U78pNaAIuoDG0Wq4ieEi8sTJ/t11BO33MKJcd9zf\n88OxrjesNM97T+emLPPU6XJLaXXrc8H0tgxvqHOKfUyBod1YZeGHOvfwg3/rjrvtouSwOTxrrMYA\nSgHPnKvHUb5zAHDXnjqH02vWhNTWv8bWzAFy7UlAxOlXzsS+cQkS9tgNm8OXD5mKgk0WTE3vqzCI\nMcc1Jl+Niz48on2+mfNG7MkpaoUcZWp9xOEEE8DAkKIzFSUvnAa88gcdTLi5c2cmjUwvYs22eGTF\nLF0p2G7RCzg5QjbvkFFNaXZNK5W5BWsm7uDN3dmvHeBFydn7U1fPIPHWvvaOQBF1vZk0+tQHffef\nuuj16bOdgLqhLnW/R/Xp0+kV5W8K6FbU27rTFnfDJ28A31jnCtI921rmGlHFPYbqw8c4RZDG+omG\niNi+CI+7St+gmDqaTklXvc7lsD19pq5nF1bOx404N3bZCmoAkS8jQrbatWVTn210WoFVML9c1zjM\nGgf8L23siGCx1lumjNp2TToFRgwMKbPGBt3tg6mYwZ374B1Jwf4soPtKG3+1HnbOzbcibjZFGZ7P\nbAwxXJ9f/23fTQem3u+8DurI232hnf4onH4AraAibO5InRVIuIe6ste9cKo9w3lv9XydU+W2boV5\n2LANa4JzefwC9A2rdP9+M550cuTyUTHdOxzbqnnAXXvp/Rk1Z3SeX8ezdifmPseFNwBtqHPlGnnq\nXt21p//27f+EaX+HOSYzadiU3jdi3ON0Bwk7YkWQp8/KvAzlLokhBCm8VjX6pi5Kv6sFwMCQ0tVt\nSC3OnPms7vbho4fSl81ULDX5LmDeZP++0ryDqzelwZBDlompTlDacFDQo4rYwYZfYDrqp8B/f+O8\nvsfTMvm76U4OmjtQWjjN1V2J/b4rsBlRAyz+VHfW7Q14TKMjrF6YWpzobkH71q26bpvbrf2A27bW\n637iVGf+E6foXJ5Nrot6mFbjqtHpFsjOtSxExfR/7Aws+0J3PO1XFO+XY+hXLcD+nd642fy+V0Md\n8PIV1mcVcOvWwL/2Dv4M4BQl+3VonitjVxwlVCdsRI2+4aD8mzUu6RTEx32jXi6UAkYdpEutCnlz\nlwEDQ0p3Uzfd4tBmt1K165WlBAYZivOmPaAbY/ip36SDmGVfRi/q8TLlZHmHgwKAB49wXVzDDrHU\n6NSfXPalDhztrj68dUbsQKahHvhhKfDFy6nvPzAMmDJa98foZupC4ePHwlXA/3ZS6oll5rN6aCub\n3ZrPnTMYpv6WUk4XFmOG6A5bC9mVRUWVf1FyVPZNgLtvvSDuImIo3X/ksi8yf25FhGJdkw49My/j\n9XAehwskKgZfvZJ5mVKz8mvnPCOVwcsWUCKtkqkELHIFgXbF3KaAIMYOrhs26j7SpozObT0AIuWa\nzPgP0LaL7og3rOcuAI4cCYy3hjJ7b2Rwx9cfjALeusX//TA5b2FGc5nxlM5t3PUkZ55f/3tjj3Cm\nw1QwdweGQGqHrYVQ1SogMIx47NkV8dt0Dre8u+uaQnb4a482EsU6FhkSlaz+h8U8YENuGBhSsOmP\n6AAIQFPgNf1R5/1cx7n9fm58nZZG8eOK1HGZw/jyJeCWl1LnBY2p6xsU2gFNTMV/dhF0St+OPkFT\n1N8r6Y5uq1r61zGMelNiB9lh6yyujDgkGhFRNopsqDwGhhRssauS+7QH8hPExVVUWCry0UE1ACx3\ntZQN2yXJiBpg2G3+7ycdGEol8Nlz5ve+eBno3B9o3THaOnP9TquzyNEjKjZtOjsj6VCyWrRLOgUp\nGBhSsJbFdcCWBbsOYj6Drih91fmNTgEUQWAowKsjzO+tmAW8cFH0ujlhh0Xz8/cdcvs8UTE46Gr9\n/x5/LVAfseHDZjuax7Gn7NRumXQKUrDxCQXLRxZ3ULcpzcEGqw8/pfLXwjdKzm5QC/AZT2bX0W9c\nfliSeZmow8N5+1Akao6qWwN7nh09xx0AemUaD50iadE+6RSkYGBYiua+q4fMKoRcR0kwybVz3HKh\nGsONEpGNeZPys16icvSz25NOQQKsOrrZnOM7bRlrSpq9yurMyxQQi5JL0QPD9POIQuR8xDy6BDnG\n/jzpFBARENy7QLnoun3qsJF211XZdJOy28m6SkYSI9iUo8riaZEMMMeQMol72DEiomIT67jjRWqL\nfVNf2903ZXuOP8rQRyxlh4EhlYzZE5tfi2Eian6i1lP103uveNaTD1WtUl83BYZZhgG1/YCrYmqh\nX9nS/7123ePZRjGrLK7C21BHhIhsJSItrelBInKJiGRRY5WK3jpX9wVTxwATb0guLUTN0b6XJJ2C\n6IK6PCpGu5yQ+jqu1vdBQdbxD8azjWxVVKamwR63+/tvsl+nO6crbMfxJt6g1e2ikI0Vt/9F9ttP\n2prvkk5BirC3Ck8DaBCRrQGMAtAbwKPBH6G8+2EZMOfNeNd5az9n2jREGxHlV776ljvomvysF8g8\nzGDnrYHWncKvr23X3NKTybbDUl933jqmFQcUy7YsrpanTYFhLuJqNHGQafxvS6uQQ6UW0ZBykdUV\ncGSlEMIGho1KqXoARwO4Uyn1OwA98pcsCuWBYXrc33x1eULF7ZCA/gepdFXk6QLXbrPon7n4Q2Cz\nEP02VrcJfl8pRGrIZjq29zov/Ocz8RYdb7k/MOiq7Ne366/0c1B9vVwDl+676OfTXw5eLqwtDwy/\n7FULzPPjqoPeqgbY6/zc1lERY3HsZjuGX3b/y4DNd89te/tckNvnYxY2MKwTkZMADAdgZyMVV/vq\n5mj5V/o5jorTc98B3gwY15eKz2qfkzXlSIAzJwDbJdRq3O8Ct+/Fua1391OiLd97L50TOOC0zMtm\n7O9U5R5EHB4w/GRUppvpLtv4L7/fb4ADrvB/v6mOWMB3jBq4nPBw6uszxwO//wbYcj9nXquAGl2m\n7dnfu+9PgV2t4vQwXc94czsrIl7+MwX1IsCuJ+rprttHW7ctzi5fgo7nY+9LfT14BHDCI87riz9E\nZEXWKj5sYHgGgH0A3KSU+kZE+gJ4KH/JokiybSCyZpEz/cDPgNdviic9VBh2dxOF4L1IZZJUUGVy\n9Cj/984xVMU44LdA7z2B7X4WXxqiNErwq6d2cI45xFFzIpuCpxABXevaEOuKEhh6ls2l/pqJsU5h\nQMnLvhfrcbszCQp+c83Rqm4NtPHs514/8V++/xDDTOs7uotn+2bIORxuqFLU1uf3+PU48/z9fhO8\njdp+wOa76S7YavsFL+unW4RcvkyC6oqa/kfu5bNNfxEJFRgqpT5TSl2ilHpMRDoBaK+UivH2jXLS\nWBf9MzOeAm7fTneWTeFd9lnSKXDsk2MOUhQdt4i2fNJD6e10rH7uvrMO8vzE1Ro1k+Evhl/WrwQg\nH53NB7KCiDA5fR17p+ekeNeVaT1DfG5Mj70P+M2MzGmIIqvjMyD9Oxyln7c93H+ZvFQRsH6jg65O\nnd2hV3C9Pfdvkem46ntA6useu/ov22fv9HmH3wp08Kl5tutJwIUfeP6jWVSNGnQVjL9PmNxuk8B9\nYthOyv4s/S7ewrZKfkNEOohILYAPAdwrIs2xq/jilE2OoT0s3eIZhRtFpRzU9Ax+P9+V5t022w64\n/Evgd1n8fgf+PtryUfvZykdgOHhE+GWPvU/X/Tn+If8L8klPBBe3xdm5e5jcJlurDub5uQSGPT1D\nmP3yAfNyx93vTEetu7zzcf7v+a3L3U3JT840L1PTK72oLagOWJgid9PxGfR9VWPw4bD1ITq3a+df\n+i+TU2Dos3E7zd46brV9wx8voY8rKw1xNGJq6TrGu26b+l625444A7Kg38q0v8IUre/3G32+LgFh\nj4gapdQaAMcAeFAptReAwflLFkXSkEVg2PQnUsDrf441OYn63Rzz/H4Hpb6Ou2jK5JjR+R+dpn13\noG2X6J/b/eRoy0etv6MagT98619pHQD2uzT8+k54OL0laRARHUgGXSC3Hqwr9B/2l9T59oUpn3f+\nxmI+y94Xmudnm55LPgLOfk1P1/TRz70NOTuA5yYzQ4B8xewIifApSnbXXXP/Tu7varroHuRpKDJg\nuOuF67N+DRqiBh9hi8LbbeZfRy5MUfL5PkNZ+v32dtFymDHtRczBb9hGMXYRaR9PtYiMN6aGbdoN\nmrY8IP09O40//YPObXTb/zJn+lf/ydy1U9ibG+8xZh+LPXZLX9Z0PrF/hx2P8d/Godfr83UJCBsY\nVolIDwDHw2l8QkkwHejZFCXbJzmlgE+fyilJBdXWallZa+geo7Klf90Xb5Hh4BFxpspsl4DcA5P2\nOTT0jxroRs19yiYwbN0xQxcdoosPz3gl8/r8Apkw/C58FZX6YpnWIjAPOYZexz/o33db+27p8377\nRbTA8JDrnGn3b33OG8B57/r//tv9DDhrop4Ouqge+LtoNyQK5vS7z13uNHXu7xRZRs1pa1qPAIOu\n9ElPhMCwfQ994Q/7/zzJpye3TAHYwdfqenKnPpdep9cvvT/7mw6ettjP/H4aw7HtPRY69jF/9LTn\ngV/8M3z3MTZT/dMeu+obC+MNqp0LOgDY65zUt9pv7kxvM8QTEBuOr0z1J73btEXNMRTRnX0fc2/6\newddA5zyTPr8tln0ElAgYa8ONwAYB+BrpdQUEekHYFb+kkW+TPWPsilKtk/S3xZZHcPffhH8/sXT\ndC7c+YZ0B104vRe50MUshhPEnudmN4h8ptxDb5qCWkl6RS3Cdm/Lm5tqErXifJgLrwiw70XpQ3UZ\nl82hGNV0kj/kOv/jJdfun7Ydpi+iQSpbAtcsAa5Zmnqx6+OzL/zqaPnZ43SgnRVguvdd285A9538\nv3uLtnAusAF1DA++JmIOps8+Pcwqrbjss9R09tnLOdeFOfbcOU8iOgC+bKb/DU1QTw5tuzp1VAHg\n8i/0enb7la6akIlf44NM3+NAq9XzVgeF76y5VY0OnkSAkx53ctVF0nMR3ce1+7frsHnqco0+/92O\nvfVx5WV3VWTXs/Tyq2LQzuec1fS7G/633t/T7zvZdjnevA23VjUB1wdTDqvPuahlO/MIJt131lUN\nvE5/KXPaEhK28cl/lFK7KKXOt17PUUodm+lzlAemyvINOeQYFlsn1pkugHb9q+rWhiKboMDQc7IL\nCjT67ONMm3IJht0CXDjF58MhL5bXLDV81JMmb/AZdnQJd/rdTnTlZLgD3tOeA7psm7687dy3o3eJ\nFCpHJsS+2vEYYKuDde5jUAOY/ocFbMZwgTngt/7L2/XxwgQ+1YZuJiqqgBaunFJTvbMK67euaqm7\nIbEvrqbGCwM9de/CtPiubOFc7IzHelB/e9ZzvvtHHbFad6EzYrWuu+tNp33DGyYw7O1qnSsVus5d\nTU+kfM9f/NOZDqpj2PdA4Lgx6e+LADsckTktfvLVP6Vt28OBPV05bLX9dF3S/V3Huuk33eciXe3F\nrpvZfado223RRlfhOdynu7OoVSDsa5zpuPUtuRCEPvd6++W8ZLphda7fypt7G/UmtYVPH59+84tA\n2MYnvUTkWRFZaj2eFpFe+U4cedRtAMZdnT4/lxzDYrHNUN13XBTe7xD0h1WNqXe0duu5loZikR2P\nzrztXE/ypsYI7u9z5oT0i9euJ/mvzz7hnzUROO0F8zJbH+ralmdfnfac/7p77BK9boz7AnTqs+Zl\nwhyD2w7Tn6+oBKoDhs3a9QT/9+wgzBTEeV02E9jRPk4M6avxFLP5FZXb2+y+M3DsaD29+6nmZTv2\nBgb+2vxeqxrg5652fn+Ym9pwZJcTzZ+rbAEnx89U9BWm1aUrUMpGh57OzYy7jt6Zr5qrD3iPhyiB\nYcpNlKGYtKIqNTc2sDFVxHPjHmeEWy7qOSPoZu3UZzP0l2d9hx2P9in6dX3Hyipd7cXez0Fd4Php\n2znaeL8de/u/F5RjmFbfNOLNy17n6Rsxtza1znrsnFr3/2P7X6Q27okaGHrrUXbZBthif1fwWWTX\nYoQvSr4fwAsANrceL1rzqJCmjAamGOowZHNn/97I3NMTl22GAr96IrhbESNvYBjwB2tsAI4f67yu\n7adzKn5n1YjoOdB5z3ux/7XnRAJkX7TZqsYcjAKpd6m990zPCQ6zzRZtgSqfi577btu7rkz1nyqr\n9f5yF4fvcQbQop15eXdQu9XBwImPBa//pMd93vA5tkesTm3AEWbfBAUYWx2sn2sy3O/uc2HqBfvo\nu30WFM8zgCPD/Oes79vOCsSvnJf6dutOqb/jMf82r6ayOjjH0PRf8QtEum4LXLsC+ONK/2SbDLvV\n2a+N9c42O2wervqAnQvs7bvPj92Bt18jlq7bAOe9o7t2Md5keY61o+7RuWmZBOXku4fai1odo+s2\n/g07tjrYZxjCHHJ57XOAUkCbLBq02YbdpnMu97vUXJf7V08CeweM9NHU+CtEUbJNJNyNZoeewfWe\nO/e31uf5z/id58LwpuuiKcAZLznbiNJjQYGEPVK7KqXcgeADIpKhx0qKXcNG8/yoLeyyacWcLwOG\n6/pK2YiaY2hS1RK4YpYuyvuL1RWNO0gEnErw7hOV30ko08kpqAWfX1HaiY/qC2nQ9wtzUnQvY6/b\nvgBkkwN6wOU6eJ1u6Pzae7Oy3TDgomnASHe3Ka709PK7KfB8r30vMRdrBwW29n7bbDtg/vvmZX71\nn/RGXN59+n+LdPFPr4HA6EN0VQY78EmhEKkPQNNy574FrJpnXtbPL8cC/xnuWl9QUbLHtcud5ey0\nuH/DKLlBNqlwuiXZtM51QQ4IXg75o24tDgCH3qCLN8MGhh2soD5lX3r2a/ed9QPQrV7fNHTHa39+\nt4Ac+pTlgzpDdt+MeY7RY+8DnvbposcWtVGZfUPRc0Dwcqbj0v4eqlG3oH3ep3V8JnuebZ5//EP6\nvLNNQLUPIDjH0Nswxr7B6L0XsCxD/XTA//94+kvA9Eed94POh5sbWipnw97flcUXGIbN9lghIqeI\nSKX1OAVAnkZ6p8iiBIbz3gduLEBXLWFsczhwxB3ZjeEKID0LPqiOYUAduXab6YrDgB7z1H2i32Jf\n52QR2MLW5cTHgLNfN79XWe3c9bb09FfnFxi2rtU5Re4cB28g5Q3E3HUYtx3m5FIOvl4X5bXtqos4\nfnl/6rajjOfasTf8L/KmIce2Th1T1y9nx81bp2vIjcBQQ/dKQSfyVjXAyU8F5EpCBz6Zuvyw6wQ1\n9ann8913Otb1n/R8r4OvBapCdC3SvltqvbkwvBXcg/pj9O7vympnH9o3C6bOiqOQCuc/s2mtc4MV\n9P0PuNxZrrLKv86xcWxaU4vbgHPCQf+n68+e84b18Sxz24K2cZIrp9ybYximAZu9br8qA17ddtDf\nya+vwaDvaP9W1a2iD58Yxg5HADsFdOdia6pjaPhP9/LctG91MHDlfKDfT5GxSLZDr9RGRW5b7Ktz\n9JtyK+0g2dpf9vzh/9Wt8e20ZQrcg4r77XX6lfAkKOxt4K8B3Ang79D/vkkATs9TmsiP3586SmA4\n1VCp2k9VK6B+Q/jl/Wy2g+5Eu/eewNy3nfm5juRw6PXAw8foFqAPHhl8XnAHUn65U9cs0yfv+vX6\n9b6X6GKnqpY692Ibn1EN9r0YmHSn9UJ07lgYl38BPHqCs0+8v6NdlGzXh6qs0n0DtvBp/eZW1RIY\n8iddZO4e2m1/V0b/6a6GR9n+Fn7HpF9jlY59nDv7TKMvtN88fDGLO2i3c4Tc+h+aPi+KlMZGdo6a\nz/9ux6OBBdOsRT0H5YFXOK1P3WJp6BElF9s1z9v6vWNvPRqFqXXtoKuAZSE76ZUK5/fb6TjgF//Q\nHer7dSkVRU0vfQP2uCtXzw7YU/pGtC7gh/n01dpjF8PMDAHGHqcDP64APrdGswkKDGv7OtPem5dM\n1RZsVy+O1sG88Tt5GdK813k619yv78dCCcoxNLEbJGYKtH87M/O67P+0d9sHXQ08eZqr9Eh0JHTR\n1OD1GYv74awDSK1uUCRCBYZKqW8BpNy6W0XJ/8hHoshHHIFhlADggMvTx0+Wiuw6h712KbB2MfA3\nVz2mMMVsWx6gOyGefFf6e/ZoA5vW6df9BqW+36IdsOkHXVx9mPU9/m+Rfz0V+86tRdv0rmWCOmPO\ndNd49uvAqm/T57dom1p3Zf/fAC+4Rm1oCgxdf9PWHYO35RZmBAhbto1pDrkW2LgmfOv2U58DHhgG\nrJyDjBfgTMVh7v9DSnclMQ0d5+7Gp6sreLJzsfwajADIvi/EHCqip/2fQuQYVlTpfg29vKNR2Pz6\nBTTpvLXezh/mWjcz1eHqFgZpXQust+o6Nl1YrXphdkCz17nO8hUVETqZDxmc262bR0Tsz8/7H2vf\nXQd9N2Vo2BWm8+o4VLXQ5/ykBbVKDrJNQKfxXvtcZK5n780xtG25H/B7QzWgXBpxtttMNyTr+9Ps\n15EnuZxBA/p7oIKKIzDc8oD0XuZ775k+zNM+F6W2rvSrH3jB5PRht7wd2Ia5yJz+X110eNrzwBF3\nmpdp0VbfuR09KnX+blbnqZtt7wSDLdpE76w5k0y5PT0HZG7pfOKjTvHYblYxjp2jEKnic5YnKm/9\nsiCnv+TcKXfYHDjxEcNCPvukQw/neEqp8+jKYWzdST8f7dOwwrSNsGO/dojQmYJfzlarGv2/2Dsg\nZyWwq5iY1fTR/0vvtprSEPCbVrfJT1HWVQud3DJvg5lc/PYzvW5ANyQAnOLBqhb6Bi7XbYW92Pv1\nV+inokoXRbpVt9Y5tl23i7aurEWs+5qERp9cuzgddpN5fuT/bY77ccejw9ehLaAsahQ3KeIjq1z5\n5RhGKIbyOyEcekN6sbG93mNHAzW9gXdu1xfFITcCH1kdvR74O+C1P6Wvr+1mTh26pnFarW1XtgQu\nzpAFD6TWk+s3KHjZLv2d6UumAxtW6T4IV8wGdg1ZPyeqQ2/QfQauXZz9Ouzcxuo2ugj01Gd1VwYA\ncPQ9wJw3gosjmuRYFOnuOqGmN7B6vh6ea9X89GW33D+3bZmOVzsYBHR3Ox37OPU+43Tx1Oy6d4os\nzxfgy2Y6v9llM/RznfX/bfot89wPIaC7nVn6mfO6/2HArHH5+e2A1NyzHrvoVsbefumyFbU4/9fj\ngKWfh1++ogroaxgC7iK/PlHzIB/jgMctqI5hJudPAr6dBLzsqq6x8/Hh67E3ered4Zgo5gA7B7kE\nhgU461AokXIMfQ7kikpDT/mu9YbJ3j/lGV3nD9B1i9p21q0r7W4wwuZKHXmXvovOthm/u17PqYah\niOLiLl4+6XHgsSwC0KF/0fVW+g3Sr92tXFt3dPWpl2fu3+TC94H6jfpOttuO2a0v8CJruDhVVOgu\nUZZ+FrKOVJbbzkex3G4nA9M9uab5vgAb66dZ27RzzJqSkMeL1xb7phYPn/iIU7WjEEz1SXMWcn+1\n2yxaw7mru0BFAAAei0lEQVRsAp3mKGodQ7duO+qHOzA81tDFGwDsckJ6DxT2Nc+uspMxJ7cZBoYi\nshbmAFAAFKjiAzXJZx1DqUwfu9W93jB/1q0P0S3Eln3h5BjYlXWB8BeofLSIyze7U1i/cUb9tOqQ\nPh5oEipb6DvrAadZdR9DdAYdKCA48yvirKyKFhS2M4wpDESvA5urI0YC65YBs8Y7/bZ130mP533o\nDYVLR1Ur3Yn7T+wuUBLIHaqsjlYPtqjkOa8j3yOfhFIC+TnHjQHe/YdTdxTQVYmiDvuZyTGj0ufZ\nGSC1W+ltZursuznmGCqlQvbPQQXhd8GLJTCsSM9N6eTKebPrQZgGRHdr1cG/o2r34Pblpm0X4PgH\nnWLgpGR7ohLxv7OOXUwBy+G36PqjO1vjoe57CTDpjvDdCoWx2Q6pRaUmFRVOVyT2xaxFW+CSoJEp\n8kAktRP3wdcBL11urqPaop3ulsavtW5zlq+LfTEEEdv9TJdOHGwYQcurU1+rG5gC676TM1qQrd8g\nZ3rwiHDju2fDLt2q7Zu5+hKAsryWIbeiZCoWcQSGFZW6XqBt+yN0v3O2fS/R9eHsBh3ZKERF/CTt\ncGTSKSgeYepr5XpObdkuteX11ofowDDO4+zct+KpqpGEn5ylHyYVleZWls3Zdj8H+j6uu+SJU6sa\nYEPYltF51rK9zgkL41LDGMLFYP/L8rfuPc/WjQAz9iFq/c+L6f8eIwaGpcSv0vyKWbo5fRhBRcnV\nrXTF/wePSB8ftbJa999l+8UdTv2L057X/XqF3XaZ/pkSNfh64NnzdMORohCiKDlf4jy+4m7BTsWr\nVQdguM8447k4921g8Sfxr5fiJxKxY/nyvJYxMCwV61cBb/uMyfnipalBW6AMneD2+6lu6ddtp+DV\n7DHcme43KOS2KW+2GwZcFXEItXyK2vgkDnb9zq0OCV4uH7r0B75E7vWgeNNUHrrt5HRR1WkL/aDy\nU6b/VwaGpWLd8njWE1SUbMtLS7+UROR5/VTUwvSvl43afnrc67grqYdx8LW6Rblf/VpqXs43dBpO\nZag8r2UMDEtFXBfRz57zWX8BWszZwzqVYqtjiiiBHEMgh3G3c1RZzZxzouaGOYZUFtYuMs8vRMOQ\nqhZ6SDpvtzhU+rrtDLTrqjsUXzUvZOOT8jypElFzUZ7nMAaGpSKOi2hDwIgPher7rUWbwmyHCuv8\nd/TzwmnAvQcHL5vvxidERAVRnueyMu8/pIzEcTGdOML/vYIME0blz76BSagomYh8lGcQk4iOdu8P\n5XkOY45hqbBHHgGA33yqx+edPxkYf034dcx50/89Uye4RFGFydnOV+OTUrbPRcCij3PrJ5TIhP+z\n+J3+EjBvsq4eZXLYn53OsksQA8NSoVyBYcfe+rH2u2jrcAeXbue/p+uHEcUlVA43L1hNOvQATv9v\n0qkgojA6bA7sdIz/+/tcWLi05AEDw1JhKuqNUi/wuQuBpTNT5x03BvjmbaDbDrmljagJcwyJqAhU\n5zree/PFwLBUmHL7arcK//npD6fP2+lY/SCKHeszEVFCLv8SqGqZdCpKFgPDUmEKDHvsovsGtHvY\nJ0paqFxANj4hojxq3z3pFJQ0BoalQvnUD6zp7V93kCgpgY2SWZRMzdjwF4GW7ZNOBZEvBoalwg7+\nht6cOr+ikl3NUBFhdzVEgfoemHQKiAKxH8NSUb9BP/fYNXW+VPrnJgYZeGbuaSLysnMBg1olN+UY\n8vRDRFRseGYuFRtW6edWHVPnV1RFL0reZijw89vjSRdRigg5hixKJiIqOixKLhXr7cCwJnV+RUUW\ndQx5QaY8CRPs7Xep7sx551/mPz1EzV33nfVzzz2STQeVDAaGpaJuvX6ubp06v6IqelEyc2oo34KK\nkmt6AWeOL1xaiJqzrQ7Wo2U1DeNGFIxFyaWiYZN+9vbNJGx8QsWENx1ERYdBIUXAHMNSYQeGlZ7A\ncP33wIIPgPpN/uM2EhUcO7gmIipFzDEsFQ2bdCvOSk8sv2KWfv5TlLGOmatDedKhh37e/dRk00FE\nRFlhjmGpqN+oRzkhKmatOwHXrUo6FURElCUGhqWiYVN6MXK22PiE8onHFxFRyUqkKFlEakVkgojM\nsp47+Sw33FpmlogMd83fQ0RmiMhsEblDRF+J/NYrItuJyHsislFErvBsY6iIfGmt68p8fu+cNLAO\nIREREeVXUnUMrwQwUSnVH8BE63UKEakFcB2AvQDsCeA6VwB5N4CzAfS3HkMzrHclgEsA3ObZRiWA\nuwAcDmAHACeJyA4xfcd41ceYY9i6Y+ZliIiIqNlJKjA8EsBYa3osgKMMyxwGYIJSaqVS6nsAEwAM\nFZEeADoopSYrpRSAB12fN65XKbVUKTUFQJ1nG3sCmK2UmqOU2gTgcWsdxadhI1BZHc+6vOMtExER\nESG5wLCbUmqRNb0YQDfDMj0BzHe9XmDN62lNe+eHXW+YbRSfhk3pfRhmq2X7eNZDREREZSVvjU9E\n5FUA3Q1vXe1+oZRSIhJ7p2f5WK+InAPgHADo06dPnKvOrH4TWyUTERFRXuUtMFRKDfZ7T0SWiEgP\npdQiq2h4qWGxhQAGuV73AvCGNb+XZ/5CazrMer3bcHcJ715XGqXUKACjAGDgwIGF7cH3q1cKujki\nIiJqfpIqSn4BgN3KeDiA5w3LjAMwREQ6WY1OhgAYZxUVrxGRva3WyKe5Ph9mvW5TAPQXkb4i0gLA\nidY6iIiIiJqdpPoxvBnAkyJyJoBvARwPACIyEMB5SqmzlFIrReRG6OANAG5QSq20pi8A8ACA1gBe\nsR5B6+0OYCqADgAaReQ3AHZQSq0RkYugg9BKAGOUUjPz+L2z09ign3vvnWw6iIiIqKwlEhgqpVYA\nOMQwfyqAs1yvxwAY47PcThHWuxipxc/u914G8HKE5BfeB6P08+IZyaaDiIiIyhrHSi4FP1oZpXXr\nkk0HERERlTUGhqVA+DMRERFR/jHiKAUMDImIiKgAGHGUhML2jENERETNEwPDUqByDAzfui3zMkRE\nRNTsMTAsCTkGhq/dGE8yiIiIqKwxMCwFqtGaEMOb1ryakEP0XfhBHCkiIiKiMsTAsBTYRcliCAx3\nPMr/Pa+fnAV03Ta+dBEREVFZSWrkE4rELko2BH9HjwJ+XAGsmp95NWzdTERERAEYKZSCLtvo56P+\nlf5eVQugXXeEq4cYIleRiIiImi0GhqWgwsrY3XyA+X2RcC2XwxQ3ExERUbPFwLAU2I1PfIuCGfAR\nERFR7hgYloKgxifOQpnXU78hluQQERFReWJgWAq+eVM/++UYhi0invZALMkhIiKi8sTAsBRMf0Q/\nB7Uq5qh5RERElCMGhqUksI4hI0MiIiLKDQPDUpJNUXJDXX7SQkRERGWHgWEpCSxK9skx3Lg2P2kh\nIiKissPAsJT45gyyKJmIiIhyx8CwlNj9GXoFNUpmp9ZEREQUEgPDUtJY7/+eX1FymBFRiIiIiMDA\nsLRUVPu8EZAr6A4MB18fa3KIiIiovDAwLBVdtgU69AhYwC/H0FX83GWbWJNERERE5YWBYSmQSmD7\nXwS8LwFFyY2pyxERERH5YGBYClRjcFc1gUXJ7gYrDAyJiIjIHwPDYqcUAJUhMAR8i5JXzYs7RURE\nRFSmGBgWO7uIOCgwDCpKHjMkdTkiIiIiHwwMi51dFBwY1IUN+BgYEhERkT8GhsUuVGAIhBr5hDmG\nREREFICBYdELWZQcCgNDIiIi8sfAsNg15Rhm+KnCjHBS1TL39BAREVHZYmBY7EIFhoJQRclb7h9H\nioiIiKhMMTAsdmECwzBFyTscyTqGREREFIiBYbGLqyi56/bxpIeIiIjKFgPDYhdXUXLDprhSRERE\nRGWKgWGxC9vB9frvgboN/ss01sWbLiIiIio7DAyLXdiiZAB4/CT/9xoYGBIREVEwBobFLsrIJ1+/\nlv5Wpy31847HxJkqIiIiKkNVSSeAMoiSY2hSuxXQpgvQZ6/40kRERERliTmGxc4ODINGLQnKTVSN\n7KaGiIiIQmFgWOx+WKKf29RmuQIFDoVHREREYTAwLHbrVujn9j2y+7xS2RdDExERUbPCiKHYqQb9\nLJUBywT0YciiZCIiIgqJgWGxm/Omfq4I+qkyjZPMwJCIiIgyY2BYzJbMBCbfpaezzjFUzDEkIiKi\nUBgYFrN1y53piqCehYJyDFnHkIiIiMJhxFDM7PqFAFCRQx1DIiIiohAYGBazRldQF1SUHJRjyKJk\nIiIiComBYTFz5/YFNT4JyjFkUTIRERGFxIihmKkccwyVAua/nyFwJCIiItIYGBazXOsYznxWP3/z\nZnxpIiIiorLFwLCYNboCw2xyDNcsjDU5REREVN4YGBazXHMM2SKZiIiIImBgWMxyrWNIREREFAED\nw2JWv9GZDmyV7DefASMRERGFx8CwWCkFvPtP53VFddDCPrNZlExEREThMTAsVvM/AJZ94byubuO/\nrG/OIHMMiYiIKDwGhsWqfn3q66CiZOYYEhERUQwYGBarKKOVmHIM1ywC3r49vvQQERFR2WNgWKwi\nDWNnCAyfOgOo+zG25BAREVH5SyQwFJFaEZkgIrOs504+yw23lpklIsNd8/cQkRkiMltE7hARCVqv\niGwnIu+JyEYRucKzjbnWuqaLyNR8fu9oJPyiphzD9aviSwoRERE1C0nlGF4JYKJSqj+AidbrFCJS\nC+A6AHsB2BPAda4A8m4AZwPobz2GZljvSgCXALjNJz0HKaV2U0oNzPWLxWb8NREWNgSGjfWxJYWI\niIiah6QCwyMBjLWmxwI4yrDMYQAmKKVWKqW+BzABwFAR6QGgg1JqslJKAXjQ9XnjepVSS5VSUwDU\n5eXb5MN3H4Zf1pRj6B41hYiIiCiEpALDbkqpRdb0YgDdDMv0BDDf9XqBNa+nNe2dH3a9XgrAeBGZ\nJiLnhEx/kTHlGDIwJCIiomiq8rViEXkVQHfDW1e7XyillIjE3uFehPXur5RaKCKbAZggIl8opd4y\nLWgFjucAQJ8+fWJMbY6acgxd9RLZVQ0RERFFlLfAUCk12O89EVkiIj2UUousouGlhsUWAhjket0L\nwBvW/F6e+Qut6TDr9aZzofW8VESeha7PaAwMlVKjAIwCgIEDBxZR79GsY0hERES5S6oo+QUAdivj\n4QCeNywzDsAQEelkNToZAmCcVVS8RkT2tlojn+b6fJj1NhGRtiLS3p62tvFp9l8rIXaOobhyDFmU\nTERERBElFRjeDOBQEZkFYLD1GiIyUERGA4BSaiWAGwFMsR43WPMA4AIAowHMBvA1gFcyrLe7iCwA\n8FsA14jIAhHpAF0H8R0R+RjABwBeUkr9L79fPaStDo6wsKnxCYuSiYiIKJq8FSUHUUqtAHCIYf5U\nAGe5Xo8BMMZnuZ0irHcxUoufbWsA7Bol7QXTom34ZX3HSiYiIiIKjyOfFKv6Tc70aYEl4g4GiERE\nRJQDBobFqmGjM91vUIaFGRASERFR7hgYFqv6jZmXsZkan7gd+a/c00NERERlj4FhscomMDQ1OGlZ\nA+x+cjxpIiIiorLGwLBYNWzKvEwTV1HyhjWxJ4WIiIiaBwaGxeqUp8Mv6250YgeUfsXKRERERD4Y\nGBar9qbRBH1I0FB4bJhCRERE4TAwLAumEU+YY0hERETRJNLBNYV03rtAVavMy6UMhccxkomIiCg7\nDAyLWfe0wV3MUgLDuvykhYiIiMoei5LLgiswbLByDO1gMUq3N0RERNSsMTAsB6ai5B+W6OcGBoZE\nREQUDgPDsmAoSq7pnUxSiIiIqGQxMCwH4voZ7aLkrtsmkxYiIiIqWQwMy4Gp8YnwpyUiIqJoGD2U\nBXfjEwaGRERElB1GD+XA2F0NO7gmIiKiaBgYlgVDdzUcCo+IiIgiYmBYDtzFxnaOoWJgSERERNEw\nMCwHYqhjqBqTSQsRERGVLAaG5aBFW2fa7uCagSERERFFxMCwHBxynTNt5xiuX5lMWoiIiKhkMTAs\nB606ONONdcDsicB3HyWXHiIiIipJDAzLxfnv6eeGOmD+B8mmhYiIiEoSA8Ny0a6bfm6sT22MMuC0\nZNJDREREJYeBYbmorNLPk0amzj/izsKnhYiIiEoSA8NyUVGtn1fPA9YtTzYtREREVJIYGJaLympn\nWjgcHhEREUXHwLBcVFQ501KZXDqIiIioZDEwLBfuXELmGBIREVEWGBiWI456QkRERFlgYFiO7GHx\nwJxDIiIiCo+BYTmyA8MK1jUkIiKi8BgYliM7MGQjFCIiIoqAgWE5atFeP/c9MNl0EBERUUlhYFiO\n2nbRz8NuTTYdREREVFIYGJajxgb9XNUq2XQQERFRSWFgWI6UFRiy8QkRERFFwMCwHK1fpZ+r2ySb\nDiIiIiopDAzL0eoFQOtaoGW7pFNCREREJYSBYTlq2ARUtkg6FURERFRiGBiWk30v1s+N9UBFVbJp\nISIiopLDwLCcdO6vnxvrgQr+tERERBQNo4dyItbYyI0NHPWEiIiIImN5Y1mxAsN5k5JNBhEREZUk\n5hiWEzvHkIiIiCgLDAzLCgNDIiIiyh4Dw3LCHEMiIiLKAQPDssLAkIiIiLLHwLCcMMeQiIiIcsDA\nsKwwMCQiIqLsMTAsJ8wxJCIiohwwMCwrDAyJiIgoewwMywlzDImIiCgHDAzLVXXbpFNAREREJYaB\nYTlZ+pkzXcnRDomIiCgaBoblpLHBmZbK5NJBREREJYmBYTmprHamKxgYEhERUTQMDMtJhTswZFEy\nERERRcPAsJy4WyWzKJmIiIgiYmBYTlSjM13Bn5aIiIiiSSR6EJFaEZkgIrOs504+yw23lpklIsNd\n8/cQkRkiMltE7hDRWWV+6xWRk0XkE+szk0RkV9e6horIl9a6rsz3d88rd2DIHEMiIiKKKKlspSsB\nTFRK9Qcw0XqdQkRqAVwHYC8AewK4zhVA3g3gbAD9rcfQDOv9BsBPlVI7A7gRwChrG5UA7gJwOIAd\nAJwkIjvE+1ULyB0YtqpJLh1ERERUkpIKDI8EMNaaHgvgKMMyhwGYoJRaqZT6HsAEAENFpAeADkqp\nyUopBeBB1+eN61VKTbLWAQCTAfSypvcEMFspNUcptQnA49Y6SpM7MIRKLBlERERUmpIKDLsppRZZ\n04sBdDMs0xPAfNfrBda8nta0d37Y9Z4J4JUM2zASkXNEZKqITF22bJnfYslx92OoGBgSERFRNHnr\n00REXgXQ3fDW1e4XSiklIrFHMab1ishB0IHh/lmucxSsYuiBAwcWX+TlzjHstGViySAiIqLSlLfA\nUCk12O89EVkiIj2UUousouGlhsUWAhjket0LwBvW/F6e+Qutad/1isguAEYDOFwptcK1jd4+6yo9\n7sDwqH8llw4iIiIqSUkVJb8AwG5lPBzA84ZlxgEYIiKdrEYnQwCMs4qK14jI3lZr5NNcnzeuV0T6\nAHgGwKlKqa9c25gCoL+I9BWRFgBOtNZRmtzFxy3bJ5cOIiIiKklJBYY3AzhURGYBGGy9hogMFJHR\nAKCUWgndgniK9bjBmgcAF0Dn/s0G8DWcOoPG9QL4I4DOAP4lItNFZKq1jXoAF0EHoZ8DeFIpNTNv\n3zrfUhqfEBEREUUjio0UsjJw4EA1derUpJORavUC4O876ukRq5NNCxERERUNEZmmlBqYaTkOj1FO\nanplXoaIiIjIBwNDIiIiIgLAwJCIiIiILAwMiYiIiAgAA0MiIiIisjAwJCIiIiIADAyJiIiIyMLA\nkIiIiIgAMDAkIiIiIgsDQyIiIiICwMCQiIiIiCwMDImIiIgIAFCVdAIoZkP/ClS1TDoVREREVIIY\nGJabvc9LOgVERERUoliUTEREREQAGBgSERERkYWBIREREREBYGBIRERERBYGhkREREQEgIEhERER\nEVkYGBIRERERAAaGRERERGRhYEhEREREABgYEhEREZGFgSERERERAWBgSEREREQWBoZEREREBICB\nIRERERFZGBgSEREREQAGhkRERERkYWBIRERERAAYGBIRERGRRZRSSaehJInIMgDf5nkzXQAsz/M2\nmhvu03hxf8aP+zRe3J/x4z6NV6H25xZKqa6ZFmJgWMREZKpSamDS6Sgn3Kfx4v6MH/dpvLg/48d9\nGq9i258sSiYiIiIiAAwMiYiIiMjCwLC4jUo6AWWI+zRe3J/x4z6NF/dn/LhP41VU+5N1DImIiIgI\nAHMMiYiIiMjCwLAIichQEflSRGaLyJVJp6eUiMhcEZkhItNFZKo1r1ZEJojILOu5kzVfROQOaz9/\nIiIDkk19cRCRMSKyVEQ+dc2LvA9FZLi1/CwRGZ7EdykGPvtzhIgstI7T6SIyzPXeVdb+/FJEDnPN\n53kBgIj0FpHXReQzEZkpIpda83mMZilgn/I4zZKItBKRD0TkY2ufXm/N7ysi71v75wkRaWHNb2m9\nnm29v6VrXcZ9nTdKKT6K6AGgEsDXAPoBaAHgYwA7JJ2uUnkAmAugi2feLQCutKavBPBXa3oYgFcA\nCIC9AbyfdPqL4QHgQAADAHya7T4EUAtgjvXcyZrulPR3K6L9OQLAFYZld7D+8y0B9LXOBZU8L6Ts\nox4ABljT7QF8Ze03HqPx71Mep9nvUwHQzpquBvC+dfw9CeBEa/49AM63pi8AcI81fSKAJ4L2dT7T\nzhzD4rMngNlKqTlKqU0AHgdwZMJpKnVHAhhrTY8FcJRr/oNKmwygo4j0SCKBxUQp9RaAlZ7ZUffh\nYQAmKKVWKqW+BzABwND8p774+OxPP0cCeFwptVEp9Q2A2dDnBJ4XLEqpRUqpD63ptQA+B9ATPEaz\nFrBP/fA4zcA63n6wXlZbDwXgYABPWfO9x6l9/D4F4BAREfjv67xhYFh8egKY73q9AMF/UEqlAIwX\nkWkico41r5tSapE1vRhAN2ua+zq8qPuQ+zazi6yizTF2sSe4PyOxitt2h86N4TEaA88+BXicZk1E\nKkVkOoCl0DceXwNYpZSqtxZx75+mfWe9vxpAZySwTxkYUrnZXyk1AMDhAC4UkQPdbyqdN8+m+Dng\nPozF3QC2ArAbgEUA/pZsckqPiLQD8DSA3yil1rjf4zGaHcM+5XGaA6VUg1JqNwC9oHP5tks4SaEw\nMCw+CwH0dr3uZc2jEJRSC63npQCehf4zLrGLiK3npdbi3NfhRd2H3LcBlFJLrItGI4B74RQNcX+G\nICLV0AHMI0qpZ6zZPEZzYNqnPE7joZRaBeB1APtAV2Wost5y75+mfWe9XwNgBRLYpwwMi88UAP2t\nlkstoCuhvpBwmkqCiLQVkfb2NIAhAD6F3n92i8PhAJ63pl8AcJrVanFvAKtdRVGUKuo+HAdgiIh0\nsoqfhljzCE2Bi+1o6OMU0PvzRKuFYl8A/QF8AJ4Xmlj1ru4D8LlS6nbXWzxGs+S3T3mcZk9EuopI\nR2u6NYBDoetuvg7gOGsx73FqH7/HAXjNyvn229f5k8+WLXxk3ZppGHSrsK8BXJ10ekrlAd0S7mPr\nMdPed9D1NCYCmAXgVQC11nwBcJe1n2cAGJj0dyiGB4DHoIuN6qDrs5yZzT4E8GvoitKzAZyR9Pcq\nsv35kLW/PoE+8fdwLX+1tT+/BHC4az7PC3o/7A9dTPwJgOnWYxiP0bzsUx6n2e/TXQB8ZO27TwH8\n0ZrfDzqwmw3gPwBaWvNbWa9nW+/3y7Sv8/XgyCdEREREBIBFyURERERkYWBIRERERAAYGBIRERGR\nhYEhEREREQFgYEhEREREFgaGREQxEZEGEZnuelwZ47q3FJFPMy9JRJS9qsyLEBFRSOuVHgKLiKgk\nMceQiCjPRGSuiNwiIjNE5AMR2dqav6WIvCYin4jIRBHpY83vJiLPisjH1mNfa1WVInKviMwUkfHW\niAoQkUtE5DNrPY8n9DWJqAwwMCQiik9rT1HyCa73ViuldgYwEsA/rHl3AhirlNoFwCMA7rDm3wHg\nTaXUrgAGQI/kA+jhsO5SSu0IYBWAY635VwLY3VrPefn6ckRU/jjyCRFRTETkB6VUO8P8uQAOVkrN\nEZFqAIuVUp1FZDn0MGN11vxFSqkuIrIMQC+l1EbXOrYEMEEp1d96/QcA1UqpP4nI/wD8AOA5AM8p\npX7I81clojLFHEMiosJQPtNRbHRNN8CpJ/4z6PGABwCYIiKsP05EWWFgSERUGCe4nt+zpicBONGa\nPhnA29b0RADnA4CIVIpIjd9KRaQCQG+l1OsA/gCgBkBariURURi8qyQiik9rEZnuev0/pZTdZU0n\nEfkEOtfvJGvexQDuF5HfAVgG4Axr/qUARonImdA5g+cDWOSzzUoAD1vBowC4Qym1KrZvRETNCusY\nEhHlmVXHcKBSannSaSEiCsKiZCIiIiICwBxDIiIiIrIwx5CIiIiIADAwJCIiIiILA0MiIiIiAsDA\nkIiIiIgsDAyJiIiICAADQyIiIiKy/D9dspA+rrwXZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}